[
  {
    "id": "d8d3f9b2c0a6462db9f374fd73d387b8",
    "doc_id": "92d05ac612784f42b1afeb27179c9bea",
    "doc_name": "_Web_development_3sha.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T05:20:19",
    "updated_at": "2025-08-18T05:20:19"
  },
  {
    "id": "341f1c6801594dd4a3f344a3b2f45f6a",
    "doc_id": "4ae589d28ba844e9aa5d54b817fa484a",
    "doc_name": "Adobe_Finale_Document.pdf",
    "heading": "Document",
    "content": "Adobe India Hackathon 2025  Grand Finale Connecting the Dots Challenge Theme: From Brains to Experience  Make It Real Introduction Congratulations on making it to the Finale! In Rounds 1A  1B, you built: - Round 1A  A robust PDF understanding engine. - Round 1B  A persona-driven document intelligence system. Now, its time to turn those brains into a real, interactive user experience. User Journey  Document Insight  Engagement System 1. Context  Problem Users (e.g., researchers, students, professionals) deal with large volumes of documents daily  research papers, business reports, study material, etc. Over time, it becomes impossible to remember all details or connect insights across these documents. 2. Goal of the System Help users by:  Quickly surfacing related, overlapping, contradicting, examples or other insightful information from their personal document library.  Using AILLM-powered capabilities to enhance understanding and engagement  grounded on the documents theyve read. 3. Journey Flow Step 1  Reading  Selection  Trigger: User is reading a document within the system.  Action: User selects a portion of text (e.g., a scientific method, a business strategy, a key metric).  System Response: Instantly surfaces relevant sections from past documents in the users library. Uses semantic search and optionally an LLM to ensure context-aware matching. Speed and quality is important for user engagement. Step 2  Insight Generation  Goal: Go beyond finding related text.  System adds value by: Generate insights related to the selected text. E.g. overlapping, contradictory viewpoints, examples, or other insights. Offering contextual insights that enrich understanding. Grounding all results in documents the user has actually read (uploaded)  not generic web sources. Step 3  Rich Media Experience  Optional Action: User requests an audio overview  podcast for the selected topic.  System Capabilities: Generate a natural-sounding, engaging audio overview of the topic using the selected text as a seed. Pull content from users documents to maintain trust and accuracy. Structure audio for easy listening  highlights key points, contrasts perspectives, and connects concepts. Making the audio overviewpodcast natural sounding, engaging, contextual, and grounded is key. 4. Key UX Considerations  Speed: Minimize delay between text selection and insight surfacing  keeps user engaged.  Relevance: High-relevance matches ensure trust in the system.  Engagement: Audio should be natural and dynamic, not robotic.  Extensibility: Users can explore beyond core tasks (bonus features) while staying aligned with the main flow. 5. Example Use Case A researcher reading a paper on neural network training techniques selects a paragraph on transfer learning. The system instantly shows:  Similar methods in 3 previous papers.  Contradictory findings from another study. Or how another paper has extended the technique. Or how another paper has found problems with the technique.  An audio overview  podcast summarizing these related sections and insights for quick listening on the go. Your Mission Build a web-based PDF reading experience powered by your earlier work that: 1. Displays PDFs with full fidelity  zoompan. 2. Connects the dots  when the user selects some text, shows related sections and snippets from other PDFs. 3. Accelerates understanding by showing context-aware insights. 4. Adds Audio Overview  Podcast features for richer engagement. Core Features (Mandatory) PDF Handling - Bulk Upload: User uploads multiple PDFs at once (represents past documents that the user have read). - Fresh Upload: User opens an additional new PDF (represents current document that the user is reading). - Display: Render PDFs at high fidelity (using PDF Embed API is preferred). Connecting the Dots - Identify  highlight upto 5 relevant sections across PDFs with high accuracy. - Sections  Just as was defined in Round 1A, the headings in a PDF logically break the PDF into a number of sections (heading along with their content) - Snippets  24 sentence extracts from the context of the section. Similar to how web- search results how relevant snippets along with the URL  Title. - When the user clicks a snippet, show the corresponding PDF and navigate to the relevant part of the PDF containing that section. Speed - Related sectionssnippets must load quickly after selection for better user engagement. - Ingestion speed for past documents follows earlier round limits. Follow-On Features (Optional, Bonus Points) Insights Bulb (5 points) Insights can use LLM calls. Some kinds of insights could be: - Key takeaways - Did you know? facts - Contradictions  counterpoints - Examples - Cross-document inspirations Audio Overview  Podcast Mode (5 points) - 2-5 min audio podcast (between 2 speakers) or audio overview(single speaker). Audio podcast is preferred but audio overviews are also acceptable. It should be based on: - Current section - Related sections - Insights from Bulb feature - Azure TTS for evaluation (Google TTSlocal allowed for development). Clarifications from Jury QA - Snippets logic: Select text  system finds relevant sectionssnippets  user clicks to jump. - Connections: Automatic linking across PDFs based on semantic meaning (it will be judged for relevance). - Prompt bar: Not required; use context from selection (and surrounding text if needed) - LLM Usage: API calls allowed; Round 1B logic can be replaced. - Offline requirement: Only LLM, TTS, and Embed API may use internet. - Backend: Fully runnable in Docker (combined frontend  backend). - Performance: No strict execution limit; faster is better. - Model size: Preferably under 20 GB for Docker image size. How will your solution be built and run. The following docker command will be used to build the solution. docker build --platform linuxamd64 -t yourimageidentifier . The following docker command will be used to run your solution docker run v pathtocredentials:credentials -e ADOBE_EMBED_API_KEYADOBE_EMBED_API_KEY -e LLM_PROVIDERgemini -e GOOGLE_APPLICATION_CREDENTIALScredentialsadbe-gcp.json -e GEMINI_MODELgemini-2.5-flash -e TTS_PROVIDERazure -e AZURE_TTS_KEYTTS_KEY -e AZURE_TTS_ENDPOINTTTS_ENDPOINT -p 8080:8080 yourimageidentifier Running the above command should bring up an application accessible on http:localhost:8080 Note that making external LLMTTS calls is entirely optional, and candidates may choose any on-device solution(like ollama for LLM or festival for TTS) which does not make any external calls. However, if any external LLMTTS calls are required, they must use the environment variables(which will be passed by Adobe). This is to ensure that user credentialsAPI keys are not embedded inside the code in git repo. Sample Scripts  LLM call: https:github.comrbabbar-adobesample- repoblobmainchat_with_llm.py  Generate Audio from text: https:github.comrbabbar-adobesample- repoblobmaingenerate_audio.py  Dependencies to be installed for the above: https:github.comrbabbar- adobesample-repoblobmainrequirements.txt Note that the generate_audio.py script generates mp3 file for a text. Multiple invocations of the script may be required to generate an audio podcast involving multiple speakers. For audio overviews(with a single speaker), a single invocation may be enough. Candidates are expected to include the above scripts in their code and make use of the their functions to make LLMTTS calls so that their solution work as expected in the evaluation. If the solution involves a non-python based solution, then a similar script in the other language may be used, if it respects the environment variables. Note that the evaluation will be done using Gemini-2.5-Flash for LLM, and Azure TTS. However, candidates may use other LLMTTS solutions which they have access to for local development and validation, as they are supported by the scripts above. Here are the list of environment variables along with the details Environment Variables which will be passed during evaluation. Environment Variable Value Description ADOBE_EMBED_API_KEY To be provided by the candidate. Optional, If you are using Adobe PDF Embed API, then this should be provided by the candidates while submitting their solution. LLM_PROVIDER gemini GOOGLE_APPLICATION_CREDEN TIALS Will be set by Adobe. GEMINI_MODEL gemini-2.5- flash TTS_PROVIDER azure AZURE_TTS_KEY Will be set by Adobe. AZURE_TTS_ENDPOINT Will be set by Adobe. The sample scripts shared above provide the ability to make use of other LLMTTS providers. See some possible examples below. Using Gemini for both LLM and TTS calls. docker run v pathtocredentials:credentials -e ADOBE_EMBED_API_KEYADOBE_EMBED_API_KEY -e LLM_PROVIDERgemini -e GOOGLE_APPLICATION_CREDENTIALScredentialsadbe-gcp.json -e GEMINI_MODELgemini-2.5-flash -e TTS_PROVIDERgcp -p 8080:8080 yourimageidentifier Using Local LLM(using Ollama) and local TTS implementation. docker run -e ADOBE_EMBED_API_KEYADOBE_EMBED_API_KEY -e LLM_PROVIDERollama -e OLLAMA_MODEL llama3 -e TTS_PROVIDERlocal -p 8080:8080 yourimageidentifier Deliverables Submit by deadlines: 1. Working Prototype (Docker runnable, accessible on localhost:8080). 2. Private GitHub Repo with code, Dockerfile, README. 3. Pitch Deck (max 6 slides): Problem, solution, features, innovation, demo, impact. 4. 2-min demo video. 5. ADOBE_EMBED_API_KEY(optional, if using Adobe Embed API): to be submitted via form. Evaluation Criteria Stage 1  Backend Evaluation (50) Core Functionality  20 points Technical Implementation  15 points Integration of Prior Rounds  10 points Performance  Reliability  5 points Stage 2  Live Finale (50) Demo Effectiveness  15 points UX  Design Quality  10 points Innovation  Creativity  10 points Impact  Storytelling  10 points QA Handling  5 points Bonus: 5 for Insights Bulb, 5 for Podcast Mode 8. Key Dates - Code Freeze: 19 Aug 2025 - Pitch Deck Deadline: 3 Sep 2025 - Finale Day: 5 Sep 2025 9. Pro Tips - Tell a story  show real persona impact. - Highlight magic moments in your demo. - Prepare metrics showing time saved  comprehension improved. - Rehearse until its smooth  predictable.",
    "page_start": null,
    "page_end": null,
    "word_count": 1453,
    "created_at": "2025-08-18T05:23:11",
    "updated_at": "2025-08-18T05:23:11"
  },
  {
    "id": "8993a6915a09436eb2990b36eedc2357",
    "doc_id": "737279ebdfa6464ea513e54936631268",
    "doc_name": "_Web_development_3sha_1.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T05:23:38",
    "updated_at": "2025-08-18T05:23:38"
  },
  {
    "id": "773b7ed2e3f14288985cace56e60e5da",
    "doc_id": "e86d4b37b130408da9fcea1fa562f554",
    "doc_name": "_Web_development_3sha_2.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T05:24:08",
    "updated_at": "2025-08-18T05:24:08"
  },
  {
    "id": "14e89dca388a40bbbf54534a26be0785",
    "doc_id": "12541da4f9b445699094009ac40f20b0",
    "doc_name": "_Web_development_3sha_3.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T05:36:32",
    "updated_at": "2025-08-18T05:36:32"
  },
  {
    "id": "76cd26b265404f32a49916b26c74904a",
    "doc_id": "2d6c65d97a6544b68e6aa3066e9ac35f",
    "doc_name": "_Web_development_3sha_4.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T05:36:42",
    "updated_at": "2025-08-18T05:36:42"
  },
  {
    "id": "932d6f3a0d5d480e883662ee6f8948b3",
    "doc_id": "73aca46be8ee4e11a4961427e4d374b9",
    "doc_name": "_Web_development_3sha_5.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T05:39:59",
    "updated_at": "2025-08-18T05:39:59"
  },
  {
    "id": "e8310b38f8784fb792252749e87b204a",
    "doc_id": "247612c59c854878924d125872049bf7",
    "doc_name": "_Web_development_3sha_6.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T05:41:26",
    "updated_at": "2025-08-18T05:41:26"
  },
  {
    "id": "d7abc7e679cd48688a93648443894244",
    "doc_id": "ebeb6aefc57b49bfa49c5668435a1834",
    "doc_name": "_Web_development_3sha_7.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T05:46:18",
    "updated_at": "2025-08-18T05:46:18"
  },
  {
    "id": "7e45bf6f8c9d466b84390556edae1c19",
    "doc_id": "96bd5ddbcebc4d0d9aa81954897c3668",
    "doc_name": "_Web_development_3sha_8.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T05:46:29",
    "updated_at": "2025-08-18T05:46:29"
  },
  {
    "id": "20562ae8e64a42d9826b9ffb07cc1e16",
    "doc_id": "60e3d9f1c0654a63ac2151081fe2f418",
    "doc_name": "_Web_development_3sha_9.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T05:50:09",
    "updated_at": "2025-08-18T05:50:09"
  },
  {
    "id": "286675c0ea814c479a89768d80048e15",
    "doc_id": "2bd16d8b54f945479518db59a5f40990",
    "doc_name": "_Web_development_3sha_10.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T05:51:21",
    "updated_at": "2025-08-18T05:51:21"
  },
  {
    "id": "05ac1a299d6241d19cd6f209f08e9105",
    "doc_id": "c298141ae2ca4f8bae2ea29d9e293226",
    "doc_name": "_Web_development_3sha_11.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T05:51:30",
    "updated_at": "2025-08-18T05:51:30"
  },
  {
    "id": "d9bedd0cc28744888d9bbee5d8f09b62",
    "doc_id": "1e866eb897f744be94c90d11230ac436",
    "doc_name": "_Web_development_3sha_12.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T05:57:10",
    "updated_at": "2025-08-18T05:57:10"
  },
  {
    "id": "c4539f9a62ec42018785f1b95e3a3bfd",
    "doc_id": "4d67b750e4144ea2a3ad0657abd04a6b",
    "doc_name": "_Web_development_3sha_13.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T06:02:23",
    "updated_at": "2025-08-18T06:02:23"
  },
  {
    "id": "f603bb1b562c4f4e94dc20c1c5214ef4",
    "doc_id": "095cb36540434f3891f736dbe04824cf",
    "doc_name": "_Web_development_3sha_14.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T06:06:22",
    "updated_at": "2025-08-18T06:06:22"
  },
  {
    "id": "58706ef5bfab47ab9aed9334cb0501de",
    "doc_id": "367e00cc3ab0452a82b8a8058810d37f",
    "doc_name": "_Web_development_3sha_15.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T06:07:12",
    "updated_at": "2025-08-18T06:07:12"
  },
  {
    "id": "92395d81aced4d64bab445ed14da9ad8",
    "doc_id": "b0811d9fc2b840869a2259b9d4859906",
    "doc_name": "_Web_development_3sha_16.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T06:07:59",
    "updated_at": "2025-08-18T06:07:59"
  },
  {
    "id": "b5d120307d00463284c5569bcdca341c",
    "doc_id": "b6068bacb9c24b4cbc2c7f0ffa815df1",
    "doc_name": "_Web_development_3sha_17.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T06:10:56",
    "updated_at": "2025-08-18T06:10:56"
  },
  {
    "id": "ed74887c9e23446c9d451eb0dd2f0751",
    "doc_id": "13c5ffe6fab64be48e981768b1dc2a6b",
    "doc_name": "_Web_development_3sha_18.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T06:14:59",
    "updated_at": "2025-08-18T06:14:59"
  },
  {
    "id": "1b5ffd6b19074d1ea03cd9221b3c6b33",
    "doc_id": "4a67ae1a1ba1445b9d22761e3e2320e2",
    "doc_name": "_Web_development_3sha_19.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T06:17:32",
    "updated_at": "2025-08-18T06:17:32"
  },
  {
    "id": "4ccb7c7f28bf45e0a9009da06d80ef5d",
    "doc_id": "37bc9859b89e4655ba45db48dac8db28",
    "doc_name": "_Web_development_3sha_20.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T06:34:21",
    "updated_at": "2025-08-18T06:34:21"
  },
  {
    "id": "2b2aa6e9d1df4295ad146dcb46be8cf9",
    "doc_id": "01b2570a6df5469aa8367dfc5d148c3a",
    "doc_name": "_Web_development_3sha_21.pdf",
    "heading": "Document",
    "content": "Introduction to Web Development Welcome to an in-depth exploration of the essential components that make up the world of web development. From the foundational markup language HTML to the dynamic interactivity of JavaScript, well dive into the key technologies that power the modern web. by Adithi Garipelly HTML: The Structure of Web Pages 1 Semantic Markup HTML elements provide meaning and structure to web content, improving accessibility and SEO. 2 Document Structure Organizing content with headings, paragraphs, lists, and other elements creates a clear hierarchy. 3 Multimedia Integration Embedding images, videos, and interactive elements enhances the user experience. CSS: Styling Web Pages Visual Design CSS provides complete control over the appearance of web elements, from colors and typography to layout and animations. Responsive Design CSS media queries allow for the creation of websites that adapt seamlessly to different screen sizes and devices. Performance Optimization Efficient CSS can improve website load times and overall user experience. JavaScript: Adding Interactivity 1 Dynamic Interactions JavaScript enables web pages to respond to user actions, creating engaging and interactive experiences. 2 Browser Manipulation JavaScript can be used to modify the HTML and CSS of a web page, allowing for dynamic content updates. 3 Asynchronous Functionality Features like AJAX allow JavaScript to fetch data from the server without reloading the entire page. PHP: Server-Side Scripting Dynamic Content PHP enables the creation of websites with personalized, database-driven content that can be updated on the fly. Server-side Logic PHP code runs on the server, handling tasks like processing forms, managing user sessions, and interacting with databases. Web Application Development PHP is a popular choice for building complex, feature-rich web applications and content management systems. SQL: Databases and Data Management Data Storage SQL databases provide secure, structured storage for web application data. Data Manipulation SQL queries allow for efficient retrieval, insertion, and manipulation of data. Relational Models Organizing data into tables with relationships enables complex data structures and queries. Git: Version Control and Collaboration 1 Commit Changes Git allows developers to track changes to their codebase, creating a history of revisions. 2 Branching and Merging Developers can work on separate features or bug fixes in parallel, then merge them back into the main codebase. 3 Remote Collaboration Git-based platforms like GitHub enable teams to collaborate on projects, share code, and review changes. Responsive Web Design Mobile-First Designing for small screens first and then scaling up ensures a great user experience on any device. Fluid Layouts Flexible grid systems and CSS media queries enable web pages to adapt to different screen sizes. Progressive Enhancement Building a solid foundation for the smallest screens and then adding enhancements for larger devices. Full Stack Web Development Front-end Development Crafting the user interface and interactive elements that users engage with directly. Back-end Development Building the server-side infrastructure, database integration, and application logic. Deployment and Testing Ensuring the smooth and secure delivery of the web application to end- users. Conclusion and Next Steps Weve covered the fundamental building blocks of web development, from the structure and styling of web pages to the dynamic interactivity and server-side functionality. Now its time to take the next step and dive deeper into these technologies to turn your web development dreams into reality.",
    "page_start": null,
    "page_end": null,
    "word_count": 534,
    "created_at": "2025-08-18T06:34:48",
    "updated_at": "2025-08-18T06:34:48"
  },
  {
    "id": "5caa24974b8f4be38c7fa326d0b3d451",
    "doc_id": "3978c20c61c84675b26f5f3844df507f",
    "doc_name": "A_comprehensive_survey_of_federated_transfer_learn.pdf",
    "heading": "Document",
    "content": "A comprehensive survey of federated transfer learning: challenges, methods and applications Wei GUO1, Fuzhen ZHUANG ()1,2, Xiao ZHANG ()3, Yiqi TONG1, Jin DONG4 1 Institute of Artificial Intelligence, Beihang University, Beijing 100191, China 2 SKLSDE, School of Computer Science, Beihang University, Beijing 100191, China 3 School of Computer Science and Technology, Shandong University, Shandong 266237, China 4 Beijing Academy of Blockchain and Edge Computing, Beijing 100080, China The Author(s) 2024. This article is published with open access at link.springer.com and journal.hep.com.cn Abstract Federated learning (FL) is a novel distributed machine learning paradigm that enables participants to collaboratively train a centralized model with privacy preservation by eliminating the requirement of data sharing. In practice, FL often involves multiple participants and requires the third party to aggregate global information to guide the update of the target participant. Therefore, many FL methods do not work well due to the training and test data of each participant may not be sampled from the same feature space and the same underlying distribution. Meanwhile, the differences in their local devices (system heterogeneity), the continuous influx of online data (incremental data), and labeled data scarcity may further influence the performance of these methods. To solve this problem, federated transfer learning (FTL), which integrates transfer learning (TL) into FL, has attracted the attention of numerous researchers. However, since FL enables a continuous share of knowledge among participants with each communication round while not allowing local data to be accessed by other participants, FTL faces many unique challenges that are not present in TL. In this survey, we focus on categorizing and reviewing the current progress on federated transfer learning, and outlining corresponding solutions and applications. Furthermore, the common setting of FTL scenarios, available datasets, and significant related research are summarized in this survey. Keywords federated transfer learning, federated learning, transfer learning, survey 1 Introduction In recent years, we have witnessed breakthroughs in machine learning, especially deep neural networks (DNNs), in various fields such as computer vision, smart cities, health care, and recommendation systems. Driven by high-quality training data, these methods have achieved impressive performance and even outperformed humans in certain tasks. With the rapid growth of the mobile Internet, a large amount of data is produced by billions of smart devices. However, these collected data cannot be directly uploaded to cloud servers or data centers for centralized processing due to limitations in data security, user privacy protection, and network bandwidth, which poses substantial challenges to the traditional machine learning approach. Such phenomena is commonly known as  isolated data islands. One emerging paradigm for enabling distributed machine learning to solve this problem is federated learning (FL), which was first proposed by [1]. The main idea of FL is to collaboratively train a centralized machine learning model with privacy preservation by transmitting and aggregating model parameters between the distributed participants, which eliminates the requirement of local data sharing and each participant can maintain ownership of their data. However, in certain FL scenarios, the data distribution varies widely between participants. For example, the training data from different participants share the same feature space but may not share the same sample ID space, or the training data from different participants may not even share the same feature space [2]. Therefore, when participants want to utilize global information to improve model utility through FL aggregation, the difference in data distributions, feature space, and label space among participants will influence the model convergence to the optimum [36]. Furthermore, due to inconsistent local storage, computational, and communication capabilities among different participant devices, FL may grapple with system heterogeneity challenges, leading to straggler situations or high error rates. In addition to the above-mentioned data heterogeneity and system heterogeneity problems, FL also suffers from model heterogeneity, incremental data, and labeled data scarcity challenges, which are also focal points of attention among many researchers. To address the aforementioned challenges, transfer learning (TL) is employed in FL as an effective method of facilitating knowledge transfer between source and target domains [7]. The main concept of TL is to minimize the divergence between the distributions of different domains. Similarly, in one communication round of FL, we could consider each Received January 12, 2024; accepted April 21, 2024 E-mail: zhuangfuzhenbuaa.edu.cn; xiaozhangsdu.edu.cn Front. Comput. Sci., 2024, 18(6): 186356 https:doi.org10.1007s11704-024-40065-x REVIEW ARTICLE participant as the target domain and the other participants as the source domains. Given that FL often involves multiple participants, i.e., multiple source domains, and requires the central server to aggregate information (e.g., model parameters) from multiple participants to guide the update of the target participant. In the process of continuous interaction among participants, knowledge is mutually transferred, which allows a local model obtained from a specific domain to be used by other participants through TL, thus alleviating limitations such as data heterogeneity, system heterogeneity, incremental data, and labeled data scarcity. We rethink FL in [ 8] from the perspective of TL, and refer to the combination of FL and TL as federated transfer learning (FTL) shown in Fig. 1. However, in classical TL strategies, the target domain can directly access the source domain data or model information, which contradicts the principle of FL. Hence, these TL strategies could not be directly applied in the FL. Moreover, the standard FL scheme contains a sending and receiving process through the communication between participant and server to ensure that the global model is updated and optimized across all local participants. So in a communication round, local participants can act as source and target domains at different stages. Concretely, during the sending stage, each participant acts as a source domain to transfer local knowledge to other participants. During the receiving stage, each participant serves as the target domain to receive knowledge from others. These conditions increase the difficulty of applying TL to FL. Overall, the above unique challenges of FTL have captured the attention of numerous researchers, and many significant contributions have been made. Existing surveys in the FL field mainly focus on traditional FL [3,911], including horizontal federated learning, vertical federated learning [6], incentive mechanism [12 ], privacy protection [13,14], or introducing FL applications such as healthcare [15,16], mobile edge networks [ 17], and internet of things (IoT) [18]. Despite some studies [19,20] focus on not identically and independently distributed (Non-IID) or other heterogeneous scenarios, such as model heterogeneity, device heterogeneity in FL, there is still a lack of systematic and comprehensive review on the definition, challenges, and corresponding solutions specific for the application of TL in FL, i.e., FTL. To fill this gap, this survey is dedicated to giving a comprehensive survey of FTL, including definitions, a categorization, and a discussion of existing challenges and corresponding solutions, common setting scenarios of distribution heterogeneity, available datasets, as well as an outline of current FTL applications and future prospects. In detail, Fig. 2 shows the categorizations of FTL and corresponding solutions. Section 2.1 demonstrates related definitions of FL and TL, we classify the common settings of FTL scenarios into six categories, including homogeneous FTL, heterogeneous FTL, dynamic heterogeneous FTL, model adaptive FTL, semi-supervised FTL, and unsupervised FTL. Sections 3.1 to 3.5 systematically summarize the corresponding solutions of existing FTL works in these scenarios, including motivation, core algorithm, model design, privacy-preserving mechanism, and communication architecture they adopt. Since some studies have involved multiple FTL scenarios, we only describe the major issues addressed by these studies. Finally, recognizing that systems and infrastructure are critical to the success of FTL, we outline current applications of FTL and propose future prospects. The key contributions of this work are summarized as follows. Fig. 1 The overview of FTL 2 Front. Comput. Sci., 2024, 18(6): 186356 1. This survey is the first to systematically and comprehen- sively rethink FL based on TL (FTL). We provide the definitions of FTL and its challenges including homogeneous FTL, heterogeneous FTL, dynamic heterogeneous FTL, model adaptive FTL, semi- supervised FTL, and unsupervised FTL, and further detail these challenges of FTL through examples. 2. Based on existing FTL solutions, which include both data-based and model- based strategies, we give the current research status for FTL challenges. 3. We summarize the scenario settings of the homogene- ous FTL shown in Section 2.3.1, which is the most common situation in FTL, including the setup methods and applied datasets. Meanwhile, to make checking convenient, we outline the existing research on FTL. 2 Overview In this section, the common notations used in this survey are listed in Table 1 for convenience. Besides, we further introduce the definitions, categorizations, and open challenges related to transfer learning, federated learning, and federated transfer learning. 2.1 Definition Following with previous works [7,8], we first give the definitions of domain, task, transfer learning, and  federated learning that are used in this survey, respectively. Fig. 2 Categorizations of FTL Table 1 The common notations Symbol Definition Symbol Definition n Number of instances m Number of domains k Number of participants k Actual number of participants z Number of classes l Number of model layers p Number of servers s Server u Participant g Global d Threshold f Decision function x Feature vector y Label e Communication round F Device A Active participant B;C Passive participant D Domain T Task X Feature space Y Label space X Instance space I Sample ID space Y Label set corresponding to X S Source domain T Target domain L Labeled instances U Unlabeled instances L Loss function R Relationship matrix M Model E Extractor  Mean  Variance  Importance variable  Tradeoff parameter  Interpolation coefficient Ω Structural risk  Model parameters Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 3 The involved common notations are summarized in Table 1. D X P(X) X X  fxjxi 2 X;i  1;:::; ng D D  fX; P(X)g X P(X) Definition 1 (Domain) A domain is constituted by two elements: a feature space and an probability distribution , where the symbol represents an instance set, i.e., . Thus, a domain can be denoted as . In general, if two domains are different, then they may have different feature spaces or different probability distributions [21]. T Y f T  fY; f g f y 2 Y f Definition 2 (Task) A task is constituted by a label space and a decision function , denoted as . Given the training data, the decision function is used to predict the corresponding label , where is not explicit but can be inferred from the sample data. mS 2 N f(DS i ;TS i )ji  1; :::; mS g mT 2 N f(DTj ;TT j )jj  1; :::; mT g f T j ( j  1; :::; mT ) Definition 3 (Transfer Learning) Given ansome observa- tion(s) corresponding to source domain(s) and task(s) (i.e., ), and ansome observation(s) about target domain(s) and task(s) (i.e., ), transfer learning aims to utilize the knowledge implied in the source domain(s) to improve the performance of the learned decision functions on the target domain(s) [7]. k u1; :::; uk X1; :::; Xk X  X1 [ [ Xk MS UM MF ED Xi Definition 4 (Federated Learning) Assume there are participants , each aiming to train a machine learning model with their own private datasets . A conventional approach is to upload all data together and use to train a global model . However, in many application scenarios, participants cannot directly upload their own data or access the data of other participants. Therefore, a typically federated learning system is a distributed learning process in which the participant collaboratively train a model without sharing local private data [8]. i j i  1; :::; k Xi , Xj Yi , Yj Pi(Xi;Yi) , Pj(Xi;Yj) X X Y Fi , F j Xi e1 , Xi e  i Xi  XL ! 0 k u1; :::;uk fspjp  1; :::;Ng E Definition 5 (Federated Transfer Learning) Given there are some challenges in FL for participants and ( ), including data heterogeneity ( or or , where instance space consists of feature space and label space ), system heterogeneity ( ), incremental data ( ), and scarcity of labeled data ( ), the FL combines the TL to solve these challenges, called FTL. The specific definition of FTL as follows. Given participants in FL, the central server set is designed to achieve model convergence over the communication rounds. During each communication round, the typical federated transfer learning process includes two distinct stages: ui 1  i  k DS i 1  i  mS DS i TS i DS i TS i sp X1; :::; Xk 1. Sending stage: participant(s) ( ) is(are) assumed as the role of the source domain(s) ( ), where they are responsible for contributing local ansome observation(s) ( , ) corresponding to and task(s) to the central server without sharing local raw data . The server then leverages the collected sending information to implement the aggregation process. uj 1  j  k f(DTj ;TT j )jj  1; :::;mT g 2. Receiving stage: once the aggregation is complete, participant(s) ( ) then are assumed as the role of the target domain(s) and utilize the received global aggregation information to perform local model updates. E p  0 The above sending and receiving stages are assumed to repeat for communication rounds, or until the model is observed to converge. Particularly, when , the above process is considered as a decentralized federated transfer learning process. 2.2 Category of federated learning According to the characteristics of data distribution among connected participants, FL can be categorized into horizontal FL (HFL) and vertical FL (VFL). Generally, HFL considers the distributed participants to have data with the same features but are different in sample space, while VFL considers the distributed participants to have the same samples but different features to jointly train a global model [6,19]. Federated transfer learning in [8] refers that these participants have differences in both feature space and label space. Due to the limited research on federated transfer learning in [8], this survey categorizes federated transfer learning and VFL as a type of VFL for description. On the other hand, depending on whether there is asome central server(s) responsible for coordinating participants, FL can also be divided into centralized FL (CFL) and decentralized FL (DFL), where CFL assumes that there is asome server(s) to gather local model-related information or other training information from the participants and then distributes the updated global model back to the participants, while DFL assumes participants directly aggregate information from neighboring participants [8]. In the following, we will provide a brief introduction to these FL frameworks and discuss the various settings of source domains, target domains, and tasks when employing transfer learning within these frameworks. 2.2.1 Horizontal federated learning X I i j Xi  Xj Ii  Ij S T HFL is commonly found in scenarios where participants share the same feature space but different sample space , which meets homogeneity FTL described in Section 2.3.1. For example, the medical record data of two regional hospitals and may be very similar due to they use the same information system, which both record the patients name, age, gender, and other user private data, so their feature spaces are the same ( ). However, the two hospitals have different user groups ( ) from their respective regions, and the user intersection of their local datasets is very limited. In FTL, any participant can serve as a source domain ( ) to provide knowledge or as a target domain ( ) to receive knowledge from other participants in the same feature space, therefore, we define HFL in homogeneous FTL as: XS i  XT j ;YS i  YT j ; IS i , IT j ;8XS i ; XT j orXT i  XS j ;YT i  YS j ; IT i , IS j ;8XT i ; XS j ;i , j: From an extended perspective, HFL meets heterogeneous FTL 4 Front. Comput. Sci., 2024, 18(6): 186356 when participants label space is inconsistent in the knowledge-transferring process, the HFL in heterogeneous FTL can be represented as: XS i  XT j ;YS i , YT j ; IS i , IT j ;8XS i ; XT j orXT i  XS j ;YT i , YS j ; IT i , IS j ;8XT i ; XS j ;i , j: 2.2.2 Vertical federated learning X I I i j i Xi Xj Xi , Xj Unlike HFL where all participants have their own local data labels, in the VFL scenario, participants feature spaces are inconsistent, and their sample spaces may also not be entirely the same. For example, suppose there is a high degree of overlap in the customer groups between a bank and a telecommunications company in the same region. The bank has information on users credit history ( ), such as loan repayment details and credit card usage, while the telecommunications company holds data on users call logs, data usage, and payment records ( ), where the feature space is different ( ). These two entities, which all act both as source domains or as target domains, can engage in VFL to mutually enhance their services in different feature spaces. We summarize VFL in heterogeneous FTL as: XS i , XT j ;YS i  (,)YT j ; IS i  (,)IT j ;8XS i ; XT j orXT i , XS j ;YT i  (,)YS j ; IT i  (,)IS j ;8XT i ; XS j ;i , j: 2.2.3 Centralized federated learning Standard CFL requires one or more central servers to build a global model by collecting local information from distributed participants [22], which involves three fundamental steps as described below: 1. Receiving stage: each participant receives the initial model sent by the server. ui Xi 2. Sending stage: participants use their own private data to train the local model (add local model notion), and then send the local model to the server. Mg 3. Receive stage: The central server updates the global model by collecting and aggregating all the local updates and then sends the updated global model back to the participants. In the FTL setting, during the sending and receiving stages, participants share knowledge through a central aggregation strategy, where each participant can act as a source domain providing knowledge or a target domain receiving knowledge. For instance, during the sending stage, participants act as source domains providing model parameters, while the server acts as the target domain, aggregating these parameters to form a global model. Conversely, in the receiving stage, the server serves as the source domain providing global model parameters to each participant. 2.2.4 Decentralized federated learning fu1; :::; ukg Compared with CFL, DFL is conducted over different participants without a central parameter server for global model aggregation. Each participant uses a private local dataset to optimize their local model after receiving model updates from other participants. This process involves two fundamental steps as described below: ui Mi Xi Mi fu1; :::; uk1g 1. Receiving stage: participant federally train its initial model locally with its own dataset , and then send the model to other participants without direct data exposure. ui Mg fM1; :::; Mkg 2. Sending stage: participant obtain the aggregated model by aggregating the received local model , and then update local model with aggregated model. Similar to CFL, each participant in DFL could still serve as either a source domain or a target domain without a central server during different stages of the FL process. 2.3 Federated transfer learning Transfer learning has achieved remarkable success by enabling the application of knowledge from one domain to improve performance in another, significantly reducing the need for extensive data collection and training time in new tasks [2326]. However, as shown in Fig. 3, constrained by the unique distributed learning paradigm of FL, current FTL studies face many additional challenging situations, including homogeneous FTL, heterogeneous FTL, dynamic heterogeneity FTL, model adaptive FTL, semi-supervised FTL and unsupervised FTL. More descriptions are presented below. 2.3.1 Homogeneous federated transfer learning i j Di Dj Di , Dj Xi , Xj Pi(X) Pj(Y) i j Ti , Tj Yi , Yj Pi(yjx) , Pj(yjx) Assume that the local data of participant and participant constitute a source domain and a target domain , respectively. represents a difference in either their feature spaces ( ) or marginal distributions ( or ). Similarly, if the task between participant and participant is not same, that is , then there is a difference in their label spaces ( ) or in their conditional distribution ( ). Pi(X) Pj(Y) Pi(yjx) , Pj(yjx) ni , nj HOFTL refers to differences in marginal distributions ( or ), conditional distributions ( ), or sample sizes between participant data, which is often caused by diversity in domain or task between participants. Based on this, HOFTL includes five scenarios: Pi(Y) , Pj(Y)  Prior shift: Pi(X) , Pj(X)  Covariate shift: Pi(xjy) , Pj(xjy)  Feature concept shift: Pi(yjx) , Pj(yjx)  Label concept shift: ni , nj  Quantity shift: Specifically, as described in Subsection 2.2.1, horizontal federated learning assumes that participants have the same feature space, so HOFTL is a form of transfer learning under this HFL assumption. Moreover, HOFTL can also be presented in vertical federated learning when there is partial overlap in the feature space between participants. Unless specifically stated, the HOFTL methods discussed in this survey are all related to HFL. Overall, compared with homogeneous transfer in traditional transfer learning that only considers marginal and conditional probability distributions, Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 5 HOFTL also considers changes in the total sample size, which corresponds to the Non-IID data setting in federated learning. The detailed challenges of HOFTL are described below, and considering that homogeneous FTL is one of the most frequently discussed, we have summarized the specific settings for each homogenous FTL scenario demonstrated in Table 2.  Prior shift P(Y) P(yjx) Prior shift, also known as class imbalance, implies that the prior probability distribution could be inconsistent between different participants when the conditional probability distribution is consistent [182]. In FL, the prior probability distribution inconsistency may occur when different participants have different class distributions in their local datasets. If these differences are not properly handled, they can lead to a federated model that performs unfair and suboptimal performance. For example, an FL system is designed to improve predictions for a specific disease (e.g., diabetes) across different hospitals that participate in model training without sharing private patient records. Hospital A is located in an urban area with a high prevalence of diabetes, possibly due to lifestyle factors prevalent in the population it serves. As a result, in hospital As patient data, 30 of patients might have diabetes. On the other hand, hospital B serves a rural area with a different demographic and lifestyle, resulting in only 10 of its patients having diabetes. This difference in the prevalence of diabetes is a classic example of prior probability shift. In some extreme cases, hospital B may even have no positive cases for this disease.  Covariate shift P(X) P(yjx) Covariate shift, or feature distribution imbalance, describes a situation where the input feature distribution is varied between participants while the conditional probability remains consistent. This presents a unique challenge in FL because the global model is trained on data from multiple participants, and each participants local data may represent a different underlying distribution of input features. For example, the patient population at hospital A had a higher average body mass index (BMI), which is a known risk factor for diabetes, while the patient population at hospital B had a lower average BMI. There is a significant difference in the input data (in this case, the BMI distribution) between the two hospitals, known as covariate shift.  Feature concept shift x y P(xjy) P(y) x y Concept drift, which includes feature concept shift and label concept shift, refers to the change in the relationship between variables and , where feature concept shift implies to discrepancy among participants with the same prior distribution [182]. This type of shift can be particularly challenging in federated learning because models need to generalize across all participants data. For example, consider two hospitals A and B jointly predicting the incidence of diabetes, where   represents the patients health characteristics and   represents the presence or absence of diabetes. Hospital As diabetic population mostly has a higher socioeconomic status, resulting in a different set of health characteristics, such as better control of blood sugar levels and fewer complications. In contrast, patients with diabetes at Fig. 3 The challenges of FTL 6 Front. Comput. Sci., 2024, 18(6): 186356 x y P(xjy) hospital B may have lower socioeconomic status and poorer health characteristics, such as uncontrolled blood sugar levels and higher rates of complications. Differences in the distribution of health characteristics ( ) for a given diabetic patient ( ) are an example of a shift.  Label concept shift P(yjx) P(x) P(yjx) A A ui A Similar to feature concept shift, the label concept drift refers to inconsistent among participants with the same covariate distribution [182]. Some external events or changes may lead to changes of in either the sourcetarget domain, which further renders the models from the sourcetarget domain no longer suitable for tasks in the targetsource domain. For example, in a federated recommendation system, geographical location is commonly used as the input feature to predict users favorite items. Thus, if the emergence of tendentious policy or new pillar industries supports the economic development of area , the consumption level of will be improved. In this situation, the expected user preference will change, causing the prediction results of participant from to become invalid and unsuitable for the improvement of model predictions from other regions participants.  Quantity shift Different from the prior shift, quantity shift refers to the situation where there is a significant imbalance in the number of training samples available among participants. In FL, some participants might have a large dataset, while others may have a relatively small one. This can lead to a situation where the global model is disproportionately influenced by participants with more data, potentially leading to biases or overfitting to the characteristics of those datasets. For example, a large-scale hospital may have thousands of patient records, while a small clinic may only have a few hundred. This difference in data volume is a classic example of quantity shift in FL. In summary, with uniform feature and label spaces, participants in homogeneous FTL still face data distribution shift problems, including prior shift, covariate shift, feature concept shift, label concept shift, and quantity shift. Most current FTL studies focus on prior and quantity shifts, with few studies tackling covariate shifts. Feature concept shift and label concept shift are even less explored. However, external elements change, like time or policy, may change the Table 2 Data heterogeneity settings of HOFTL and HEFTL Problem categorization Setting Reference Dataset HOFTL Prior shift Fixed ratio [2733] CIFAR-101), CIFAR-1001), MNIST2), Tiny-Imagenet3), ImageNet3), FEMNIST4), OFFICE [34], DIGIT [35], OpenImage [36], WESAD [37], KDD995), SVHN6), HAR7), OFFICE-Caltech 108), MIMIC-III9), Shakespeare [1], DomainNet10), NSL-KDD9911), CINIC10 [38], CelebA [39], StackOverflow [40] Natural partition [4157] 1 classparticipant [37,5860]  1 classesparticipant [ 1,48,59,61117] Dirichlet Distribution [118121,121133], [32,81,93,112,134146] JensenShannon divergence [147] Half-normal distribution [47,148] Log-normal distribution [79] Covariate shift 1 domainparticipant [41,65,125,149151], [1,70,76,129,152156], [89,91,96,112,157166] Mixed domainparticipant [167,168] Feature concept shift 1 degreeparticipant [169] Label concept shift [89,163] Quantity shift Natural [52,54,89,98,159,170,171], [109,114,161,172] By data source [1,61,125,157] By parameter [88] HEFTL Feature space hetergeneity Overlapped feature [ 37,87,133,173175] CIFAR-101), CIFAR-1001), MNIST2), MovieLens [176], ModelNet [177], FEMNIST4) NUS-WIDE [176] Non-overlapped feature [176,178181] Label space heterogeneity Feature and label space heterogeneity 1) See cs.toronto.edukrizcifar.html website. 2) See kaggle.comdatasetshojjatkmnist-dataset website. 3) See kaggle.comctiny-imagenet website. 4) See github.comwenzhu23333Federated-Learning website. 5) See kdd.ics.uci.edudatabaseskddcup99 website. 6) See ufldl.stanford.eduhousenumbers website. 7) See github.comxmouyangFL-Datasets-for-HAR website. 8) See v7labs.comopen-datasetsoffice-caltech-10 website. 9) See physionet.orgcontentmimiciii-demo1.4 website. 10) See ai.bu.eduM3SDA website. 11) See s.uci.edudataset227nomao website. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 7 relationship between features and labels for partial participants while leaving others unchanged. This intensifies the feature concept drift and label concept drift among participants, which is worth deeper study in the future. 2.3.2 Heterogeneous federated transfer learning i j Di Dj Di , Dj Ti , Tj Xi , Xj Yi , Yj HEFTL mainly refers to the problem of inconsistency in feature or label space between participants in FL. To the specific, similar to HOFTL, it is assumed that there are two participants and , whose private data constitute the source domain and the target domain , respectively. HEFTL demonstrates the differences in either their domain ( ) or task ( ), which caused by their various feature spaces ( ) orand label space ( ). Based on this, HEFTL has three scenarios: Xi , Xj  Feature space heterogeneity: Yi , Yj  Label space heterogeneity: Xi , Xj Yi , Yj  Feature and label space heterogeneity: and Unlike HOFTL, where all participants train models with the same data structure, HEFTL allows for collaboration between datasets that are not identically structured. Thus, vertical federated learning is a prime case for HEFTL, since the local private data among participants in VFL may contain different sets of attributes or dimensions. Next, we give a detailed description of the settings as mentioned above.  Feature space heterogeneity X Y A B Feature space heterogeneity refers to the situation that the feature space of different participants is inconsistent, while the label space is consistent, particularly when different datasets involved in the training process have different sets of features. For example, in FL, two retailers are trying to identify fake reviews by local model prediction. Retailer has a feature space that includes review length, the number of purchases, and purchase history, whereas retailer utilizes review timing, user location, and account age as feature space. They all annotated their reviews with binary labels as true (0) or fake (1). Although the reviews obtained by different retailers have inconsistent feature space, these retailers still aim to leverage FL to enhance the predictive performance of their respective local models within a consistent label space.  Label space heterogeneity X Y A C Label space heterogeneity refers to the situation where different participants have consistent feature space but inconsistent label space , which is the exact opposite of feature space heterogeneity. For example, in FL, two international e-commerce platforms are aiming to improve their recommendation systems. Each platform operates in a different region and thus has different product categories that are relevant to their local markets. Platform serves the Asian market and uses categories like Apparel, Gadgets, Furniture, and Anime Merchandise. Platform is based in North America, and uses labels such as Clothing, Tech, Home Improvement, and Sports Equipment. All two platforms collect user data including browsing time, click- through rates, purchase history, and search queries, which make up their consistent feature space. However, the way they categorize their products (labels) varies due to regional differences in terminology and market demand, leading to an inconsistent label space.  Feature and label space heterogeneity X Y A B Feature and label space heterogeneity, indicates the feature space and label space are both inconsistent among different participants. For example, there are two different specialty health clinics using federated learning to predict if patients will need to return for more treatment. Each clinic has its own set of measurements and outcomes. Clinic focuses on heart health, measuring things like heartbeat patterns and blood tests, and is concerned with whether patients might come back with heart issues. Clinic is a general clinic in a remote area, tracking health indicators like blood pressure and weight, and wants to predict if patients will return for any follow-up care or need a specialist. Each clinic collects different health information (different feature spaces) and has different categories for what counts as a patient needing to return (different label spaces). Considering the health indicators may be helpful in predicting patients heart issues in the future. Thus, they want to use FL to build better prediction models without sharing sensitive patient data. In summary, heterogeneous FTL may occur when there is inconsistency in the participants feature spaces or label spaces. The existing research primarily focuses on FTL with heterogeneous feature spaces where only the feature spaces are inconsistent. Other heterogeneous situations in FTL remain worthy of deeper investigation. 2.4 Dynamic heterogeneous FTL DHFTL refers to the condition where the participant set that contributes to the FTL aggregation or the local raw data of partial participants in this set is dynamically changing at each round. We further provide detailed descriptions of the causes of dynamic heterogeneity. 2.4.1 System heterogeneity F rg e1 u1; :::;un e m(0  m  k;n , m) e rg e e rg e1 Each participants local device in FL could have different storage, computation, and communication abilities. Due to the varying storage or computational capabilities, some devices may not be able to complete the local training in time before aggregation. Meanwhile, the communication ability among participants is also influenced by network connections, and some devices may lose connection during a communication round because of connectivity or power issues [70,183,184]. These aspects greatly amplify the straggler issue in the aggregation process [185], forming a dynamically changing set of participants during FL iterations as shown in Fig. 4. Assuming that there is a global optimal direction for the global model aggregated by participants in communication round , and participants could send their local model to server in time due to devices limitation in communication round , the global optimal direction aggregated by participants in round may have a significant difference with when there is data heterogeneity among participants, which is not conducive to 8 Front. Comput. Sci., 2024, 18(6): 186356 u1; :::; uk Fi , F j;i; j 2 (1;k) the global model convergence. Therefore, how to transfer the knowledge among participants within a dynamically changing participant set is a key challenge for DHFTL. Dynamic heterogeneous FTL for participants caused by system heterogeneity ( ) can be expressed as: DS 1 ; :::; DS n  Se1 , Se  DS 1 ;:::; DS m; n m e 1 e Se e where and represent the actual number of participants in communication round and of FTL, respectively. indicates the set of actual participants in the communication round . 2.4.2 Incremental data u1; :::; uk Xi e1 , Xi e Real-world FL applications are often dynamic, where local participants receive the new data, classes or tasks in an online manner [102,186,187], which proposes a key challenge is how to execute FTL from dynamically changing data distributions [ 188,189]. If only some participants are constantly adding data, or even if each participant synchronously adds new data, the newly added data could disrupt the original local data distribution, potentially exacerbating the differences between participant distributions as represented in Fig. 5. This requires the model to generalize well across both the old and new domains [190,191]. In addition, its also possible that the feature space of the newly added data is inconsistent with the original feature space. Dynamic heterogeneous FTL for participants caused by incremental data ( fX;Y gi e1 , fX;Ygi e;i 2 (1;k) e or ) in communication round . Dynamic heterogeneous FTL in a participant can be written as: P(XS i;e1) , P(XS i;e) or fX;Y gS i;e1 , fX;YgS i;e: Another situation where dynamic heterogeneous FTL of multiple participants can be denoted as: P(XS i;e) , P(XS j;e) or fX;Y gS i;e , fX;YgS j;e:(i; j 2 (1;k)): Nevertheless, we can only observe popularity in typical incremental learning approach [188,189,191,192], while these problems in incremental FTL receive relatively less attention. 2.4.3 Model adaptive FTL M In practical scenarios, due to differences in training objectives, participants may employ different model architectures for training [164,193]. Therefore, employing conventional aggregation methods for heterogeneous models output representations or parameters, such as averaging participants parameters in FedAvg [1], cannot effectively complete knowledge transfer between participants in FL. Additionally, even if the dimensions of the intermediate feature outputs are consistent across different models, the representational capacity of these features for local data could still vary, hindering performance improvement of the model in the target participant [66,164,193,194]. TL generally assumes that the model architectures in the source domain and the target domain are consistent, however, the inconsistency of models Fig. 4 The detailed description of system heterogeneity in FTL. In each round of global communication, due to the resource heterogeneity among the participants, partial participants could not participate in the global aggregation in time, which results in the actual optimization direction of the aggregated model dynamically changing and deviating from the global optimal optimization direction Fig. 5 The detailed description of incremental data in FTL. In each round of global communication, due to the increase in the local user data or class, the local data distribution of participants may change, causing the actual optimization direction of the aggregated model to constantly vary and deviate from the global optimal optimization direction Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 9 u1; :::; uk Mi , Mj;i; j 2 (1;k) in FL poses a new challenge for the performance of target participants tasks by aggregating process. This challenge, implementing effectively federated training in a model heterogeneous setting, is referred to as model adaptive FTL, which can be denoted as for participants : 2.4.4 Semi-supervised and unsupervised FTL u1; :::; uk  i Xi  XL ! 0;i 2 (1;k) Real-world FL applications especially need to use unlabeled data more than others [195,196]. On the one hand, in cross- device federated learning [182], individual devices create a lot of unlabeled data, like photos, texts, and health record data from wearables. Its unrealistic to label all this data due to its large volume. On the other hand, cross-silo FL [182] involves businesses, where data labeling often needs special knowledge. This is common in finance and healthcare sectors. Labeling this data would be time-consuming and expensive. Thus, SSFTL and USFTL have caught the interest of some researchers [122,123,195]. Overall, SSFTL has two common scenarios: ① only one participant has labeled data; ② several participants each have a small amount of labeled data locally, where case ③ is often seen in VFL, where it s usually assumed that only one active party has data label information. USFTL in FTL refers to the scenario where all participants lack labeled information. The labeled data scarcity in FTL for participants is denoted as: . 3 Methodology We elaborate on the current research strategies for each FTL challenge mentioned in Section 2.3. As shown in Fig. 6, it mainly includes two mainstreams: data-based and model- based strategies. Specifically, data-based strategies emphasize knowledge transfer by modulating and transforming participants data for space adaptation, distribution adaptation, and data attribute preservation or adjustment [7 ] without exposing any raw private data. The model-based strategies aim to improve the predictive accuracy of any given participant by the models from other participants in FTL. Tables 35 demonstrate related works on solving FTL challenges through these strategies. Note that currently there are very few FL works that specifically address the issues of label space heterogeneity or label  feature space heterogeneity. Therefore, we will not discuss them in a separate subsection. 3.1 Homogeneous federated transfer learning Homogeneous FTL and heterogeneous FTL are two of the most studied challenges in FTL. We will first illustrate the strategies for addressing homogeneous FTL challenges from both data-based and model-based perspectives as shown in Fig. 6. 3.1.1 Prior shift This subsection describes solutions to the prior shift challenge in HOFTL, which is one of the most common issue in FTL.  Instance augmentation Instance augmentation in FTL aims to enhance data homogeneity of various participants through techniques like oversampling [27,28,208 210] and undersampling [211213], which mainly occurs on the side of the participants. In detail, FedHome [28], Astraea [47], and FEDMIX [61] consider resolving prior probability bias at the local participant level. However, they ignore the effectiveness of global information in the local augmentation process. Therefore, some studies [ 118,119] suggest bridging the gap between participant and global distribution by creating a public dataset, but this also increases the risk of privacy leakage. To mitigate this issue, Faug [ 29], an FTL approach based on the generative adversarial network (GAN), is proposed to avoid privacy issues from multiple data transfers. Faug trains a GAN on minority class data at a central location, then sends it back to participants for data generation, helping build independent and identically distributed (IID) datasets. However, the construction of the generator increases extra computational and communication costs. the study [62] introduces a batch normalization (BN) based data augmentation approach, involving the following steps: t ith i 2 1; :::; l 1. BN layer parameterization: In the round of global iteration, the BN layer ( ) of the global Fig. 6 Data-based and model-based strategies of FTL 10 Front. Comput. Sci., 2024, 18(6): 186356 Mg (i 1)th i i y( j)(1  j  z) z x(z) M( x( j)) 1  j  z ai i; i model from the round can be parameterized as a distribution of means and variances . For a given target category , where represents the total number of possible categories, participants will sample from the Gaussian distribution to generate samples by forward propagate ( ), meanwhile the intermediate activation values produced in sample process are applied to obtain the BN statistics ). x( j)  ar gminx l i1 i i2 2  i i2 2 LH (M ( x( j)); y( j)) H 2. Augmented Data Update: The computation of the loss function follows the formula: , where represents the cross-entropy loss. During the M x( j) backpropagation process, the parameters of the model are kept fixed, and only is updated to obtain augmented data that is closer to the real distribution.  Instance selection Instance selection aims to select a subset of the available data that is most representative or informative for the training process. In distributed learning, numerous studies [214220] suggest that constructing local training samples that are closer to the target distribution through instance selection methods can effectively enhance model performance. However, these methods are designed for traditional distributed training where training data is public. They are not suitable for FL, which uses private datasets from different owners. To solve this Table 3 FTL frameworks Reference FTL challenges Architecture StrategyDHFTL MAFTL SSFTL USFTL HOFTL HEFTL HFL VFL CFL DFLSystem Incremental PS CS FCS LCS QS FSH LSH FLSH [27,28,62]    IA [47]    IA,MS,FC [120]     IA [61]     IA [29]    IA, KD [121]     IA, FC [30,58]    IS [197]     IS [198]     IS [123]     IS [149]    FA [176]      FA [172]     FA [125]      FA [126]    FM [199]     FM [200]    FM [173]     FC [167]    FC,CR [37,175]    FS [174,178]    FS [49]    FS [181]     FS [179]     FS [65]     FC,MC [48]     FS [180]     FS,MS [150]     FAI [103]     FAI [201]     FAI [66]      FAI,MC [41]     CR [151]    CR [102]     CR,MS [50]     CR,PD [72]    CR,PR [67]     DCR,KD [128]    DCR [153]    DCR 1 Abbreviation: PS: prior shift; CS: covariate shift; FCS: feature concept shift; LCS: label concept shift; QS: quantity shift; FSH: feature space heterogeneity; LSH: label space heterogeneity; FLSH: feature and label space heterogeneity; HFL: horizontal FL; VFL: vertical federated learning; CFL: centralized FL; DFL: decentralized FL; IA: instance augmentation; IS: instance selection; FA: feature augmentation; FM: feature mapping; FS: feature selection; FC: feature clustering; FAI: feature alignment; CR: consistency regularization; DCR: domain-dependent consistency regularization; PS: parameter sharing; PR: parameter restriction; PD: parameter decoupling; PP: parameter pruning; MW: model weighting; MS: model selection; MC: model clustering; MI: model interpolation; KD: knowledge distillation. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 11 problem, the study [58] suggests using a benchmark model that is trained on a targeted small dataset before FL starts to evaluate the relevance of each participants data, where only highly relevant data is used for local training. However, this method does not consider the influence of other participants on the benchmark model during FL training. Accordingly, the study [197] finds that calculating the sample loss during FL training can reflect the samples homogeneity with the global data distribution. Among these, local samples with stronger homogeneity are more conducive to improving the global models utility. As a result, this study proposes an FL framework named FedBalancer, which employs a selection strategy based on sample loss to filter local samples, aiming to build a local training set that is better aligned with the global sample distribution. However, FedBalancer increases computational costs because it requires calculating the loss value for every individual local sample. The study [30] introduces a less computationally expensive method for selecting samples. This method uses the gradient upper bound norms of samples to assess their importance to global model performance. It calculates gradients from the loss of the last layers pre-activation output, rather than calculating the gradient from the overall model parameters. This usually requires just one forward pass to accurately estimate a samples importance. Specifically, the proposed algorithm includes the following two steps: 1. Participant selection: using the private set intersection (PSI) protocol, each participant is informed about the target categories relevant to the target task. Participants Table 4 FTL frameworks (continued) Reference FTL challenges Architecture StrategyDHFTL MAFTL SSFTL USFTL HOFTL HEFTL HFL VFL CFL DFLSystem Incremental PS CS FCS LCS QS FSH LSH FLSH [69]    PS [154]    PS [68,117]    PS,PD [202]      PR [75,77,78,130,131]    PR [74,76]     PR [73]    PR,MI [71]     PR,MI [63,79,80,83]    PD [155]    PD [81]     PD [84]     PD [82]     PD [156]     PD [110]     PD,MI [106]     PD,MC [133]        PD,KD [132]    PD,PP,MS [129]     PD,PR,MI [87]       PP [85]     PP [86]     PP,MC [134]     PP,KD [1]      MW [157]      MW [88]     MW [158]    MW [203]    MW [204]    MW [90]     MW [135]    MW,CR [89]       MW,CR [152]     MW,PD [122]     MW,MC [91]     MS [60,92,93,136,148]    MS [51]     MS [52]     MS 1 Abbreviation: PS: prior shift; CS: covariate shift; FCS: feature concept shift; LCS: label concept shift; QS: quantity shift; FSH: feature space heterogeneity; LSH: label space heterogeneity; FLSH: feature and label space heterogeneity; HFL: horizontal FL; VFL: vertical federated learning; CFL: centralized FL; DFL: decentralized FL; IA: instance augmentation; IS: instance selection; FA: feature augmentation; FM: feature mapping; FS: feature selection; FC: feature clustering; FAI: feature alignment; CR: consistency regularization; DCR: domain-dependent consistency regularization; PS: parameter sharing; PR: parameter restriction; PD: parameter decoupling; PP: parameter pruning; MW: model weighting; MS: model selection; MC: model clustering; MI: model interpolation; KD: knowledge distillation. 12 Front. Comput. Sci., 2024, 18(6): 186356 d with low relevance to the target task are filtered out. Reversely, if their total number of samples, which match the target categories, reach a certain threshold , these qualified participants can participate in global aggregation. This prevents participants with large category distribution bias from interfering with the global models convergence. tth u u (xu;i;t) fxu;ign i1 (xu;i;t) 2. Sample selection: during round, each participant of the selected participants measures the importance of samples related to the target tasks categories, where is defined as: (xu;i;t)   j  u;l t;l u;i t;l u;i f (xu;i;t)j2; t;l u;i; t;l u;i lth xu;i tth  t;l l  diag(l( 1); :::; l( rl )) j( )j ,  where are the input and output of the last layer ( ) of sample in the iteration, respectively. . and f (x;) : k u1 nu n fu()  (xu;i;t) . is the output matrix, is a trade-off parameter. The importance of a sample is indicated by the value of : higher values mean higher importance. (xu;i;t) Additionally, this selection strategy assumes that there are mislabeled samples locally, and these are often significantly more important than correctly labeled samples [30]. Therefore, by filtering out outlier samples where is significantly higher than most other samples, the above algorithm can effectively measure the true distribution of local categories, selecting samples closer to the global distribution for local model training.  Feature clustering Feature clustering seeks to find a more abstract representation of original features to group similar data distributions together [7]. In the past, most clustering methods in FTL used model- Table 5 FTL frameworks (continued) Reference FTL challenges Architecture StrategyDHFTL MAFTL SSFTL USFTL HOFTL HEFTL HFL VFL CFL DFLSystem Incremental PS CS FCS LCS QS FSH LSH FLSH [53,55,95,97,137], [100,101,147]     MS [96]     MS [54,99]      MS [56]      MS [205]    MS [108,162,206]    MS [94]    MS,MW [98]      MS,MW [111]     MS,MC [42,45,46,59,64,127]    MC [43]     MC [169]    MC [160,168]    MC [170]    MC [145]     MC [159]     MC [107]    MC [171]    MC [207]    MC [109,161]     MC [44,104]    MC,MI [105]    MC,KD [112]     MI [31]     MI [116]     MI [163]      KD [32,119,138140], [33,57,141]    KD [113,115,144,166]     KD [164]     KD [124]      KD [114]       KD [118,142,143]    KD [165]    KD [146]     KD 1 Abbreviation: PS: prior shift; CS: covariate shift; FCS: feature concept shift; LCS: label concept shift; QS: quantity shift; FSH: feature space heterogeneity; LSH: label space heterogeneity; FLSH: feature and label space heterogeneity; HFL: horizontal FL; VFL: vertical federated learning; CFL: centralized FL; DFL: decentralized FL; IA: instance augmentation; IS: instance selection; FA: feature augmentation; FM: feature mapping; FS: feature selection; FC: feature clustering; FAI: feature alignment; CR: consistency regularization; DCR: domain-dependent consistency regularization; PS: parameter sharing; PR: parameter restriction; PD: parameter decoupling; PP: parameter pruning; MW: model weighting; MS: model selection; MC: model clustering; MI: model interpolation; KD: knowledge distillation. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 13 related information for clustering [4246,59,64,127,169,221]. This model-related information includes things like model parameters [46,64,127], gradient information [42,59], training loss [44,169,221], and other external info [43,45]. Except for these, clustering methods based on data-related information also can be applied to mitigate the prior shift issue. For example, Astraea [47] traverses all unassigned participant data distributions by a greedy strategy, looking for a group of participants that can make the overall data distribution of each cluster as close to a uniform distribution as possible.  Feature selection For example, Fed-FiS [175] generates local feature subsets on each participant by estimating the mutual information between features and between features and categories. Then, the server ranks each feature and uses classification tasks to obtain a global subset of features. Similarly, research [37] utilizes a federated feature selection algorithm based on MI in the operation of autonomous vehicles (AV). This algorithm completes global iterative feature selection by locally executing an aggregation function based on Bayes theory, which greatly reduces the computational cost. Moreover, Feature selection is a common idea to extract important features, which can obtain similar performance across different domains, and these important features can serve as a connection for knowledge transfer [7]. In HOFTL, the local dataset of different participants may have similarities in feature space, and high dimensional features can delay the training time, leading to more energy consumption [48]. In this case, removing irrelevant features and selecting useful overlapping features is crucial to address the distribution shift problem in FTL. Current FL researches [37,48,49,175] have proposed a variety of solutions to the above problems, which mainly include three steps: 1. Local filtering: filter the optimal subset of local features of each participant. 2. Global filtering: aggregate local optimal feature subsets to obtain global feature set. 3. Sharing: feed back the global feature set to the participants, allowing participants to focus on the features most relevant to the global representation. For example, FPSO-FS [49] is a federated feature selection algorithm based on particle swarm optimization (PSO), which proposes two global filtering strategies to determine the global optimal feature subset: i acci j; j  1;:::; k X  maxfXij1 k k j1 acci j(Xi;Datj);i  1;:::; kg Xi ith B Datj jth B acci j(Xi Datj) Xi Datj 1. Mean assembly strategy: for the private optimal feature subset of the participant, its average classification accuracy is obtained by the classification accuracy from all participants, i.e., . Then, an optimal subset with the highest average classification accuracy is selected as the overall optimal feature subset, as follows: , where is the private optimal feature subset from the participant, is the sample data held by the participant, and , is the classification accuracy of evaluated by . i acci j; j  1;:::; k X  maxfXijminj1;:::;k(acci j(X;Datj));i  1; :::; kg 2. Maximum and minimum assembly strategy: the first step is to identify the minimum classification accuracy for participant  s optimal feature subset, using the classification accuracy obtained by all participants. Following this, the subset with the highest minimum classification accuracy among all private optimal subsets is selected as the overall optimal fea- ture subset: .  Consistency regularization The FTL atrategies also can be explained from a model perspective. Figure 6 shows the corresponding strategies. Among them, consistency regularization [7] refers to the addition of regularization terms to the objective function of local (or global) model optimization, which aims to improve the model robustness of participants, facilitating the transfer of knowledge from the source model to the target model during the training process. In traditional transfer learning, domain adaptation machine [222,223] and consensus regularization framework [224,225] are widely used for knowledge transfer in multi-source domains [7], which are applicable to FL scenarios with two or more participants. The objective function is represented as: minf T LT;L( f T ) 1ΩD( f T ) 2Ω( f T ); f T where the first term, as a loss function, is used to minimize the classification error of labeled target domain instances, the second term represents different regularizers, and the third term is used to control the complexity of the final decision function . In addition, according to the research [7], domain-dependent consistency regularization can be expressed as: minf T nT;L j1 ( f T (xT;L j ) yT;L j ) 2 2Ω( f T ) 1 kS u1 u nT;U i1 ( f T (xT;U i )  f S u (xT;U i )) 2 ; u uth LT;L u ( f T ) where represents the weighting parameter that is determined by the relevance between the target domain and the source domain. For example, pFedMe [72] utilizes Moreau envelopes for regularizing participants loss functions. This approach effectively separates the optimization of individualized models from the learning process of the overarching global model within a structured bi-level framework tailored for FTL. MOON [128] proposes a contrastive learning-based federated optimization algorithm that uses the distribution difference in intermediate outputs between global and local models. Each participants local optimization goal, beyond the cross-entropy loss term , aims to minimize the distance between the local and global model representations (reducing weight divergence) and maximize the distance between the local model and its previous version (accelerating convergence). 14 Front. Comput. Sci., 2024, 18(6): 186356  Parameter sharing The parameters of a model essentially reflect the knowledge that the model has learned. Therefore, in FL, participants can also transfer knowledge at the parameter level [7] by parameter sharing, which avoids the privacy risks brought by direct transmission of local data [1]. For instance, the source and target models share parameters, and the target models use their local data to fine-tune the final layers of the source model, thereby creating a new model [68,69,154]. Since parameter sharing is a common approach in FL and often forms the basis for other methods, this section will focus on explaining the basic method of locally fine-tuning the global model. Specifically, study [154] finds that fine-tuning global model parameters with local training sets significantly improves prediction accuracy, particularly for local models that differ greatly from global predictions. Unlike FedPer [68], which fine-tunes the global models top parameters using local data, Per-FedAvg [69] averages all participant model parameters and fine-tunes all global model parameters using the MAML meta-learning method. However, the majority of existing FL frameworks based on parameter sharing mainly focus on improving the global models performance on each participant using the participants local data, overlooking the enhancement of a global models generalization performance from the servers perspective. [117] proposes an FL framework based on a fine-tuning and head model aggregation method, called FedFTHA, which includes FedFT and FedHA. From the participants perspective, FedFT focuses on improving the performance of the global model to the participants local dataset by retaining and fine-tuning the local head model. From the servers perspective, FedHA works to reconstruct a global model that exhibits generalized performance, leveraging the participants head model developed during FedFT. This approach enables both participants and the server engaged in FL to mutually benefit and realize a situation where all parties are advantaged.  Parameter restriction The knowledge learned from participants is kept as model parameters and is transferred by the server in CFL. Using the global model directly as the local model usually requires a strong correlation between the global and local data distributions. If there are large differences in data distribution among participants, using the global model directly and optimizing it with local data could lead to a significant decrease in the models generalization ability. To address this, some studies [7176,129,202] in FTL restrict the similarity between the source and target models by parameter restriction [7]. For example, FedProx [202] controls the differences between local and global model parameters by a proximal term, which aims to avoid the global model being significantly skewed by too many local updates and further affecting robust convergence. This approach keeps updates close to the initial model, helping to tackle the problem of prior distribution shift and covariate shift issues. However, this proximal term could not align local and global optimal points, and considering the potential loss of important parameter information when the global model is transferred locally. FedCL [130] introduces elastic weight consolidation (EWC) from continual learning [226]. By using a server-side proxy dataset to estimate the importance of global model weights, local updates can be adjusted to prevent significant changes in vital parameters during local adaptive training: minf T LT;L u ( f T )   i; j Ri jT;L u;i jS;L g;i j 2 ; R  Ri j cu (c cu) c(c  fcug(u  1)k) where represents the importance matrix of the global model, derived using the servers proxy dataset. FedCL prevents divergence between global and local model weights and ensures better generalization and accuracy. Additionally, FedNova [131] addresses distribution inconsistencies between participants by normalizing and scaling local updates, enhancing model convergence. SCAFFOLD [77] focuses on reducing gradient variance, which first introduces a control variable for the direction of the participant model gradient, and then corrects local model updates based on the difference between this control variable and a global variable , alleviating shift issue. Additionally, FedCSA [78] adjusts the weights of classifier parameters based on the distribution of each category on the participant. This adjustment enhances performance when dealing with class imbalance.  Parameter decoupling Research [227] indicates that the classifier of the model may exhibit significant accuracy decreasing when dealing with imbalanced prior probabilities. Therefore, many studies [227230] have suggested that sending partial local models for aggregation by decomposing models of participants into body (extractor) and head (classifier) parameters can improve accuracy in the target domain, which is called parameter decoupling. The body parameters can capture general data information and be maintained locally, enabling each participant to learn data characteristics for specific tasks, while head parameters learn specific features of the target domain for sharing with the server to improve the effectiveness of knowledge transfer. For instance, FedRep [79], FedBABU [80], FedAlt [155], FedPer [68] and SPATL [132] perform global aggregation by sharing a homogeneous feature extractor. LG-FedAvg [63], CHFL [133], and FedClassAvg [81] share a homogeneous classifier. Different from these, Fed-ROD [129] shows great effectiveness by splitting the model head into general and personalized layers. Two predictors are trained using a shared body model to handle the competing objectives of generic and personalized federated learning. One predictor employs empirical risk minimization (ERM) for personalization, while the other uses balanced risk minimization (BRM) for general learning. Moreover, FedU [82] proposes a local sharing protocol based on a Siamese network. By aggregating only the online models from the source Siamese network to update the target model, it effectively enables knowledge transfer between participants. However, the study [80] finds that existing parameter decoupling methods by updating the entire model during the training process, lead to a decrease in personalization Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 15 performance. Therefore, it proposed an FL framework, called FedBABU. It updates only the body part of the model parameters during the federated training process, and the head is fine-tuned for personalization during the evaluation process. To more accurately determine the degree of impact each layer of the model has on the target domain, the study [83] introduces a layered sharpness-aware Minimization (LWSAM) algorithm, which addresses the problem of poor participant performance due to the biased generic information shared by all participants. This method first calculates the distance between the global and local models at each layer, determining how much each layer is affected by the target domain. It accurately divides the model into head and body parts. Then it uses the sharpness-aware minimization (SAM) algorithm as a local optimizer. By adding more disturbances to the model body, the method adjusts the influence of the target distribution on the model from a global perspective.  Parameter pruning Due to varying data distributions among participants, directly applying an aggregated global model to a target domain often does not give optimal results. One popular solution [132] is parameter pruning, which selects a subset of model parameters from the source domain to apply to the target domain. For example, FedMask [85] uses a method called model binary masks to selectively activate certain model parameters for training. This can happen after just one step of communication, without needing to fine-tune the model on local data, which reduces redundancy in communication and computation. In study [132], each participant uses a pre- trained reinforcement learning agent to choose parameters for combining data in a federated manner. They then use a global encoder and a local predictor to transfer knowledge from the combined model to individual models. Another study [86] proposes a federated search method, which uses a lightweight search controller to find an accurate local sub-network for each participant. This method is good at extracting useful information and lowers the energy used for analysis and training. HeteroFL [87] proposes splitting the global model along its width while maintaining the full depth of the participants DNN models, which aims to find more suitable local models. However, this approach could construct very thin and deep subnetworks, leading to a significant loss of basic features. To overcome this issue, the study [134] introduces a federated learning framework, named ScaleFL, which uses early exits to adaptively reduce DNN models width and depth, finding models best suited for training with limited local resources.  Model weighting The knowledge transfer between participants can be accomplished by sharing local model-related information, such as model parameters, which involves aggregating them before local training, called model aggregation. However, different participants may have distinct optimal goals, simply averaging their model information with the same weight could result in the combined results not being the best solution [1]. The global model in the server may also be overly influenced by a single participants model, causing model drift [131,231]. Thus, model weighting is applied to aggregate models according to their contributions. This prevents model performance degradation caused by directly averaging information from different actors into a domain. For example, the study [88] finds local data with higher prediction errors has more contributions to improve the overall model performance, and then introduces an FL framework, called FedCav. FedCav measures the quality of local data using their prediction errors to decide the weights in the model aggregation process. Considering that the server doesnt know the local data distribution, FedFusion [203] uses a global representation of multiple virtual components with different parameters and weights to portray the data distribution of different participants. The server uses a variational autoencoder (VAE) to learn the best parameters and weights of the distribution components based on limited statistical information taken from the original model parameters. Additionally, the study [135] treats the blending of multiple models in FL as a graph-matching task, and then proposes an algorithm, called GAMF. It views channels and weights as nodes and edges of a graph, respectively. Then it uses a new hierarchical algorithm to increase the similarity of weights between channels, and proposes a cycle-consistent multi-graph matching method to merge various local source models in FL, enhancing the global models generalization. Experiments show that GAMF can be used as a plug-in to boost the performance of existing FL systems.  Model selection In reality, participants local data may significantly differ from the optimal global distribution, and each participants data characteristics can not be directly controlled, thus it is important to select participants related to the data or specific target labels for training, called model selection. For example, the study [91] proposes a DFL algorithm based on directed acyclic graphs (DAG). In the DAG, each participant selects the model updates of other participants based on their data similarity, which has been demonstrated effectively in both prior and covariate shift scenarios. Astraea [47], a scheduler-based multi-participant rescheduling framework, re-schedules multiple participants through a scheduler, which follows that the data distribution of multiple participants is most similar to a uniform distribution. Dubhe [148] is an FL algorithm based on repeated model selection, which allows the server to repeatedly select local models to participate in aggregation, and then send the encrypted distribution of the selected participants to the server to check the similarity between the global data distribution after aggregation and the consistency distribution. This process continuously adjusts the aggregation strategy and improves classification accuracy. Another study [60] proposes a Shapley value-based federated averaging algorithm. It calculates the Shapley value of each participant to assess its relevance to the servers learning objective, estimating the participants contribution in the next communication round of FL. This allows the server to select local models with higher contributions for training in each round of aggregation. In addition, some studies [51,92] require each participant to 16 Front. Comput. Sci., 2024, 18(6): 186356 collect models from all other participants and use an additional local validation set to evaluate the similarity between participants. In contrast, the study [93] utilizes mathematical analysis methods instead of using empirical search from validation data sets to characterize the similarity between participants. Apart from data distribution, another study [52] considers differences in local data volumes between participants. Note that uniform sampling could overlook participants with more data, reducing their contributions to the global model training and impacting the models performance. To address this, the researchers propose FedSampling [52], a framework that uses a data uniform sampling strategy. When the participants data distributions are highly imbalanced, participants randomly select others based on the ratio of the servers desired sample volume to the total available participant sample volume, further improving FL model performance.  Model clustering Some studies [44,103] suggest that grouping similar participants for FL can address the model drift issue caused by data distributions heterogeneity among participants, called model clustering. They determine participant similarity based on factors like model parameters [44,64,104,168,170,232], gradients [42,59], training loss [44,169], or other external information [43,45]. For example, FedCluster [42] uses cosine similarity of the model gradient to split participants into multiple clusters, maximizing similarity within clusters while minimizing it between clusters. To further enhance model adaptability in the target domain by utilizing the sub-model clustering method, the study [105] designs a scale-based aggregation strategy, which scales parameters according to the pruning rate of the sub-models and aggregates overlapping parameters. It further introduces a server-assisted model adjustment mechanism to promote beneficial collaboration between device source models and suppress detrimental collaboration. This mechanism dynamically adjusts the sub-model structure of server devices based on a global view of device data distribution similarity. In addition, studies [43,45] use exogenous information like the types of local devices participants use or patient drug features (drugs given within the initial 48 hours of ICU admission, comprising 1399 binary drug features) for clustering. However, these methods tend to overlook the cluster skew issue caused by grouping, leading to the global model overfitting to a specific clusters data distribution. Cluster skew refers not only to an imbalance in the category distribution among groups after clustering but also to an imbalance in the number of participants in each cluster. To address this issue, study [109] suggests a new FL aggregation method with deep reinforcement learning, called FedDRL, which can tap into the self-learning capability of the reinforcement learning agent, rather than setting explicit rules. Specifically, FedDRL utilizes a unique two-stage training process designed to augment the training data and reduce the training time of the deep reinforcement learning model. Moreover, the study [107] proposes a DFL framework based on hierarchical aggregation, named Spread. In this framework, the server acts as the FL coordinator, and edge devices are grouped into different clusters. Selected edge devices, as cluster leaders, responsible for model aggregation tasks. Spread monitors training quality and manages model aggregation congestion by adjusting both intra-cluster and inter-cluster aggregations.  Model interpolation      Different from model weighting methods that combine local models with varying weights, model interpolation aims to blend global and local model parameters proportionally to increase local prediction accuracy [104,110,129]. For example, research [73] prevents the local model and global model from diverging excessively by setting an interpolation coefficient artificially. When is set to 0, the local model only performs local model learning; as increases, the local model gradually becomes similar to the global model, realizing mixed model learning; when is very large, all local models are forced to be similar, maximizing the transfer of knowledge from the global to the local model. Research [112] interpolates a global model trained globally with a local k- nearest neighbors (KNN) model based on the shared representation, which is provided by the global model. The experiments show that it is also suitable for covariate issues except for prior shift issues. However, research [44] demonstrates that these methods, which rely on the separated training of the global model and local model to find the optimal interpolation coefficient , may not always be the best. Therefore, it proposes a combined optimization strategy that improves both local and global models at the same time. Similarly, research [31] proposes a model interpolation method based on elastic aggregation. They measure the sensitivity of each parameter by calculating the change in the overall prediction function output when each parameter changes, which reduces the update magnitude for more sensitive parameters, preventing the global model from excessively interfering with the local data distribution. 3.1.2 Covariate shift  Feature augmentation Some studies [125,149] have proposed solving covariate shift issues from the perspective of feature augmentation. For example, the study [149] proposes an FL paradigm based on a feature representation generator, called FRAug. It optimizes a common feature representation generator to help each participant generate synthetic feature representations locally, which are converted into participant-specific features by a locally optimized RTNet, which aims to make global and local feature distributions as similar as possible, increasing the training space for each participant. In addition, similar to the study [62] in data augmentation, the study [125] further proposes FedFA to augment features from a statistical perspective. The statistics of augmented data should match as closely as possible with the statistics of the original training data. FedFA follows two preconditions: 1. The data distribution of each participant can be characterized as the statistics of the latent feature distribution, i.e., mean and variance [125]. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 17 2. The statistics of latent features can capture basic domain perceptual characteristics [233235]. Therefore, when dealing with data shift in FL, discrepancies in feature statistics across local participants are inconsistent, and they display uncertain changes compared to the actual distributions statistics. Based on this, FedFA leverages a statistical probability augmentation algorithm based on a normal distribution to enhance the local feature statistics for each participant following as: x   xu u u  u; u u u  N(u; 2 u )   N(; 2 u ) xu xu  (xuu) u ( u; u) u u where the mean and variance are the original statistics of the latent features, and , . is normalized by , then expanded using the new statistical values . The variance dictates how much the latent features are augmented. The magnitude of this variance represents how much the latent feature distribution deviates statistically from the target distribution. By adjusting the variance appropriately, the skewness issue can be resolved either at the level of individual participants or across all participants. Moreover, it can be integrated as a plugin into any layer of any network to enhance features or solve any feature statistical bias, such as skewness in test time distribution. FedFA also exhibits excellent accuracy performance when addressing prior shift or quantity shift problems of homogeneous FTL.  Feature clustering PFA [65] first clusters participants with similar data distribution by computing Euclidean Distance of local representations, and then orchestrates an FL procedure on a group basis to effectively achieve adaptation based on their clustering results. Additionally, the study [167] introduces a new FL framework, called FPL, which is based on prototype clustering. FPL uses the mean features of local data as prototypes, and clusters these prototypes at the server using a comparison learning method. This process brings similar prototypes closer and pushes different ones further apart. Moreover, to increase the stability of model training, FPL uses consistency regularization to minimize the distance between the representative prototypes and their unbiased counterparts. Compared to transferring model parameters in the FPL, the size of the prototypes is much smaller than that of the model parameters, which significantly reduces communication costs.  Consistency regularization From the models perspective, directly adding model-level regularizers to the local objective function of the participants or server is a natural idea [7]. In this way, the knowledge maintained in the model(s) of the participants (source model) can be transferred to the model of another participant (target model) during the training process. For fully supervised learning, each participant first uses their local labeled data to obtain the classification loss term. For example, FedBN [151] keeps participant-specific batch normalization layers to normalize local data distribution. Its classification loss term can be written as: LT;L( f T )  k i1 xi j1 ( f T (xi j);yi j) 2 : Except for the classification loss term, a consistency loss term can be introduced based on a cluster-aware mechanism, which uses the differences in both intermediate outputs and predictions between the global and local models to guide local model optimization [153]. It further groups the participants into clusters based on the feature clustering method by harnessing the similarity among lower-level features of each participants model. Each cluster has its own global feature vector and average prediction value. By minimizing the L2 norm between each participant and the global feature and prediction value within its cluster, the method improves the robustness of local models under the covariate shift issue of homogeneous FTL. Additionally, PFL [167] introduces a consistency regularization term based on a global unbiased prototype. It suggests that the cluster prototype averaged by the server, as an unbiased prototype, can provide a relatively fair and stable optimization point. Calculating the square difference loss between the local feature vector and the unbiased prototype can address the issue of unstable prototype convergence. The regularizer in PFL can be expressed as: Lre gularizer  v j1 (xi; j U k j ) 2 ; i j u v U where and index samples in dataset of participant and the dimensions of feature output, respectively. is the number of dimensions. is the unbiased prototype.  Parameter decoupling Parameter decoupling is not only suitable for prior shift or quantity shift problems [87], but also for solving covariate shift problems in homogeneous FTL. For example, the study [156] proposes a more flexible way of decoupling, designing an FL algorithm based on structured pruning, called Hermes. In this method, participants determine the sub-networks to participate in server aggregation through model pruning. To prevent information loss that could result from directly averaging local models, Hermes only averages overlapping sub-network parameters on the server, keeping the parameters of the remaining non-overlapping parts unchanged. The aggregated parts of the sub-networks are then sent back to the local devices for network updates, thereby improving the models performance on local tasks.  Model weighting FedUReID [157] enhances the adaptability of the aggregated global model to each participants local model by applying an exponential moving average (EMA) to update the global model for each participant, where the weight of the EMA represents the similarity between the global and local models. Additionally, FedDG [158] takes advantage of the domain flatness constraint, which serves as a substitute for the complex domain divergence constraint, to approximate the optimal aggregate weights. Moreover, FedDG uses a momentum mechanism to dynamically assign a weight to each isolated domain by tracking the domain generalization gap, 18 Front. Comput. Sci., 2024, 18(6): 186356 improving its generalization capability. Past studies often simplify the blending of source models into a straightforward allocation problem, ignoring complex interactions between channels. Meanwhile, since model weights are shuffled during training, before merging, channels of each layer need to be aligned to maximize similarities in weights between multiple source models. This presents a quadratic assignment property problem, which is NP-hard problem. To tackle this problem, GAMF [135] propose to treat the channels and weights as nodes and edges of a graph to obtain the weights information. These weighting methods typically rely on model parameters or gradient differences to measure each participants contribution to the target prediction. However, the transmission of this information involves potential privacy leakage risks. Thus, the study [204] views the local model of each participant as black-box model, in which all data is stored locally and only the source models input and output interfaces are accessible. Each participants soft outputs are given a weight based on their inter-class variance. These weighted outputs are then used to create target pseudo-labels. Therefore, it proposes a federated adaptive learning framework called Co-MDA, called CO-MDA. CO-MDA changes the label noise learning section into a semi-supervised learning approach and proposes a Co2-Learning strategy. This strategy involves training two networks at the same time that filter each others errors through epoch-level co-teaching [236], and gradually co-guess the pseudo-labels with the outputs of both target networks to further reduce the impact of label noise.  Model clustering FedDL [170], FedAMP [104], and HYPCLUSTER [44] use model-related information (such as model parameters, convolution layer channels, LSTM hidden states, and neurons in fully connected layers) to construct a shared global model based on model clustering method. However, these methods require several communication rounds to separate all inconsistent participants, potentially affecting computational and communication efficiency. Therefore, the study [168] proposes a method FedMA to achieve hierarchical clustering of participants with a single round of communication. This method uses the difference between the initial global model parameters and local model parameters to generate multiple sub-clusters. Then, by calculating the pairwise distances between participants within all sub-clusters, similar sub- clusters are iteratively merged until only a single cluster remains, containing all samples. Furthermore, similar to study [127] in addressing prior shift issue, study [160] treats the multi-center participant clustering issue as an optimization problem, which can be effectively resolved using the expectation-maximization (EM) algorithm. In addition, different from traditional federated clustering methods, which associate each participants data distribution with only one cluster distribution (known as hard clustering), the study [159] introduces a soft-clustering-based FL paradigm, called FedSoft. FedSoft allows each local dataset to follow a mixture of multiple cluster distributions, improving the training of high-quality local and cluster models.  Model selection Model selection, a classic transfer learning method, has seen widespread use in FTL, either on its own or in combination with other methods, and it is equally effective in addressing covariate shift issues of FTL. For example, CMFL [96] compares the local update of each participant with the global update during each iteration of learning by calculating the proportion of parameters in the local update that have opposite signs to those in the global update, which aims to assess the degree of alignment between the two sets of gradients. A higher proportion indicates a greater deviation from the direction of joint convergence, rendering the local update less relevant. CMFL thus selectively excludes such divergent local updates from being uploaded, effectively minimizing communication costs in FL while ensuring convergence can still be significantly achieved. 3.1.3 Feature concept shift  label concept shift To mitigate feature concept shift challenges in FTL, study [169] utilizes an iterative federated hierarchical clustering algorithm, called IFCA. Different from traditional methods, IFCA does not require centralized clustering algorithms. The server only plays a role in average model parameters, which substantially decreases the servers computational load. However, IFCA needs to run a federated stochastic gradient descent (SGD) algorithm in each round until it converges. This process could increase computational and communication efficiency in large-scale FL systems. Regarding the label concept shift issue, current studies address it from the perspectives of feature alignment [150] or model clustering methods [89,207]. Feddg [150] uses the amplitude spectrum in the frequency domain as data distribution information and exchanges it among participants. The goal is that each participant can fully utilize multi-source data distribution information to learn parameters with higher generalization, which proves equally effective under covariate shift. In addition, considering that Bayesian optimization is a powerful surrogate-assisted algorithm for solving label concept shift issues in FL and black-box optimization problems where the local model-related information is not visible to other participants for privacy. Some researchers have turned their attention to federated Bayesian optimization [108,162,206,207]. However, these methods either have all participants work together on the same task, or make only one participant learn from others to tackle a specific task. However, in real life, the tasks of participants are often related to each other. the study [207] introduces an efficient federated multi-task Bayesian optimization framework, called FMTBO, which dynamically aggregates multi-task models based on a dissimilarity matrix derived from predictive rankings. Additionally, FMTBO designs a federated ensemble acquisition function that effectively searches for the best solution by using predictions from both global and local hyperparameters, enhancing the generalization of the global model. Besides, pFEDVEM [89] introduces an FL framework based on Bayesian models and latent variables, and combines the model weighting strategy to mitigate label concept shift issues. In this setup, a hidden shared model identifies common Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 19 patterns among local models, meanwhile, local models adapt to their specific environments using the information from the shared model, which determines the confidence levels of each participant. The confidence levels are then used to set the weights when combining local models. The extensive experiments have demonstrated that pFEDVEM robustly addresses three types of distribution shift issues including prior shift, covariate shift, and label concept shift, and obtains significant accuracy improvement compared to the baselines. 3.1.4 Quantity shift FEDMIX [61] has proven that the instance enhancement method is equally effective for quantity shift problems. Moreover, studies [161,171] view FL as a hedonic game, where each participant produces some cost (error) when joining in the FL process. Theres a Nash equilibrium between minimizing individual errors and overall errors. For example, a school may aim to minimize its local error, while a region or city may aim to minimize the overall error. Study [171] proposes to find a relatively stable participant partition by accurately estimating the expected error of each participant, which may overlook the need to minimize the overall error. Additionally, the numbers of samples for participants in [171] are only assumed to be small or large, Different from it, study [161] not only more focuses on overall social well-being, but also is suitable for any number of participants with any various numbers of samples. 3.2 Heterogeneous federated transfer learning This section will discuss existing works addressing the challenges of heterogeneous FTL, dynamic heterogeneous FTL, and model adaptive FTL from data-based and model- based perspectives as shown in Fig. 6. However, it is worth noting that there is very little research on scenarios with heterogeneous label space and heterogeneous feature and label space, so we will not elaborate on it here. 3.2.1 Feature space heterogeneity A B XB com (A;B) B;A XB com (A;B) XB com (B;A) (A;B) (B;A) XB pr i XB com (A;B) XA pr i XB com (B;A) A B Heterogeneous feature spaces often occur in VFL, thus, we mainly focus on the VFL scenario for feature space heterogeneous FTL. To address this issue, researchers can utilize methods such as feature alignment [172,237,238] or feature concatenation [176] to construct new feature datasets for model training. Among them, feature alignment in VFL can be completed by constructing a novel feature subspace [237], or filling in missing or incomplete features of each participants feature spaces [172]. These approaches enable knowledge transfer under a homogeneous feature space. Specifically, the study [172] assumes an inconsistency in the feature spaces between two participants, the active participant and the passive participant . Both of them map their features using their respective mapping functions and to the same feature space, resulting in new feature representations and . They optimize the mapping functions and by minimizing the similarity between the private features and , as well as and . Finally, participants and , through secure bilateral computation, obtain the complete XA b XB a features and respectively. Based on this, the federated aggregation can be implemented under the aligned feature space. However, these methods rely on the existing feature space of participants, ignoring the relationships among these features. By combining feature clustering methods, the active participant can create new, more valuable feature space for knowledge transfer. For example, study [173] proposes a VFL paradigm based on feature space decomposition clustering, called PrADA. The specific steps include: C p C q h h  p q 1. Feature grouping: participant applies domain expertise to divide raw features into groups, each containing tightly related features. Moreover, participant constructs interactive feature among pairs of feature groups, resulting in a total of feature groups (where ). B C fE  ffE;ig i  1 h 2. Pretraining stage: this stage involves collaborative efforts between source participant and participant , to train a set of feature extractors ( for to ) that are capable of learning features which are both invariant across domains and discriminative for labels. A C A C fE  ffE;ig i  1 h 3. Fine-tuning stage: this process is executed in collaboration between active participant and participant , with the goal of training participant s target label predictor by utilizing the pre-trained set of feature extractors ( , where to ). In addition, PrADA enhances privacy and security using a secure protocol based on partial homomorphic encryption. However, not all features of the participants are relevant to the task. Therefore, the novel feature space generated by aggregating the features of all parties needs to filter features that are not relevant to the task through feature selection. However, current feature selection techniques [232,239] in distribution learning, often need numerous training iterations, particularly when dealing with high-dimensional data. Directly applying them in FTL to solve feature space heterogeneous issues produces significant computational and communication overhead, as each training round involves multiple encryptions, decryption operations, and intermediate parameter transfers. For example, study [179] suggests using an embedded method to combine autoencoders with L2 constraints on feature weights for feature selection, and sets a threshold for post-training to determine the selected features for mitigating the problem of model parameter shrinkage [240]. Different from previous VFL research scenarios where there were mostly two participants and binary classification tasks, study [178] proposes an FL feature selection scheme suitable for multi-participant multi-classification. In addition, previous studies that mainly focus on the relationship between features and labels [180], ignoring the relationship between features, to solve this problem, similar to research [37,175] using MI theory into federated feature selection in HFL, study [174] proposes a feature selection VFL framework based on conditional mutual information, called FEAST. FEAST integrates feature information into a single statistical variable for FL transmission, which not only accomplishes key feature selection and further reduces communication costs while 20 Front. Comput. Sci., 2024, 18(6): 186356 ensuring privacy and security. In addition, study [181] first proposes a theoretically verifiable feature selection method, formalizing the feature selection problem in the VFL environment, and providing a theoretical framework to prove that unimportant features have been removed. 3.3 Dynamic heterogeneous FTL 3.3.1 System heterogeneity System heterogeneity among participants could lead to the emergence of stragglers in each iteration. To address this problem, instance selection can be leveraged by researchers to mitigate the computational burden of participants when they have heterogeneous local computational resources. However, it could lead to decreased model performance due to the reduced statistical utility of the training dataset. Study [198] obtains the optimal data selection scheme through an optimization function that includes lower and upper limits of resources, as well as arbitrary, non-decreasing cost functions per resource, meanwhile, it treats this problem as a scheduling problem of tasks assignment to resources, seeking to maximize the number of participants in each round of FL updates. In addition, FedBalancer [197] chooses samples for training by measuring their statistical utility, derived from the sample loss list based on the latest model. However, it is inefficient to wait for every participant to finish local training before proceeding with aggregation due to system heterogeneity. Thus, under a constant FL round deadline setting, instance selection could not immediately enhance the time-to-accuracy ratio. Furthermore, model-based strategies, such as consistency regularization [202], model selection [1], model clustering [55,97,98,145], parameter decoupling [106], parameter pruning [85] can also be applied as effective ways to address the straggler issue in FL. For example, Fedavg [1] directly drops models of these stragglers which can not accomplish local training in time when the other participants have completed the same amount of training. Based on this, FedProx [202] allows varying local training epochs across participants, tailored to each devices system capabilities. Subsequently, it aggregates the non-convergent updates submitted by stragglers, rather than dropping these less responsive participants from this iteration. However, these frameworks may come at the cost of sacrificing accuracy due to the omission of partial information and waiting for all participants to complete a uniform number of training epochs tends to extend the convergence time of FL. FedAT [106] blends synchronous and asynchronous updates by decoupling the model parameters at the layer level, which stratifies local models based on the time each participant needs to complete a round of training. During each training round, FedAT randomly selects some local models in each layer to calculate the loss gradient of local data, completing the synchronous update of models in that specific layer. Each layer, acting as a new training entity, then asynchronously updates the global model. The faster layers have shorter round-response delays, speeding up the convergence of the global model. The slower layers contribute to global training by asynchronously sending model updates to the server, which further improves the predictive performance of the model. Besides, [85,86,134] selectively use local models for transfer knowledge by parameter pruning methods under the limited computational resources. Study [145] introduces a FL framework, called FedHiSyn, which uses a resource-based hierarchical clustering approach. This framework first categorizes all available devices according to their computing capabilities. Given that a ring topology is more suitable for models with uniform resources, after local training, the models are sent to the server. Then, within their respective categories, they exchange local model weight updates based on the ring topology structure to mitigate the lag effect caused by system heterogeneity. Considering that the main challenge of dynamic heterogeneous FTL is the appropriate scheduling of participants, essentially a local model selection issue at each iteration. Some researchers [53,9496,136,137] assume the central party has 1-lookahead in source model selection strategies, which means that the dynamic input data beforehand is known. However, this can not be applied when dealing with unpredictable time series inputs. Thus, reinforcement learning has been increasingly used to design source model selection strategies [54,94,97,111,132]. Studies [97,98] propose FL paradigms based on the multi-armed bandit (MAB). In situations where the available computing resources of participants are unknown, these approaches calculate the difference between the data distribution of multiple combined participants and a class-balanced data distribution, and then pick local models for aggregation and assign weights based on these differences. In another study, Study [55] proposes a participant scheduling strategy by age of update (AoU) measurement. This strategy considers the age of the received parameters and the current channel quality at the same time, which improves efficiency and allows effective aggregation in federated joint learning. Except for the above-mentioned reinforcement learning methods in dynamic heterogeneous FTL, TiFL [99], a hierarchical federated learning framework, addresses system heterogeneity issues by grouping participants with similar training performance. Meanwhile, in each round of updates, TiFL adaptively selects local models for training within each group by simultaneously optimizing accuracy and training time. To accelerate model convergence under system heterogeneity, FedSAE [100] suggests choosing participants with larger local training losses to take part in aggregation during each training round. FedSAE also designs a mechanism to predict each participants maximum tolerable workload, aiming to dynamically adjust their local training rounds. Research [205] found that differences in bit-width among participant devices can impact the performance of the global model. Low-bit-width models are more compatible with hardware but could limit the models generalization, leading to poor performance of high-bit-width models. To tackle this, the study [205] introduces ProWD, a FL framework that considers bit-width heterogeneity. This framework selects sparse sub- weights compatible with full-precision model weights from the low-bit-width models received by the server. These selected sub-weights then participate in central aggregation along with the full-precision model weights. Another study Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 21 [101] proposes a FL framework, called Aergia, which freezes the most computation-heavy parts of the model and trains the unfrozen parts. Moreover, the server chooses a more reasonable offloading solution based on the training speed reported by each participant and the similarity between their datasets. In this way, the training of the frozen parts can be offloaded to participants with ample resources or faster training speeds. PyramidFL [147] is a fine-grained participant selection, which considers not only the distribution and system heterogeneity between the selected and non-selected participants but also within the selected participants themselves. Specifically, the server uses feedback from past training rounds to rank participants based on their importance, participants then use their rank to determine the number of iterations for data efficiency and the parameters to drop for system efficiency. Furthermore, the utility of each participant isnt static but varies across training rounds. If a participant is selected, its data utility will then decrease since these data have been seen by the model. Thus, reducing the likelihood of selection in subsequent training rounds allows participants who were not selected previously to have a higher probability of being chosen, which further improves the model performance under the dynamic heterogeneity and further enhances the fairness of participant selection. 3.3.2 Incremental heterogeneity T(t)i ui t T(t 1)j uj t 1 uj ui Existing FTL strategies mainly focus on model-based techniques, for example, GLFC [102] addresses the continuous emergence of new classes in federated online learning by consistency regularization method. It introduces a class-aware gradient compensation loss to ensure the consistency of the learning pace for new classes with the forgetting pace of old classes. It separately normalizes the gradients for new and old classes and reweights them for the local optimization goal, where the relationships between classes are obtained based on the best old classification model from the previous tasks. As the local data or tasks increase, where the tasks may be a new batch of data, the task learned by participant in round could be similar or related to the task learned by participant in round . In this situation, the transmission of aggregated global information among participants can facilitate knowledge transfer across participants. Nonetheless, when a new joined task in participant is irrelevant to the tasks of participant , it could affect the optimization direction of the local model, leading to a decrease in accuracy. To mitigate this issue, each participant selectively utilizes only the knowledge of the relevant tasks that have been trained on other participants during each iteration, while ignoring as much as possible the knowledge of irrelevant tasks that may interfere with local learning. Thus, parameter decomposition and model weighting methods have attracted the attention of researchers [110]. For example, FedWeIT [110] solves this problem by decomposing parameters into three different types for training: global parameters that capture the global and generic knowledge across all participants, local base parameters that capture generic knowledge for each participant, and task-adaptive parameters for each specific task per participant. Meanwhile, FedWeIT applies sparse masks to select parameters relevant to a given task, minimizing interference from irrelevant tasks of other participants and allocating attention to the servers aggregated parameters to selectively filter parameter information. However, all these methods lack a theoretical basis for ensuring convergence. FedL [56] uses dynamic adaptation to measure the extent to which online decision constraints are breached and calculates a maximum limit for this measure, which guarantees that the expected contribution of the chosen source model to the FL models performance matches its actual contribution. 3.4 Model adaptive FTL Q R R Q X R Model adaptive FTL is often caused by model heterogeneity, i.e., the local models of different participants may be inconsistent in architecture, which could cause incompatibility in the feature dimension and representational capacity among participants [115]. It means that the average aggregation approach based on consistent features can not be used directly in FL. To mitigate this issue, data-based strategies are proposed in FTL. For example, the study [200] proposes to apply a feature mapping method to obtain consistent representation space and complete FL. Considering that even with different model structures, they possess some common knowledge for the same input, i.e., the feature extraction layers generate similar feature maps, research [200] uses model drafts to align local data distributions of participants. These outputs from specific layers or models are interpreted as blurred images of data and defined as model drafts. By minimizing the similarity difference between the local and global drafts, the data distribution difference between participants can be reduced. Except for feature mapping [7,126] using explicit features, some implicit features can be utilized to align the source and target domains, facilitating knowledge transfer within this aligned space [7], called feature alignment. Implicit features include subspace attributes [201], spectral characteristics [150], prototype graphs [66]. For example, the study [66] uses prototypes to effectively transfer information under the model adaptive FTL by minimizing local and global prototype graphs within the same feature space, thereby capturing the semantic information of class structures. It avoids the possibility of data from different classes (across various participants) merging into a single class, or data from the same class being spread across multiple classes. Similarly, FedHeNN [103] is a FL framework based on instance-level representation. Each participant randomly selects part of local data and obtains instance-level representation to guide local training. By introducing a distance function based on centralized kernel alignment as a proximal term of the local loss function, it aligns local and global model representations, enabling federated learning across heterogeneous models. FedFoA [201] adds a linear calibration layer at the end of each local model to first calibrate the different feature dimensions among participants to the same dimension space. Participants use decomposition to obtain feature subspace and feature correlation matrix . The central server minimizes the reconstruction of local features and the product of global 22 Front. Comput. Sci., 2024, 18(6): 186356 Q Q and initial vector to get the optimal , guiding the update of the local sub-feature space. Inconsistent model architectures also make the simple average aggregation of model parameters ineffective. From the model-based perspective, studies [87,156] implement FedAvg on top of local sub-networks by parameter decoupling. [87] assumes that the model architectures of participants can dynamically change with each iteration, and further proposes to leverage parameter decoupling to obtain at least one fixed sub-network for each type of heterogeneous situation and aggregate them into a single global model. Thus, smaller local models can gain more from global aggregation by performing less global aggregation on a subset of the parameters from larger local models. Similarly, study [84] combines parameter decoupling with model clustering method to group local models based on the similarity of their personalized sub-networks, maximizing the level of knowledge sharing between participants. Besides, research [144] designed a Mapper at the local level to convert feature representations from different semantic spaces to the same feature space, and accomplish the knowledge transfer based on knowledge distillation (KD). They deploy a global generator at the server to extract global data distribution information and distill it into the local model of each participant. Then, local models are viewed as discriminators to reduce the difference between global and local data distributions in heterogeneous feature spaces. Since the feature representations synthesized by the global generator are usually more faithful and homogeneous to the global data distribution, they can achieve faster and better convergence. Additionally, local generators also can be used to enhance hard-to-judge sample data, improving model performance [144]. In real-world scenarios, cross-institutional FL is often more content with the VFL scenario. However, traditional VFL can only benefit from samples shared among multiple parties, which severely limits its application. Therefore, research [115] proposes a VFL framework based on representation distillation, called VFedTrans. This framework collaboratively models common features among multiple parties and extracts federated representations of shared samples, aiming to maximize data utility as much as possible through KD. Knowledge distillation is also often used for dealing with model adaptive FTL induced by model heterogeneity as shown in Table 5. FedMD [163] uses KD for federated learning in situations where different models are used. Instead of just combining model parameters, FedMD calculates class scores for each participant using a shared dataset. These scores are then sent to a server to calculate an average, which guides the training of local models. This method allows for knowledge sharing while keeping private data and model structures secure, and it works even when different local models are used. Contrary to the assumption that participants local models are entirely different, research [119] assumes that local models of participants are not fully heterogeneous, and there are cases where some models share the same structure. Therefore, they propose an FL framework based on ensemble distillation, called FedDF. It creates several prototype models, which represent participants with identical model structures. In each round, FedAvg is performed among participants with the same prototype model to initialize a global model (student model), followed by cross-architecture learning through knowledge distillation. In this process, the parameters of the local model (teacher model) are tested on an unlabeled public dataset to generate predictions for training each student model on the server. Research [113] adopts a federated communication strategy, denoted as FSFL, which is similar to FedMD, innovatively adding a latent embedding adaptive module to alleviate the impact of domain discrepancies between public and private datasets. However, these studies [32,33,57,114,119,139141,163,164,241] strongly rely on the construction of public datasets, which undoubtedly compromises data privacy and is operationally challenging in practice. The impact of the quality of these prerequisites on the performance of federated learning is also unknown [124]. Therefore, research [124] proposes an FL framework based on zero-shot knowledge distillation, called FedZKT. FedZKT requires no prerequisites for local data, and its distillation tasks are assigned to the server to reduce the local workload. In addition, some studies [29,115,142144,165] introduce generators to avoid the need for public datasets, enabling the aggregation of local information in a data-free manner. For example, FedFTG [143] uses the log-odds of each local model as a teacher to train a global generator and fine-tunes the global model using pseudo-data generated by the global generator. Due to the additional computational and communication costs imposed by the introduction of a generator, ScaleFL [134] proposes a self-distillation method based on exit predictions. This method treats self-distillation as an integral part of the local training process, requiring no extra overhead. ScaleFL enhances the knowledge flow among local sub-networks by minimizing the KullbackLeibler divergence (KL divergence) between early exits (students) and final predictions (teachers). Furthermore, the study [166] introduces a FL framework based on a data-free semantic collaborative distillation, called MCKD. This framework transfers soft predictions from local models to a server to learn representations that dont change across different domains. MCKD also introduces a knowledge filter to mitigate the potential amplification of irrelevant or malicious participants influences on the target domain by traditional averaging aggregation. This knowledge filter generates consensus knowledge for unlabeled data and sets a threshold to drop models where local model predicted classes are inconsistent with consensus classes, further adapting the central model to target data. However, most of these methods construct ensemble knowledge by merely averaging the soft predictions of multiple local models, overlooking that local models have a differential understanding of distillation samples. Research [138] suggests that a model is more likely to make the correct predictions when the samples are included in the domain used for the models training. Based on this, the study [138] proposes treating each participants local data as a specific domain and designs a domain-aware federated distillation method named DaFKD. DaFKD can recognize the importance of each model to the distillation samples. For a given distillation sample, if the local model has a significant Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 23 relevance factor with it, DaFKD assigns a higher weight to this local model. 3.5 Semi-supervised and unsupervised FTL The labeled data scarcity is a common scenario in both HFL and VFL scenarios. Data-based methods receive some attention in reality for SSFTL issues, some studies [120,121] utilize these methods to obtain valuable augmented data by strategically optimizing predictions or similarity calculations on these data. For example, SemiFed [120] uses both past local and global models to predict labels for local unlabeled data. When predictions have high confidence, the pseudo- labeled data is used to train as available data. Orchestra [121] uses a feature clustering method to obtain more available data for training. It refines local data clusters through interactions with the server, which helps find effective samples in unlabeled data, further increases sample size, and reduces distribution heterogeneity when labeled data is scarce. Different from them without considering the high-class imbalance of unlabeled data, based on the instance selection method, CBAFed [123] uses the empirical distribution of all training data from the last round of global communication to design category-balanced adaptive thresholds. That is, if the model uses more data for training in one class, the threshold for labeling unlabeled data in this class will increase, and vice versa. It aims to shrink the gap between local and global distributions by selecting a local training set, and further influence category distribution, preventing a decline in global model performance due to prior probability bias. Model-based strategies similarly show the effectiveness in addressing the SSFTL issue, study [50] introduces a framework based on the consistency regularization method, called FedMatch. This framework splits local model parameters into two parts, one for updating with labeled data, and another for unlabeled data. For the parameters associated with labeled data, the loss function solely includes cross- entropy loss, and the loss function related to unlabeled data follows as: minf T;U LT;U ( f T;U ) L2  L U 2 2 L1 U 1; L1 L2 U U  L where and regularization on aims to make sparse, while not drifting far from the current optimal parameter trained with labeled data. The first term differs from the studies [120,152], which not only uses the prediction results of unlabeled data and its augmented data to obtain cross-entropy loss but also considers pseudo-labels as real labels to obtain new category loss, making the model perform the same on original data and slightly perturbed data. Contrary to previous studies that assume the presence of both labeled and unlabeled data locally, research [50] describes a federated semi-supervised learning scenario where some participants have fully labeled data while others have only unlabeled data. To guide unlabeled participants learning by building the interaction between the learning at labeled and unlabeled participants [120,152], FedIRM [41] applies the intermediate output of the local model, which is trained by the labeled participants, to construct a category relationship matrix. By transmitting the relationship matrix of each labeled participant, it guides the unlabeled participants to learn their local relationship matrix. The formula is as follows: minf T;U (w)(LT;U ( f T;U ) LIRM); LIRM  1 z z j1 (LKL(RL j jjRU j ) LKL(RU j jjRL j )); Rj j where denotes the relation vector of class . Furthermore, researchers [67,128] also apply domain-dependent consistency regularization to solve this issue. For example, [67] uses the KL divergence between local and global model predictions to control the update of the local objective function. Additionally, other model-based strategies, such as parameter decoupling [50], model weighting [90,122,152], also attract some researchers attention. FedMatch [50] accomplishes the FTL under semi-supervised learning scenarios by training labeled and unlabeled data separately through model parameter decomposition. FedConsist [152] and RscFed [122] improve the performance of models in semi-supervised FTL by changing the weights of participants with labeled and unlabeled data. Since semi-supervised learning often deals with data that only has positive and unlabeled (PU) samples, one participants negative class could be made up of multiple positive classes from other participants. However, traditional PU learning mostly focuses on binary problems with only one kind of negative sample. Thus, study [90] assumes that the available data has multiple types of positive and negative classes (MPMM-PU), and further proposes to redefine the expected risk of MPMM-PU, which aims to decide each participants weight and examines the limits of the models generalization. In response to unsupervised FTL, a straightforward approach is to combine self-supervised methods with FL, such as [195,242]. However, this challenge is often accompanied by homogeneous or heterogeneous FTL issues. The feature- based strategies have obtained some attention to solve this problem, such as FedCA [199] based on feature selection, FSHFL [48] based on feature mapping, and FedFoA [201] based on feature augmentation. Moreover, studies also propose to alleviate the data drift issues between participants by model-based strategies, such as FedX [146] based on KD, and FedEMA [116] based on model interpolation. FedCA [199] shares features of local data and employs an auxiliary pubic dataset to minimize disparities in the representation space across participants. However, it ignores the inconsistency between the feature representation of local unlabeled data and global feature representation. To solve this problem, FSHFL [48], an FL framework based on an unsupervised federated feature selection approach, proposes the feature cleaning locally and global feature selection. The local feature cleaning utilizes an enhanced version of the one- class support vector machine (OCSVM) algorithm, called FAR-OCSVM, to identify features that lack sufficiently representative global features. The identification relies on local feature clustering, features within each cluster exhibit strong interrelationships, thus the clusters with more features contribute more significantly to the FTL process. Meanwhile, 24 Front. Comput. Sci., 2024, 18(6): 186356 the server selects global features from the collected local feature sets and then returns these global features to participants, directing them to select local features that are closest to the global representation. Considering that the public dataset has potential privacy leakage risk [199], FedX [146] incorporates local knowledge distillation and global knowledge distillation into the FedAvg [1] without any public data. Local knowledge distillation trains the network using the feature representations of local unlabeled datasets, and global knowledge distillation aims to mitigate data shift, which only relies on global model sharing. Besides, FedEMA [116] utilizes self-supervised learning methods with predictors, including MoCo and BYOL, to update local encoders through the EMA of the global encoder. 4 Application In this section, we explore and outline the prevalent applications where FTL makes a significant impact. 4.1 Federated cross-domain recommendation Cross-domain recommendation (CDR) aims to reduce data sparsity by transferring knowledge from a data-rich source domain to a target domain. However, this process presents significant challenges related to data privacy and knowledge transferability [243,244]. Therefore, federated learning is introduced to CDR to improve the performance of the target domain model while providing privacy protection. Currently, classic recommendation algorithms have been widely applied to federated learning, such as federated collaborative filtering [245,246], federated matrix factorization [247249], and federated graph neural networks [250]. However, these methods neglect the heterogeneity among them including data, resource, or model heterogeneity. Researchers have combined transfer learning techniques to accomplish tasks related to federated cross-domain recommendation. They utilize parameter sharing [251], parameter decoupling [252,253], and model clustering [254] to facilitate the process. 4.2 Federated medical image classification Medical data involving patient information is sensitive and its use is strictly regulated, limiting the application of current artificial intelligence technologies in the medical field. Federated learning, which trains models on local devices without sharing raw data, could protect patients data privacy security. However, data provided by different medical institutions, acting as source domains, often have heterogeneity in format, features, or distribution [255]. By integrating FL with transfer learning, we can leverage data from different healthcare institutions for model training, enhancing the models performance while preserving data privacy. This approach presents a promising direction for the application of artificial intelligence in the medical field. Common methods include federated knowledge distillation [139,256], federated weighting aggregation [204,257260], federated consistency regularization [261,262], federated model selection [263], federated model interpolation [264], federated model decoupling methods [265], federated model clustering [266], and feature clustering methods [267]. 4.3 Federated financial service Federated financial services include credit risk control [268272], stock prediction [273275], financial fraud transaction detection [276,277], etc. Among them, credit risk control is a standard procedure for financial institutions, which estimates whether an individual or entity is able to make future required payments [269]. Stock prediction allows economists, governors, and investors to model the market, manage the resources and enhance stock profits [273]. Fraudulent transaction detection is another difficult problem for individual banks to curb company financial losses and maintain positive customer relationships [276]. However, their predictive models all rely on large amounts of data to achieve training. Since the customers data or fraudulent transaction data can not be shared among different institutions, the prediction model of each financial institution often suffers from the limited sample issue, which significantly hinders the performance improvement of the model. The emergence of federated learning not only breaks this data silos problem but also can combine instance augmentation [271,275], instance selection [270], feature selection [268,269,277], feature alignment [237,238], parameter sharing [272], model weighting [273], model selection [274,276], knowledge distillation [272], and other technical approaches to achieve information exchange between institutions. 4.4 Federated traffic flow prediction Given that precise and up-to-date traffic flow data is of immense significance for traffic management, predicting traffic flow has emerged as a crucial element of intelligent transportation systems. However, current traffic flow prediction techniques, which rely on centralized machine learning, require the collection of raw data from mobile phones and cameras for model training, causing potential privacy issues. Researchers found that this issue also can be addressed using FTL, such as federated clustering aggregation [278283], federated weighting aggregation [284286], federated parameter sharing [285], and federated parameter control methods [283]. 5 Conclusion and future work This survey provides a systematic summary of federated transfer learning, which emerges from the combination of transfer learning with federated learning, offering the corresponding definitions, challenges, and strategies. For convenience for researchers, we compile settings for the most common data heterogeneity scenarios, including both homogeneous and heterogeneous federated transfers, and summarize some significant FTL studies for various challenges. In the future, the utility performance of FL models in more complex scenarios deserves further exploration, including label concept shift, feature concept shift, label space heterogeneity, and feature  label space heterogeneity. Due to differences or mutations in unknown or hidden relationships between input and output variables between participants, the concept shift becomes an important but less studied direction in FL. Meanwhile, label space heterogeneity and feature  Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 25 label space heterogeneity also deserve more in-depth study. Furthermore, although federated transfer learning uses model selection, weight aggregation and other transfer learning methods to solve the problems of data heterogeneity, dynamic heterogeneity and labeled data scarcity in FL, new privacy concerns may have also emerged about these strategy preferences. After participants crack the weighting strategy or selection mechanism used by the server to aggregate information from all parties, they may use unfair means to make the training of the global model develop in a direction more beneficial to themselves. Thus, possible leakage of strategy preferences should be properly considered when designing FTL strategies in the future. Finally, the communication costs, communication efficiency, and computational costs caused by these FTL strategies could be paid more attention by FL researchers. Compared with traditional FL methods, FTL strategies, such as instance augmentation, instance selection, feature selection, model selection, and model clustering, etc., increase additional computational costs and introduce more shared information among participants. Based on some traditional methods [287290] for enhancing computational or communication efficiency, FTL researchers can further optimize federated algorithms. In summary, FTL still has great potential research value in utility, privacy, and communication issues. Acknowledgements The research work was supported by the National Key RD Program of China (No. 2021ZD0113602), the National Natural Science Foundation of China (Grant Nos. 62176014 and 62202273), and the Fundamental Research Funds for the Central Universities. Competing interests The authors declare that they have no competing interests or financial conflicts to disclose. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the articles Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the articles Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:creativecommons.orglicenses by4.0. References McMahan B, Moore E, Ramage D, Hampson S, Arcas B A Y. Communication-efficient learning of deep networks from decentralized data. In: Proceedings of the 20th International Conference on Artificial Intelligence and Statistics. 2017, 12731282 1. Feng S, Li B, Yu H, Liu Y, Yang Q. Semi-supervised federated heterogeneous transfer learning. Knowledge-Based Systems, 2022, 252: 109384 2. Zhang C, Xie Y, Bai H, Yu B, Li W, Gao Y. A survey on federated learning. Knowledge-Based Systems, 2021, 216: 106775 3. Gao L, Fu H, Li L, Chen Y, Xu M, Xu C Z. FedDC: Federated learning with non-IID data via local drift decoupling and correction. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 1010210111 4. Shi Y, Zhang Y, Xiao Y, Niu L. Optimization strategies for client drift in federated learning: a review. Procedia Computer Science, 2022, 214: 11681173 5. Liu Y, Kang Y, Zou T, Pu Y, He Y, Ye X, Ouyang Y, Zhang Y Q, Yang Q. Vertical federated learning: concepts, advances and challenges. 2022, arXiv preprint arXiv: 2211.12814 6. Zhuang F, Qi Z, Duan K, Xi D, Zhu Y, Zhu H, Xiong H, He Q. A comprehensive survey on transfer learning. Proceedings of the IEEE, 2021, 109(1): 4376 7. Yang Q, Liu Y, Chen T, Tong Y. Federated machine learning: concept and applications. ACM Transactions on Intelligent Systems and Technology, 2019, 10(2): 12 8. Rahman K M J, Ahmed F, Akhter N, Hasan M, Amin R, Aziz K E, Islam A K M M, Mukta M S H, Islam A K M N. Challenges, applications and design aspects of federated learning: a survey. IEEE Access, 2021, 9: 124682124700 9. Liu J, Huang J, Zhou Y, Li X, Ji S, Xiong H, Dou D. From distributed machine learning to federated learning: a survey. Knowledge and Information Systems, 2022, 64(4): 885917 10. Li L, Fan Y, Lin K Y. A survey on federated learning. In: Proceedings of the16th IEEE International Conference on Control  Automation (ICCA). 2020, 791796 11. Zhan Y, Zhang J, Hong Z, Wu L, Li P, Guo S. A survey of incentive mechanism design for federated learning. IEEE Transactions on Emerging Topics in Computing, 2022, 10(2): 10351044 12. Yin X, Zhu Y, Hu J. A comprehensive survey of privacy-preserving federated learning: A taxonomy, review, and future directions. ACM Computing Surveys, 2022, 54(6): 131 13. Lyu L, Yu H, Yang Q. Threats to federated learning: a survey. 2020, arXiv preprint arXiv: 2003.02133 14. Pfitzner B, Steckhan N, Arnrich B. Federated learning in a medical context: a systematic literature review. ACM Transactions on Internet Technology, 2021, 21(2): 50 15. Nguyen D C, Pham Q V, Pathirana P N, Ding M, Seneviratne A, Lin Z, Dobre O, Hwang W J. Federated learning for smart healthcare: a survey. ACM Computing Surveys, 2023, 55(3): 60 16. Lim W Y B, Luong N C, Hoang D T, Jiao Y, Liang Y C, Yang Q, Niyato D, Miao C. Federated learning in mobile edge networks: a comprehensive survey. IEEE Communications Surveys  Tutorials, 2020, 22(3): 20312063 17. Nguyen D C, Ding M, Pathirana P N, Seneviratne A, Li J, Poor H V. . Federated learning for internet of things: a comprehensive survey. . IEEE Communications Surveys  Tutorials, 2021, 23(3): 16221658 18. Zhu H, Xu J, Liu S, Jin Y. Federated learning on non-IID data: a survey. Neurocomputing, 2021, 465: 371390 19. Tan A Z, Yu H, Cui L, Yang Q. Towards personalized federated learning. IEEE Transactions on Neural Networks and Learning Systems, 2023, 34(12): 95879603 20. Pan S J, Yang Q. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 2010, 22(10): 13451359 21. Konečný J, McMahan H B, Ramage D, Richtárik P. Federated optimization: distributed machine learning for on-device intelligence. 2016, arXiv preprint arXiv: 1610.02527 22. Long M, Wang J, Sun J, Yu P S. Domain invariant transfer kernel learning. IEEE Transactions on Knowledge and Data Engineering, 2015, 27(6): 15191532 23. Long M, Cao Y, Wang J, Jordan M I. Learning transferable features with deep adaptation networks. In: Proceedings of the 32nd International Conference on Machine Learning. 2015, 97105 24. Bengio Y. Deep learning of representations for unsupervised and transfer learning. In: Proceedings of 2011 International Conference on Unsupervised and Transfer Learning Workshop. 2011, 1737 25. Raina R, Battle A, Lee H, Packer B, Ng A Y. Self-taught learning: transfer learning from unlabeled data. In: Proceedings of the 24th 26. 26 Front. Comput. Sci., 2024, 18(6): 186356 International Conference on Machine Learning. 2007, 759766 Younis R, Fisichella M. FLY-SMOTE: re-balancing the non-IID IoTedge devices data in federated learning system. IEEE Access, 2022,10: 6509265102 27. Wu Q, Chen X, Zhou Z, Zhang J. FedHome: cloud-edge basedpersonalized federated learning for in-home health monitoring. IEEETransactions on Mobile Computing, 2022, 21(8): 28182832 28. Jeong E, Oh S, Kim H, Park J, Bennis M, Kim S L. Communication-efficient on-device machine learning: Federated distillation andaugmentation under non-IID private data. 2018, arXiv preprint arXiv: 1811.11479 29. Li A, Zhang L, Tan J, Qin Y, Wang J, Li X Y. Sample-level data selection for federated learning. In: Proceedings of the IEEE Conference on Computer Communications. 2021, 110 30. Chen D, Hu J, Tan V J, Wei X, Wu E. Elastic aggregation for federated optimization. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 1218712197 31. Chen H Y, Chao W L. Fedbe: Making Bayesian model ensemble applicable to federated learning. In: Proceedings of the 9th International Conference on Learning Representations. 2021 32. Seo H, Park J, Oh S, Bennis M, Kim S L. Federated knowledge distillation. 2020, arXiv preprint arXiv: 2011.02367 33. Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of 2012 IEEE Conference on Computer Vision and Pattern Recognition. 2012, 20662073 34. Peng X, Bai Q, Xia X, Huang Z, Saenko K, Wang B. Moment matching for multi-source domain adaptation. In: Proceedings of the IEEECVF International Conference on Computer Vision. 2019, 14061415 35. Kuznetsova A, Rom H, Alldrin N, Uijlings J, Krasin I, Pont-Tuset J, Kamali S, Popov S, Malloci M, Kolesnikov A, Duerig T, Ferrari V. The open images dataset V4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision, 2020, 128(7): 19561981 36. Cassara P, Gotta A, Valerio L. Federated feature selection for cyber- physical systems of systems. IEEE Transactions on Vehicular Technology, 2022, 71(9): 99379950 37. Darlow L N, Crowley E J, Antoniou A, Storkey A J. CINIC-10 is not ImageNet or CIFAR-10. 2018, arXiv preprint arXiv: 1810.03505 38. Liu Z, Luo P, Wang X, Tang X. Deep learning face attributes in the wild. In: Proceedings of the IEEE International Conference on Computer Vision. 2015, 37303738 39. Reddi S J, Charles Z, Zaheer M, Garrett Z, Rush K, Konečný J, Kumar S, McMahan H B. Adaptive federated optimization. In: Proceedings of the 9th International Conference on Learning Representations. 2021 40. Liu Q, Yang H, Dou Q, Heng P A. Federated semi-supervised medical image classification via inter-client relation matching. In: Proceedings of the 24th International Conference. 2021, 325335 41. Sattler F, Müller K R, Samek W. Clustered federated learning: model- agnostic distributed multitask optimization under privacy constraints. IEEE Transactions on Neural Networks and Learning Systems, 2021, 32(8): 37103722 42. Qayyum A, Ahmad K, Ahsan M A, Al-Fuqaha A, Qadir J. Collaborative federated learning for healthcare: multi-modal covid-19 diagnosis at the edge. IEEE Open Journal of the Computer Society, 2022, 3: 172184 43. Mansour Y, Mohri M, Ro J, Suresh A T. Three approaches for personalization with applications to federated learning. 2020, arXiv preprint arXiv: 2002.10619 44. Huang L, Shea A L, Qian H, Masurkar A, Deng H, Liu D. Patient clustering improves efficiency of federated machine learning to predict mortality and hospital stay time using distributed electronic medical records. Journal of Biomedical Informatics, 2019, 99: 103291 45. Ouyang X, Xie Z, Zhou J, Xing G, Huang J. ClusterFL: a clustering- based federated learning system for human activity recognition. ACM Transactions on Sensor Networks, 2023, 19(1): 17 46. Duan M, Liu D, Chen X, Liu R, Tan Y, Liang L. Self-balancing federated learning with global imbalanced data in mobile systems. IEEE Transactions on Parallel and Distributed Systems, 2021, 32(1): 5971 47. Zhang X, Mavromatics A, Vafeas A, Nejabati R, Simeonidou D. Federated feature selection for horizontal federated learning in IoT networks. IEEE Internet of Things Journal, 2023, 10(11): 1009510112 48. Hu Y, Zhang Y, Gao X, Gong D, Song X, Guo Y, Wang J. A federated feature selection algorithm based on particle swarm optimization under privacy protection. Knowledge-Based Systems, 2023, 260: 110122 49. Jeong W, Yoon J, Yang E, Hwang S J. Federated semi-supervised learning with inter-client consistency  disjoint learning. In: Proceedings of the 9th International Conference on Learning Representations. 2021 50. Li S, Zhou T, Tian X, Tao D. Learning to collaborate in decentralized learning of personalized models. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 97569765 51. Qi T, Wu F, Lyu L, Huang Y, Xie X. FedSampling: a better sampling strategy for federated learning. In: Proceedings of the 32nd International Joint Conference on Artificial Intelligence. 2023, 41544162 52. Chen M, Yang Z, Saad W, Yin C, Poor H V, Cui S. A joint learning and communications framework for federated learning over wireless networks. IEEE Transactions on Wireless Communications, 2021, 20(1): 269283 53. Deng Y, Lyu F, Ren J, Wu H, Zhou Y, Zhang Y, Shen X. AUCTION: automated and quality-aware client selection framework for efficient federated learning. IEEE Transactions on Parallel and Distributed Systems, 2022, 33(8): 19962009 54. Yang H H, Arafa A, Quek T Q S, Poor H V. Age-based scheduling policy for federated learning in mobile edge networks. In: Proceedings of 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2020, 87438747 55. Su L, Zhou R, Wang N, Fang G, Li Z. An online learning approach for client selection in federated edge learning under budget constraint. In: Proceedings of the 51st International Conference on Parallel Processing. 2022, 72 56. Wu C, Wu F, Lyu L, Huang Y, Xie X. Communication-efficient federated learning via knowledge distillation. Nature Communications, 2022, 13(1): 2032 57. Tuor T, Wang S, Ko B J, Liu C, Leung K K. Data selection for federated learning with relevant and irrelevant data at clients. 2020, arXiv preprint arXiv: 2001.08300 58. Duan M, Liu D, Ji X, Liu R, Liang L, Chen X, Tan Y. FedGroup: Efficient clustered federated learning via decomposed data-driven measure. 2020, arXiv preprint arXiv: 2010.06870 59. Nagalapatti L, Narayanam R. Game of gradients: mitigating irrelevant clients in federated learning. In: Proceedings of the 35th AAAI Conference on Artificial Intelligence. 2021, 90469054 60. Yoon T, Shin S, Hwang S J, Yang E. FedMix: approximation of Mixup under mean augmented federated learning. In: Proceedings of the 9th International Conference on Learning Representations. 2021 61. Hao W, El-Khamy M, Lee J, Zhang J, Liang K J, Chen C, Carin L. Towards fair federated learning with zero-shot data augmentation. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2021, 33053314 62. Liang P P, Liu T, Liu Z, Allen N B, Auerbach R P, Brent D,63. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 27 Salakhutdinov R, Morency L P. Think locally, act globally: federated learning with local and global representations. 2020, arXiv preprint arXiv: 2001.01523 Briggs C, Fan Z, Andras P. Federated learning with hierarchical clustering of local updates to improve training on non-IID data. In: Proceedings of 2020 International Joint Conference on Neural Networks (IJCNN). 2020, 19 64. Liu B, Guo Y, Chen X. PFA: privacy-preserving federated adaptation for effective model personalization. In: Proceedings of the Web Conference 2021. 2021, 923934 65. Tan Y, Long G, Liu L, Zhou T, Lu Q, Jiang J, Zhang C. FedProto: federated prototype learning across heterogeneous clients. In: Proceedings of the 36th AAAI Conference on Artificial Intelligence. 2022, 84328440 66. Shen T, Zhang J, Jia X, Zhang F, Huang G, Zhou P, Kuang K, Wu F, Wu C. Federated mutual learning. 2020, arXiv preprint arXiv: 2006.16765 67. Arivazhagan M G, Aggarwal V, Singh A K, Choudhary S. Federated learning with personalization layers. 2019, arXiv preprint arXiv: 1912.00818 68. Fallah A, Mokhtari A, Ozdaglar A. Personalized federated learning with theoretical guarantees: a model-agnostic meta-learning approach. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 300 69. Li T, Sahu A K, Talwalkar A, Smith V. Federated learning: challenges, methods, and future directions. IEEE Signal Processing Magazine, 2020, 37(3): 5060 70. Deng Y, Kamani M M, Mahdavi M. Adaptive personalized federated learning. 2020, arXiv preprint arXiv: 2003.13461 71. Dinh C T, Tran N H, Nguyen T D. Personalized federated learning with Moreau envelopes. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 1796 72. Hanzely F, Richtárik P. Federated learning of a mixture of global and local models. 2020, arXiv preprint arXiv: 2002.05516 73. Dinh C T, Tran N H, Nguyen T D, Bao W, Zomaya A Y, Zhou B B. Federated learning with proximal stochastic variance reduced gradient algorithms. In: Proceedings of the 49th International Conference on Parallel Processing. 2020, 48 74. Hanzely F, Hanzely S, Horváth S, Richtárik P. Lower bounds and optimal algorithms for personalized federated learning. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 194 75. Li T, Hu S, Beirami A, Smith V. Ditto: fair and robust federated learning through personalization. In: Proceedings of the 38th International Conference on Machine Learning. 2021, 63576368 76. Karimireddy S P, Kale S, Mohri M, Reddi S J, Stich S U, Suresh A T. SCAFFOLD: stochastic controlled averaging for federated learning. In: Proceedings of the 37th International Conference on Machine Learning. 2020, 476 77. Ma Z, Zhao M, Cai X, Jia Z. Fast-convergent federated learning with class-weighted aggregation. Journal of Systems Architecture, 2021, 117: 102125 78. Collins L, Hassani H, Mokhtari A, Shakkottai S. Exploiting shared representations for personalized federated learning. In: Proceedings of the 38th International Conference on Machine Learning. 2021, 20892099 79. Oh J, Kim S, Yun S Y. FedBABU: towards enhanced representation for federated image classification. 2021, arXiv preprint arXiv: 2106.06042 80. Jang J, Ha H, Jung D, Yoon S. FedClassAvg: local representation learning for personalized federated learning on heterogeneous neural networks. In: Proceedings of the 51st International Conference on 81. Parallel Processing. 2022, 76 Zhuang W, Gan X, Wen Y, Zhang S, Yi S. Collaborative unsupervised visual representation learning from decentralized data. In: Proceedings of the IEEECVF International Conference on Computer Vision. 2021, 48924901 82. Qu Z, Li X, Han X, Duan R, Shen C, Chen L. How to prevent the poor performance clients for personalized federated learning? In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 1216712176 83. Wang K, He Q, Chen F, Chen C, Huang F, Jin H, Yang Y. FlexiFed: personalized federated learning for edge clients with heterogeneous model architectures. In: Proceedings of the ACM Web Conference 2023. 2023, 29792990 84. Li A, Sun J, Zeng X, Zhang M, Li H, Chen Y. FedMask: joint computation and communication-efficient personalized federated learning via heterogeneous masking. In: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems. 2021, 4255 85. Yang Z, Sun Q. Personalized heterogeneity-aware federated search towards better accuracy and energy efficiency. In: Proceedings of the 41st IEEEACM International Conference on Computer Aided Design. 2022, 19 86. Diao E, Ding J, Tarokh V. HeteroFL: computation and communication efficient federated learning for heterogeneous clients. In: Proceedings of the 9th International Conference on Learning Representations. 2020 87. Zeng H, Zhou T, Guo Y, Cai Z, Liu F. FedCav: contribution-aware model aggregation on distributed heterogeneous data in federated learning. In: Proceedings of the 50th International Conference on Parallel Processing. 2021, 75 88. Zhu J, Ma X, Blaschko M B. Confidence-aware personalized federated learning via variational expectation maximization. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 2454224551 89. Lin X, Chen H, Xu Y, Xu C, Gui X, Deng Y, Wang Y. Federated learning with positive and unlabeled data. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1334413355 90. Beilharz J, Pfftzner B, Schmid R, Geppert P, Arnrich B, Polze A. Implicit model specialization through dag-based decentralized federated learning. In: Proceedings of the 22nd International Middleware Conference. 2021, 310322 91. Zhang M, Sapra K, Fidler S, Yeung S, Álvarez J M. Personalized federated learning with first order model optimization. In: Proceedings of the 9th International Conference on Learning Representations. 2021 92. Liu J, Wu J, Chen J, Hu M, Zhou Y, Wu D. FedDWA: personalized federated learning with dynamic weight adjustment. In: Proceedings of the 32nd International Joint Conference on Artificial Intelligence. 2023, 39934001 93. Wang H, Kaplan Z, Niu D, Li B. Optimizing federated learning on Non-IID data with reinforcement learning. In: Proceedings of the IEEE Conference on Computer Communications. 2020, 16981707 94. Nishio T, Yonetani R. Client selection for federated learning with heterogeneous resources in mobile edge. In: Proceedings of 2019 IEEE International Conference on Communications (ICC). 2019, 17 95. Wang L, Wang W, Li B. CMFL: mitigating communication overhead for federated learning. In: Proceedings of the 39th International Conference on Distributed Computing Systems (ICDCS). 2019, 954964 96. Xia W, Quek T Q S, Guo K, Wen W, Yang H H, Zhu H. Multi-armed bandit-based client scheduling for federated learning. IEEE Transactions on Wireless Communications, 2020, 19(11): 71087123 97. Yang M, Wang X, Zhu H, Wang H, Qian H. Federated learning with class imbalance reduction. In: Proceedings of the 29th European Signal Processing Conference (EUSIPCO). 2021, 21742178 98. 28 Front. Comput. Sci., 2024, 18(6): 186356 Chai Z, Ali A, Zawad S, Truex S, Anwar A, Baracaldo N, Zhou Y, Ludwig H, Yan F, Cheng Y. TiFL: a tier-based federated learning system. In: Proceedings of the 29th International Symposium on High- Performance Parallel and Distributed Computing. 2020, 125136 99. Li L, Duan M, Liu D, Zhang Y, Ren A, Chen X, Tan Y, Wang C. FedSAE: a novel self-adaptive federated learning framework in heterogeneous systems. In: Proceedings of the International Joint Conference on Neural Networks. 2021, 110 100. Cox B, Chen L Y, Decouchant J. Aergia: leveraging heterogeneity in federated learning systems. In: Proceedings of the 23rd ACMIFIP International Middleware Conference. 2022, 107120 101. Dong J, Wang L, Fang Z, Sun G, Xu S, Wang X, Zhu Q. Federated class-incremental learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 1015410163 102. Makhija D, Han X, Ho N, Ghosh J. Architecture agnostic federated learning for neural networks. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1486014870 103. Huang Y, Chu L, Zhou Z, Wang L, Liu J, Pei J, Zhang Y. Personalized cross-silo federated learning on non-IID data. In: Proceedings of the 35th AAAI Conference on Artificial Intelligence. 2021, 78657873 104. Zhu S, Qi Q, Zhuang Z, Wang J, Sun H, Liao J. FedNKD: a dependable federated learning using fine-tuned random noise and knowledge distillation. In: Proceedings of 2022 International Conference on Multimedia Retrieval. 2022, 185193 105. Chai Z, Chen Y, Anwar A, Zhao L, Cheng Y, Rangwala H. FedAT: a high-performance and communication-efficient federated learning system with asynchronous tiers. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021, 117 106. Hu C, Liang H H, Han X M, Liu B A, Cheng D Z, Wang D. Spread: decentralized model aggregation for scalable federated learning. In: Proceedings of the 51st International Conference on Parallel Processing. 2022, 75 107. Zhang X, Li Y, Li W, Guo K, Shao Y. Personalized federated learning via variational Bayesian inference. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 2629326310 108. Nguyen N H, Le Nguyen P, Nguyen T D, Nguyen T T, Nguyen D L, Nguyen T H, Pham H H, Truong T N. FedDRL: deep reinforcement learning-based adaptive aggregation for non-IID data in federated learning. In: Proceedings of the 51st International Conference on Parallel Processing. 2022, 73 109. Yoon J, Jeong W, Lee G, Yang E, Hwang S J. Federated continual learning with weighted inter-client transfer. In: Proceedings of the 38th International Conference on Machine Learning. 2021, 1207312086 110. Qu Z, Duan R, Chen L, Xu J, Lu Z, Liu Y. Context-Aware online client selection for hierarchical federated learning. IEEE Transactions on Parallel and Distributed Systems, 2022, 33(12): 43534367 111. Marfoq O, Neglia G, Vidal R, Kameni L. Personalized federated learning through local memorization. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1507015092 112. Huang W, Ye M, Du B, Gao X. Few-shot model agnostic federated learning. In: Proceedings of the 30th ACM International Conference on Multimedia. 2022, 73097316 113. Itahara S, Nishio T, Koda Y, Morikura M, Yamamoto K. Distillation- based semi-supervised federated learning for communication-efficient collaborative training with non-IID private data. IEEE Transactions on Mobile Computing, 2023, 22(1): 191205 114. Zhang J, Guo S, Guo J, Zeng D, Zhou J, Zomaya A Y. Towards data- independent knowledge transfer in model-heterogeneous federated learning. IEEE Transactions on Computers, 2023, 72(10): 28882901 115. Zhuang W, Wen Y, Zhang S. Divergence-aware federated self-116. supervised learning. In: Proceedings of the 10th ACM International Conference on Multimedia. 2022 Wang Y, Xu H, Ali W, Li M, Zhou X, Shao J. FedFTHA: a fine-tuning and head aggregation method in federated learning. IEEE Internet of Things Journal, 2023, 10(14): 1274912762 117. Gong X, Sharma A, Karanam S, Wu Z, Chen T, Doermann D, Innanje A. Preserving privacy in federated learning with ensemble cross- domain knowledge distillation. In: Proceedings of the 36th AAAI Conference on Artificial Intelligence. 2022, 1189111899 118. Lin T, Kong L, Stich S U, Jaggi M. Ensemble distillation for robust model fusion in federated learning. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 198 119. Lin H, Lou J, Xiong L, Shahabi C. SemiFed: semi-supervised federated learning with consistency and pseudo-labeling. 2021, arXiv preprint arXiv: 2108.09412 120. Lubana E S, Tang C I, Kawsar F, Dick R P, Mathur A. Orchestra: unsupervised federated learning via globally consistent clustering. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1446114484 121. Liang X, Lin Y, Fu H, Zhu L, Li X. RSCfed: random sampling consensus federated semi-supervised learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 1014410153 122. Li M, Li Q, Wang Y. Class balanced adaptive pseudo labeling for federated semi-supervised learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 1629216301 123. Zhang L, Wu D, Yuan X. FedZKT: zero-shot knowledge transfer towards resource-constrained federated learning with heterogeneous on-device models. In: Proceedings of the 42nd International Conference on Distributed Computing Systems (ICDCS). 2022, 928938 124. Zhou T, Konukoglu E. FedFA: federated feature augmentation. In: Proceedings of the 11th International Conference on Learning Representations. 2023 125. Yue K, Jin R, Pilgrim R, Wong C W, Baron D, Dai H. Neural tangent kernel empowered federated learning. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 2578325803 126. Long G, Xie M, Shen T, Zhou T, Wang X, Jiang J. Multi-center federated learning: clients clustering for better personalization. World Wide Web, 2023, 26(1): 481500 127. Li Q, He B, Song D. Model-contrastive federated learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2021, 1070810717 128. Chen H Y, Chao W L. On bridging generic and personalized federated learning for image classification. In: Proceedings of the 10th International Conference on Learning Representations. 2022 129. Yao X, Sun L. Continual local training for better initialization of federated models. In: Proceedings of 2020 IEEE International Conference on Image Processing (ICIP). 2020, 17361740 130. Wang J, Liu Q, Liang H, Joshi G, Poor H V. Tackling the objective inconsistency problem in heterogeneous federated optimization. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 638 131. Yu S, Nguyen P, Abebe W, Qian W, Anwar A, Jannesari A. SPATL: salient parameter aggregation and transfer learning for heterogeneous federated learning. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2022, 114 132. Liu C, Yang Y, Cai X, Ding Y, Lu H. Completely heterogeneous federated learning. 2022, arXiv preprint arXiv: 2210.15865 133. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 29 Ilhan F, Su G, Liu L. ScaleFL: Resource-adaptive federated learning with heterogeneous clients. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 2453224541 134. Liu C, Lou C, Wang R, Xi A Y, Shen L, Yan J. Deep neural network fusion via graph matching with applications to model ensemble and federated learning. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1385713869 135. Cho Y J, Wang J, Joshi G. Client selection in federated learning: convergence analysis and power-of-choice selection strategies. 2020, arXiv preprint arXiv: 2010.01243 136. Huang T, Lin W, Wu W, He L, Li K, Zomaya A Y. An efficiency- boosting client selection scheme for federated learning with fairness guarantee. IEEE Transactions on Parallel and Distributed Systems, 2021, 32(7): 15521564 137. Wang H, Li Y, Xu W, Li R, Zhan Y, Zeng Z. DaFKD: domain-aware federated knowledge distillation. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 2041220421 138. Gong X, Song L, Vedula R, Sharma A, Zheng M, Planche B, Innanje A, Chen T, Yuan J, Doermann D, Wu Z Y. Federated learning with privacy-preserving ensemble attention distillation. IEEE Transactions on Medical Imaging, 2023, 42(7): 20572067 139. Li Q, He B, Song D. Practical one-shot federated learning for cross- silo setting. In: Proceedings of the 30th International Joint Conference on Artificial Intelligence. 2021, 14841490 140. Sattler F, Marban A, Rischke R, Samek W. Communication-efficient federated distillation. 2020, arXiv preprint arXiv: 2012.00632 141. Zhu Z, Hong J, Zhou J. Data-free knowledge distillation for heterogeneous federated learning. In: Proceedings of the 38th International Conference on Machine Learning. 2021, 1287812889 142. Zhang L, Shen L, Ding L, Tao D, Duan L Y. Fine-tuning global model via data-free knowledge distillation for non-IID federated learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 1016410173 143. Yang Y, Yang R, Peng H, Li Y, Li T, Liao Y, Zhou P. FedACK: federated adversarial contrastive knowledge distillation for cross- lingual and cross-model social bot detection. In: Proceedings of the ACM Web Conference 2023. 2023, 13141323 144. Li G, Hu Y, Zhang M, Liu J, Yin Q, Peng Y, Dou D. FedHiSyn: a hierarchical synchronous federated learning framework for resource and data heterogeneity. In: Proceedings of the 51st International Conference on Parallel Processing. 2022, 8 145. Han S, Park S, Wu F, Kim S, Wu C, Xie X, Cha M. FedX: unsupervised federated learning with cross knowledge distillation. In: Proceedings of the 17th European Conference on Computer Vision. 2022, 691707 146. Li C, Zeng X, Zhang M, Cao Z. PyramidFL: a fine-grained client selection framework for efficient federated learning. In: Proceedings of the 28th Annual International Conference on Mobile Computing and Networking. 2022, 158171 147. Zhang S, Li Z, Chen Q, Zheng W, Leng J, Guo M. Dubhe: towards data unbiasedness with homomorphic encryption in federated learning client selection. In: Proceedings of the 50th International Conference on Parallel Processing. 2021, 83 148. Chen H, Frikha A, Krompass D, Gu J, Tresp V. FRAug: tackling federated learning with non-IID features via representation augmentation. In: Proceedings of the IEEECVF International Conference on Computer Vision. 2023, 48264836 149. Liu Q, Chen C, Qin J, Dou Q, Heng P A. FedDG: federated domain generalization on medical image segmentation via episodic learning in continuous frequency space. In: Proceedings of the IEEECVF 150. Conference on Computer Vision and Pattern Recognition. 2021, 10131023 Li X, Jiang M, Zhang X, Kamp M, Dou Q. FedBN: federated learning on non-IID features via local batch normalization. In: Proceedings of the 9th International Conference on Learning Representations. 2021 151. Yang D, Xu Z, Li W, Myronenko A, Roth H R, Harmon S, Xu S, Turkbey B, Turkbey E, Wang X, Zhu W, Carrafiello G, Patella F, Cariati M, Obinata H, Mori H, Tamura K, An P, Wood B J, Xu D. Federated semi-supervised learning for COVID region segmentation in chest CT using multi-national data from China, Italy, Japan. Medical Image Analysis, 2021, 70: 101992 152. Wang H, Zhao H, Wang Y, Yu T, Gu J, Gao J. FedKC: federated knowledge composition for multilingual natural language understanding. In: Proceedings of the ACM Web Conference 2022. 2022, 18391850 153. Wang K, Mathews R, Kiddon C, Eichner H, Beaufays F, Ramage D. Federated evaluation of on-device personalization. 2019, arXiv preprint arXiv: 1910.10252 154. Pillutla K, Malik K, Mohamed A, Rabbat M G, Sanjabi M, Xiao L. Federated learning with partial model personalization. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1771617758 155. Li A, Sun J, Li P, Pu Y, Li H, Chen Y. Hermes: an efficient federated learning framework for heterogeneous mobile clients. In: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking. 2021, 420437 156. Zhuang W, Wen Y, Zhang S. Joint optimization in edge-cloud continuum for federated unsupervised person re-identification. In: Proceedings of the 29th ACM International Conference on Multimedia. 2021, 433441 157. Zhang R, Xu Q, Yao J, Zhang Y, Tian Q, Wang Y. Federated domain generalization with generalization adjustment. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 39543963 158. Ruan Y, Joe-Wong C. FedSoft: soft clustered federated learning with proximal local updating. In: Proceedings of the 36th AAAI Conference on Artificial Intelligence. 2022, 81248131 159. Xie H, Xiong L, Yang C. Federated node classification over graphs with latent link-type heterogeneity. In: Proceedings of the ACM Web Conference 2023. 2023, 556566 160. Donahue K, Kleinberg J M. Optimality and stability in federated learning: a game-theoretic approach. In: Proceedings of the International Conference on Neural Information Processing Systems. 2021, 12871298 161. Dai Z, Low B K H, Jaillet P. Federated Bayesian optimization via Thompson sampling. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 812 162. Li D, Wang J. FedMD: Heterogenous federated learning via model distillation. 2019, arXiv preprint arXiv: 1910.03581 163. Li Y, Zhou W, Wang H, Mi H, Hospedales T M. FedH2L: federated learning with model and statistical heterogeneity. 2021, arXiv preprint arXiv: 2101.11296 164. Wu Y, Kang Y, Luo J, He Y, Fan L, Pan R, Yang Q. FedCG: leverage conditional GAN for protecting privacy and maintaining competitive performance in federated learning. In: Proceedings of the 31st International Joint Conference on Artificial Intelligence. 2022, 23342340 165. Niu Z, Wang H, Sun H, Ouyang S, Chen Y W, Lin L. MCKD: Mutually collaborative knowledge distillation for federated domain adaptation and generalization. In: Proceedings of 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2023, 15 166. 30 Front. Comput. Sci., 2024, 18(6): 186356 Huang W, Ye M, Shi Z, Li H, Du B. Rethinking federated learning with domain shift: a prototype view. In: Proceedings of 2023 IEEECVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023, 1631216322 167. Wang H, Yurochkin M, Sun Y, Papailiopoulos D S, Khazaeni Y. Federated learning with matched averaging. In: Proceedings of the 8th International Conference on Learning Representations. 2020 168. Ghosh A, Chung J, Yin D, Ramchandran K. An efficient framework for clustered federated learning. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 1643 169. Tu L, Ouyang X, Zhou J, He Y, Xing G. FedDL: federated learning via dynamic layer sharing for human activity recognition. In: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems. 2021, 1528 170. Donahue K, Kleinberg J. Model-sharing games: Analyzing federated learning under voluntary participation. In: Proceedings of the 35th AAAI Conference on Artificial Intelligence. 2021, 53035311 171. Gao D, Liu Y, Huang A, Ju C, Yu H, Yang Q. Privacy-preserving heterogeneous federated transfer learning. In: Proceedings of 2019 IEEE International Conference on Big Data (Big Data). 2019, 25522559 172. Kang Y, He Y, Luo J, Fan T, Liu Y, Yang Q. Privacy-preserving federated adversarial domain adaptation over feature groups for interpretability. IEEE Transactions on Big Data, 2022, doi: 10.1109TBDATA.2022.3188292 173. Fu R, Wu Y, Xu Q, Zhang M. FEAST: a communication-efficient federated feature selection framework for relational data. Proceedings of the ACM on Management of Data, 2023, 1(1): 107 174. Banerjee S, Elmroth E, Bhuyan M. Fed-FiS: a novel information- theoretic federated feature selection for learning stability. In: Proceedings of the 28th International Conference on Neural Information Processing. 2021, 480487 175. Wu Z, Li Q, He B. Practical vertical federated learning with unsupervised representation learning. IEEE Transactions on Big Data, 2022 176. He Y, Kang Y, Zhao X, Luo J, Fan L, Han Y, Yang Q. A hybrid self- supervised learning framework for vertical federated learning. 2022, arXiv preprint arXiv: 2208.08934 177. Feng S, Yu H. Multi-participant multi-class vertical federated learning. 2020, arXiv preprint arXiv: 2001.11154 178. Feng S. Vertical federated learning-based feature selection with non- overlapping sample utilization. Expert Systems with Applications, 2022, 208: 118097 179. Jiang J, Burkhalter L, Fu F, Ding B, Du B, Hithnawi A, Li B, Zhang C. VF-PS: how to select important participants in vertical federated learning, efficiently and securely? In: Proceedings of the 36th International Conference on Neural Information Processing Systems. 2022, 152 180. Castiglia T, Zhou Y, Wang S, Kadhe S, Baracaldo N, Patterson S. LESS-VFL: Communication-efficient feature selection for vertical federated learning. In: Proceedings of the 40th International Conference on Machine Learning. 2023, 37573781 181. Kairouz P, McMahan H B, Avent B, Bellet A, Bennis M, et al. Advances and open problems in federated learning. Foundations and Trends in Machine Learning, 2021, 14(12): 1210 182. Chai Z, Fayyaz H, Fayyaz Z, Anwar A, Zhou Y, Baracaldo N, Ludwig H, Cheng Y. Towards taming the resource and data heterogeneity in federated learning. In: Proceedings of 2019 USENIX Conference on Operational Machine Learning. 2019, 1921 183. Ye M, Fang X, Du B, Yuen P C, Tao D. Heterogeneous federated learning: state-of-the-art and research challenges. ACM Computing 184. Surveys, 2024, 56(3): 79 Schlegel R, Kumar S, Rosnes E, Amat A G I. CodedPaddedFL and CodedSecAgg: Straggler mitigation and secure aggregation in federated learning. IEEE Transactions on Communications, 2023, 71(4): 20132027 185. You C, Xiang J, Su K, Zhang X, Dong S, Onofrey J, Staib L, Duncan J S. Incremental learning meets transfer learning: application to multi- site prostate MRI segmentation. In: Proceedings of the 3rd MICCAI Workshop on Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health. 2022, 316 186. Chen Y, Qin X, Wang J, Yu C, Gao W. FedHealth: a federated transfer learning framework for wearable healthcare. IEEE Intelligent Systems, 2020, 35(4): 8393 187. Ditzler G, Polikar R. Incremental learning of concept drift from streaming imbalanced data. IEEE Transactions on Knowledge and Data Engineering, 2013, 25(10): 22832301 188. Elwell R, Polikar R. Incremental learning of concept drift in nonstationary environments. IEEE Transactions on Neural Networks, 2011, 22(10): 15171531 189. Tan D S, Lin Y X, Hua K L. Incremental learning of multi-domain image-to-image translations. IEEE Transactions on Circuits and Systems for Video Technology, 2021, 31(4): 15261539 190. Huang Y, Bert C, Gomaa A, Fietkau R, Maier A, Putz F. A survey of incremental transfer learning: combining peer-to-peer federated learning and domain incremental learning for multicenter collaboration. 2023, arXiv preprint arXiv: 2309.17192 191. Tang J, Lin K Y, Li L. Using domain adaptation for incremental SVM classification of drift data. Mathematics, 2022, 10(19): 3579 192. Alam S, Liu L, Yan M, Zhang M. FedRolex: model-heterogeneous federated learning with rolling sub-model extraction. In: Proceedings of the 36th International Conference on Neural Information Processing Systems. 2022, 2152 193. Yu F, Zhang W, Qin Z, Xu Z, Wang D, Liu C, Tian Z, Chen X. Heterogeneous federated learning. 2020, arXiv preprint arXiv: 2008.06767 194. Jin Y, Wei X, Liu Y, Yang Q. Towards utilizing unlabeled data in federated learning: a survey and prospective. 2020, arXiv preprint arXiv: 2002.11545 195. Diao E, Ding J, Tarokh V. SemiFL: semi-supervised federated learning for unlabeled clients with alternate training. In: Proceedings of the 36th International Conference on Neural Information Processing Systems. 2022, 1299 196. Shin J, Li Y, Liu Y, Lee S J. FedBalancer: data and pace control for efficient federated learning on heterogeneous clients. In: Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services. 2022, 436449 197. Pilla L L. Optimal task assignment for heterogeneous federated learning devices. In: Proceedings of 2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS). 2021, 661670 198. Zhang F, Kuang K, You Z, Shen T, Xiao J, Zhang Y, Wu C, Zhuang Y, Li X. Federated unsupervised representation learning. 2020, arXiv preprint arXiv: 2010.08982 199. Liao Y, Ma L, Zhou B, Zhao X, Xie F. DraftFed: a draft-based personalized federated learning approach for heterogeneous convolutional neural networks. IEEE Transactions on Mobile Computing, 2024, 23(5): 39383949 200. Liu Y, Guo S, Zhang J, Zhou Q, Wang Y, Zhao X. Feature correlation- guided knowledge transfer for federated self-supervised learning. 2022, arXiv preprint arXiv: 2211.07364 201. Li T, Sahu A K, Zaheer M, Sanjabi M, Talwalkar A, Smith V. Federated optimization in heterogeneous networks. In: Proceedings of 202. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 31 the Machine Learning and Systems. 2020, 429450 Duan J H, Li W, Zou D, Li R, Lu S. Federated learning with data- agnostic distribution fusion. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 80748083 203. Liu X, Xi W, Li W, Xu D, Bai G, Zhao J. Co-MDA: federated multisource domain adaptation on black-box models. IEEE Transactions on Circuits and Systems for Video Technology, 2023, 33(12): 76587670 204. Yoon J, Park G, Jeong W, Hwang S J. Bitwidth heterogeneous federated learning with progressive weight dequantization. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 2555225565 205. Dai Z, Low B K H, Jaillet P. Differentially private federated Bayesian optimization with distributed exploration. In: Proceedings of the International Conference on Neural Information Processing Systems. 2021, 91259139 206. Zhu H, Wang X, Jin Y. Federated many-task Bayesian optimization. IEEE Transactions on Evolutionary Computation, 2023 207. Chawla N V, Bowyer K W, Hall L O, Kegelmeyer W P. SMOTE: synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 2002, 16: 321357 208. He H, Bai Y, Garcia E A, Li S. ADASYN: Adaptive synthetic sampling approach for imbalanced learning. In: Proceedings of 2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence). 2008, 13221328 209. Han H, Wang W Y, Mao B H. Borderline-smote: a new over-sampling method in imbalanced data sets learning. In: Proceedings of the International Conference on Intelligent Computing. 2005, 878887 210. Kubat M, Matwin S. Addressing the curse of imbalanced training sets: one-sided selection. In: Proceedings of the 14th International Conference on Machine Learning. 1997, 179186 211. Yen S J, Lee Y S. Cluster-based under-sampling approaches for imbalanced data distributions. Expert Systems with Applications, 2009, 36(3): 57185727 212. Tsai C F, Lin W C, Hu Y H, Yao G T. Under-sampling class imbalanced datasets by combining clustering analysis and instance selection. Information Sciences, 2019, 477: 4754 213. Zhang L, Li Y, Xiao X, Li X Y, Wang J, Zhou A, Li Q. CrowdBuy: privacy-friendly image dataset purchasing via crowdsourcing. In: Proceedings of the IEEE Conference on Computer Communications. 2018, 27352743 214. Li A, Zhang L, Qian J, Xiao X, Li X Y, Xie Y. TODQA: efficient task- oriented data quality assessment. In: Proceedings of the 15th International Conference on Mobile Ad-Hoc and Sensor Networks (MSN). 2019, 8188 215. Katharopoulos A, Fleuret F. Not all samples are created equal: deep learning with importance sampling. In: Proceedings of the 35th International Conference on Machine Learning. 2018, 25302539 216. Alain G, Lamb A, Sankar C, Courville A, Bengio Y. Variance reduction in SGD by distributed importance sampling. 2015, arXiv preprint arXiv: 1511.06481 217. Loshchilov I, Hutter F. Online batch selection for faster training of neural networks. 2015, arXiv preprint arXiv: 1511.06343 218. Schaul T, Quan J, Antonoglou I, Silver D. Prioritized experience replay. In: Proceedings of the 4th International Conference on Learning Representations. 2016 219. Wu C Y, Manmatha R, Smola A J, Krahenbuhl P. Sampling matters in deep embedding learning. In: Proceedings of the IEEE International Conference on Computer Vision. 2017, 28592867 220. Li K, Xiao C. CBFL: a communication-efficient federated learning framework from data redundancy perspective. IEEE Systems Journal, 221. 2022, 16(4): 55725583 Duan L, Tsang I W, Xu D, Chua T S. Domain adaptation from multiple sources via auxiliary classifiers. In: Proceedings of the 26th Annual International Conference on Machine Learning. 2009, 289296 222. Duan L, Xu D, Tsang I W H. Domain adaptation from multiple sources: a domain-dependent regularization approach. IEEE Transactions on Neural Networks and Learning Systems, 2012, 23(3): 504518 223. Zhuang F, Luo P, Xiong H, Xiong Y, He Q, Shi Z. Cross-domain learning from multiple sources: a consensus regularization perspective. IEEE Transactions on Knowledge and Data Engineering, 2010, 22(12): 16641678 224. Luo P, Zhuang F, Xiong H, Xiong Y, He Q. Transfer learning from multiple source domains via consensus regularization. In: Proceedings of the 17th ACM Conference on Information and Knowledge Management. 2008, 103112 225. Kirkpatrick J, Pascanu R, Rabinowitz N, Veness J, Desjardins G, Rusu A A, Milan K, Quan J, Ramalho T, Grabska-Barwinska A, Hassabis D, Clopath C, Kumaran D, Hadsell R. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences of the United States of America, 2017, 114(13): 35213526 226. Yu H, Zhang N, Deng S, Yuan Z, Jia Y, Chen H. The devil is the classifier: investigating long tail relation classification with decoupling analysis. 2020, arXiv preprint arXiv: 2009.07022 227. Kang B, Xie S, Rohrbach M, Yan Z, Gordo A, Feng J, Kalantidis Y. Decoupling representation and classifier for long-tailed recognition. In: Proceedings of the 8th International Conference on Learning Representations. 2020 228. Yosinski J, Clune J, Bengio Y, Lipson H. How transferable are features in deep neural networks? In: Proceedings of the 27th International Conference on Neural Information Processing Systems. 2014, 33203328 229. Devlin J, Chang M W, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language understanding. In: Proceedings of 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2019, 41714186 230. Li X, Huang K, Yang W, Wang S, Zhang Z. On the convergence of FedAvg on non-IID data. In: Proceedings of the 8th International Conference on Learning Representations. 2020 231. Yamada Y, Lindenbaum O, Negahban S, Kluger Y. Feature selection using stochastic gates. In: Proceedings of the 37th International Conference on Machine Learning. 2020, 987 232. Zhou K, Yang Y, Qiao Y, Xiang T. Domain generalization with mixstyle. In: Proceedings of the 9th International Conference on Learning Representations. 2021 233. Li Q, Huang J, Hu J, Gong S. Feature-distribution perturbation and calibration for generalized Reid. In: Proceedings of 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2024, 28802884 234. Huang X, Belongie S. Arbitrary style transfer in real-time with adaptive instance normalization. In: Proceedings of the IEEE International Conference on Computer Vision. 2017, 15101519 235. Han B, Yao Q, Yu X, Niu G, Xu M, Hu W, Tsang I W, Sugiyama M. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In: Proceedings of the 32nd International Conference on Neural Information Processing Systems. 2018, 85368546 236. Liu Y, Kang Y, Xing C, Chen T, Yang Q. A secure federated transfer learning framework. IEEE Intelligent Systems, 2020, 35(4): 7082 237. Yang H, He H, Zhang W, Cao X. FedSteg: a federated transfer learning framework for secure image steganalysis. IEEE Transactions 238. 32 Front. Comput. Sci., 2024, 18(6): 186356 on Network Science and Engineering, 2021, 8(2): 10841094 Li Y, Chen C Y, Wasserman W W. Deep feature selection: theory and application to identify enhancers and promoters. Journal of Computational Biology, 2016, 23(5): 322336 239. Louizos C, Welling M, Kingma D P. Learning sparse neural networks through L0 regularization. In: Proceedings of the 6th International Conference on Learning Representations. 2018 240. Chang H, Shejwalkar V, Shokri R, Houmansadr A. Cronus: robust and heterogeneous collaborative learning with black-box knowledge transfer. 2019, arXiv preprint arXiv: 1912.11279 241. Van Berlo B, Saeed A, Ozcelebi T. Towards federated unsupervised representation learning. In: Proceedings of the 3rd ACM International Workshop on Edge Systems, Analytics and Networking. 2020, 3136 242. Liu J, Zhao P, Zhuang F, Liu Y, Sheng V S, Xu J, Zhou X, Xiong H. Exploiting aesthetic preference in deep cross networks for cross- domain recommendation. In: Proceedings of the Web Conference 2020. 2020, 27682774 243. Li P, Tuzhilin A. DDTCDR: deep dual transfer cross domain recommendation. In: Proceedings of the 13th International Conference on Web Search and Data Mining. 2020, 331339 244. Ammad-Ud-Din M, Ivannikova E, Khan S A, Oyomno W, Fu Q, Tan K E, Flanagan A. Federated collaborative filtering for privacy- preserving personalized recommendation system. 2019, arXiv preprint arXiv: 1901.09888 245. Minto L, Haller M, Livshits B, Haddadi H. Stronger privacy for federated collaborative filtering with implicit feedback. In: Proceedings of the 15th ACM Conference on Recommender Systems. 2021, 342350 246. Chai D, Wang L, Chen K, Yang Q. Secure federated matrix factorization. IEEE Intelligent Systems, 2021, 36(5): 1120 247. Du Y, Zhou D, Xie Y, Shi J, Gong M. Federated matrix factorization for privacy-preserving recommender systems. Applied Soft Computing, 2021, 111: 107700 248. Li Z, Ding B, Zhang C, Li N, Zhou J. Federated matrix factorization with privacy guarantee. Proceedings of the VLDB Endowment, 2021, 15(4): 900913 249. Wu C, Wu F, Cao Y, Huang Y, Xie X. FedGNN: federated graph neural network for privacy-preserving recommendation. 2021, arXiv preprint arXiv: 2102.04925 250. Zhang C, Long G, Zhou T, Yan P, Zhang Z, Zhang C, Yang B. Dual personalization on federated recommendation. In: Proceedings of the 32nd International Joint Conference on Artificial Intelligence. 2023, 45584566 251. Wu J, Liu Q, Huang Z, Ning Y, Wang H, Chen E, Yi J, Zhou B. Hierarchical personalized federated learning for user modeling. In: Proceedings of the Web Conference 2021. 2021, 957968 252. Wu M, Li L, Chang T, Rigall E, Wang X, Xu C Z. FedCDR: federated cross-domain recommendation for privacy-preserving rating prediction. In: Proceedings of the 31st ACM International Conference on Information  Knowledge Management. 2022, 21792188 253. Luo S, Xiao Y, Song L. Personalized federated recommendation via joint representation learning, user clustering, and model adaptation. In: Proceedings of the 31st ACM International Conference on Information  Knowledge Management. 2022, 42894293 254. Kaissis G A, Makowski M R, Rückert D, Braren R F. Secure, privacy- preserving and federated machine learning in medical imaging. Nature Machine Intelligence, 2020, 2(6): 305311 255. Sui D, Chen Y, Zhao J, Jia Y, Xie Y, Sun W. FedED: federated learning via ensemble distillation for medical relation extraction. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020, 21182128 256. Silva S, Gutman B A, Romero E, Thompson P M, Altmann A, Lorenzi257. M. Federated learning in distributed medical databases: meta-analysis of large-scale subcortical brain data. In: Proceedings of the 16th IEEE international symposium on biomedical imaging (ISBI 2019). 2019, 270274 Jin H, Dai X, Xiao J, Li B, Li H, Zhang Y. Cross-cluster federated learning and blockchain for internet of medical things. IEEE Internet of Things Journal, 2021, 8(21): 1577615784 258. Xia Y, Yang D, Li W, Myronenko A, Xu D, Obinata H, Mori H, An P, Harmon S, Turkbey E, Turkbey B, Wood B, Patella F, Stellato E, Carrafiello G, Ierardi A, Yuille A, Roth H. Auto-FedAvg: learnable federated averaging for multi-institutional medical image segmentation. 2021, arXiv preprint arXiv: 2104.10195 259. Jiang M, Roth H R, Li W, Yang D, Zhao C, Nath V, Xu D, Dou Q, Xu Z. Fair federated medical image segmentation via client contribution estimation. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 1630216311 260. Acar D A E, Zhao Y, Navarro R M, Mattina M, Whatmough P N, Saligrama V. Federated learning based on dynamic regularization. In: Proceedings of the 9th International Conference on Learning Representations. 2021 261. Chen Z, Yang C, Zhu M, Peng Z, Yuan Y. Personalized retrogress- resilient federated learning toward imbalanced medical data. IEEE Transactions on Medical Imaging, 2022, 41(12): 36633674 262. Xu A, Li W, Guo P, Yang D, Roth H, Hatamizadeh A, Zhao C, Xu D, Huang H, Xu Z. Closing the generalization gap of cross-silo federated medical image segmentation. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 2083420843 263. Jiang M, Yang H, Cheng C, Dou Q. IOP-FL: Inside-outside personalization for federated medical image segmentation. IEEE Transactions on Medical Imaging, 2023, 42(7): 21062117 264. Wang J, Jin Y, Stoyanov D, Wang L. FedDP: dual personalization in federated medical image segmentation. IEEE Transactions on Medical Imaging, 2024, 43(1): 297308 265. Adnan M, Kalra S, Cresswell J C, Taylor G W, Tizhoosh H R. Federated learning and differential privacy for medical image analysis. Scientific Reports, 2022, 12(1): 1953 266. Wu Y, Zeng D, Wang Z, Shi Y, Hu J. Federated contrastive learning for volumetric medical image segmentation. In: Proceedings of the 24th International Conference on Medical Image Computing and Computer Assisted Intervention. 2021, 367377 267. Li Y, Wen G. Research and practice of financial credit risk management based on federated learning. Engineering Letters, 2023, 31(1): 268. Xu Z, Cheng J, Cheng L, Xu X, Bilal M. Mses credit risk assessment model based on federated learning and feature selection. Computers, Materials  Continua, 2023, 75(3): 55735595 269. Lee C M, Delgado Fernández J, Potenciano Menci S, Rieger A, Fridgen G. Federated learning for credit risk assessment. In: Proceedings of the 56th Hawaii International Conference on System Sciences. 2023, 386395 270. Yang W, Zhang Y, Ye K, Li L, Xu C Z. FFD: a federated learning based method for credit card fraud detection. In: Proceedings of the 8th International Congress on Big Data. 2019, 1832 271. Wang Z, Xiao J, Wang L, Yao J. A novel federated learning approach with knowledge transfer for credit scoring. Decision Support Systems, 2024, 177: 114084 272. Pourroostaei Ardakani S, Du N, Lin C, Yang J C, Bi Z, Chen L. A federated learning-enabled predictive analysis to forecast stock market trends. Journal of Ambient Intelligence and Humanized Computing, 2023, 14(4): 45294535 273. Yan Y, Yang G, Gao Y, Zang C, Chen J, Wang Q. Multi-participant274. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 33 vertical federated learning based time series prediction. In: Proceedings of the 8th International Conference on Computing and Artificial Intelligence. 2022, 165171 Shaheen M, Farooq M S, Umer T. Reduction in data imbalance for client-side training in federated learning for the prediction of stock market prices. Journal of Sensor and Actuator Networks, 2024, 13(1): 1 275. Myalil D, Rajan M A, Apte M, Lodha S. Robust collaborative fraudulent transaction detection using federated learning. In: Proceedings of the 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA) . 2021, 373378 276. Abadi A, Doyle B, Gini F, Guinamard K, Murakonda S K, Liddell J, Mellor P, Murdoch S J, Naseri M, Page H, Theodorakopoulos G, Weller S. Starlit: Privacy-preserving federated learning to enhance financial fraud detection . 2024, arXiv preprint arXiv: 2401.10765 277. Liu Y, Yu J J Q, Kang J, Niyato D, Zhang S. Privacy-preserving traffic flow prediction: a federated learning approach. IEEE Internet of Things Journal, 2020, 7(8): 77517763 278. Zhang C, Dang S, Shihada B, Alouini M S. Dual attention-based federated learning for wireless traffic prediction. In: Proceedings of the IEEE Conference on Computer Communications. 2021, 110 279. Zeng T, Guo J, Kim K J, Parsons K, Orlik P, Di Cairano S, Saad W . Multi-task federated learning for traffic prediction and its application to route planning. In: Proceedings of 2021 IEEE Intelligent Vehicles Symposium (IV) . 2021, 451457 280. Zhang C, Cui L, Yu S, Yu J J Q. A communication-efficient federated learning scheme for IoT-based traffic forecasting . IEEE Internet of Things Journal, 2022, 9(14): 1191811931 281. Qi T, Chen L, Li G, Li Y, Wang C. FedAGCN: a traffic flow prediction framework based on federated learning and asynchronous graph convolutional network. Applied Soft Computing , 2023, 138: 110175 282. Phyu H P, Stanica R, Naboulsi D . Multi-slice privacy-aware traffic forecasting at ran level: a scalable federated-learning approach. IEEE Transactions on Network and Service Management, 2023, 20(4): 50385052 283. Xia M, Jin D, Chen J . Short-term traffic flow prediction based on graph convolutional networks and federated learning. IEEE Transactions on Intelligent Transportation Systems, 2023, 24(1): 11911203 284. Zhang L, Zhang C, Shihada B . Efficient wireless traffic prediction at the edge: a federated meta-learning approach . IEEE Communications Letters, 2022, 26(7): 15731577 285. Hu S, Ye Y, Hu Q, Liu X, Cao S, Yang H H, Shen Y, Angeloudis P, Parada L, Wu C. A federated learning-based framework for ride- sourcing traffic demand prediction. IEEE Transactions on Vehicular Technology , 2023, 72(11): 1400214015 286. Huo J T, XU Y W, Huo Z S, Xiao L M, He Z X. Research on key technologies of edge cache in virtual data space across WAN. Frontiers of Computer Science , 2023, 17(1): 171102. 287. Li H Z, Jin H, Zheng L, Huang Y, Liao X F. ReCSA: a dedicated sort accelerator using ReRAM-based content addressable memory. Frontiers of Computer Science, 2023, 17(2): 172103. 288. Jia J, Liu Y, Zhang G Z, Gao Y L, Qian D P. Software approaches for resilience of high performance computing systems: a survey. Frontiers of Computer Science , 2023, 17(4): 174105. 289. Guo J Y, Zhang L, ROMERO HUNG J, Li C, Zhao J R, Guo M Y . FPGA sharing in the cloud: a comprehensive analysis. Frontiers of Computer Science , 2023, 17(5): 175106. 290. Wei Guo is a PhD student at the Institute of Artificial Intelligence, Beihang University, China. She received his MSc degree from the School of Electronics and Computer Science at Southampton University, UK. Her research interests primarily lie in federated learning and transfer learning. Fuzhen Zhuang received the BE degree from the College of Computer Science, Chongqing University, China in 2006, and the PhD degree in computer science from the Institute of Computing Technology, Chinese Academy of Sciences, China in 2011. He is currently a full professor with the Institute of Artificial Intelligence, Beihang University, China. He has published more than 140 papers in some prestigious refereed journals and conference proceedings. His research interests include transfer learning, machine learning, data mining, multitask learning, knowledge graph, and recommendation systems. He is a senior member of the CCF. He was a recipient of the Distinguished Dissertation Award of CAAI in 2013. Xiao Zhang is an associate professor at the School of Computer Science and Technology, Shandong University, China. His research interests include distributed learning, federated learning, edge intelligence, and data mining. He has published more than 20 papers in prestigious refereed journals and conference proceedings, such as IEEE TKDE, TMC, UBICOMP, SIGKDD, SIGIR, IJCAI, ACM CIKM, and IEEE ICDM. Yiqi Tong is a PhD student at the School of Computer Science and Engineering, Beihang University, China. He received his MSc degree from the School of Informatics at Xiamen University, China. His research interests primarily lie in natural language processing and recommendation systems. Jin Dong is the General Director of Beijing Academy of Blockchain and Edge Computing (BABEC), Director of National Blockchain Technology Innovation Center, Director of Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing. He has been dedicated to technical research in the fields of blockchain, privacy computing, chip design, etc. The team led by him developed the first of kind high performance hardware -software integrated blockchain system - ChainMaker around the globe, aiming to break through the performance and security bottlenecks of large- scale blockchain applications. This has been widely adopted by a variety of key economic and industrial applications in China. Jin Dong received his PhD degree from Tsinghua University, China and has filed more than forty US patents. 34 Front. Comput. Sci., 2024, 18(6): 186356",
    "page_start": null,
    "page_end": null,
    "word_count": 29122,
    "created_at": "2025-08-18T06:38:26",
    "updated_at": "2025-08-18T06:38:26"
  },
  {
    "id": "64d62f7ad4a247749a9db1d5f3e8fef5",
    "doc_id": "72ac6da3a3534a63b27ee8dbf16f408d",
    "doc_name": "Transfer_learning_in_DL.pdf",
    "heading": "Document",
    "content": "See discussions, stats, and author profiles for this publication at: https:www.researchgate.netpublication367432163 Transfer Learning in Deep Learning Models for Building Load Forecasting: Case of Limited Data Preprint  January 2023 DOI: 10.48550arXiv.2301.10663 CITATIONS 0 READS 123 4 authors, including: Menna Nawar Alexandria Higher Institute of Engineering and Technology 2 PUBLICATIONS 11 CITATIONS SEE PROFILE Moustafa Shomer Valify Solutions 2 PUBLICATIONS 11 CITATIONS SEE PROFILE All content following this page was uploaded by Moustafa Shomer on 01 February 2023. The user has requested enhancement of the downloaded file. Authors manuscript version accepted for publication. The final published version is copyrighted by IEEE and will be available as: Menna Nawar, Moustafa Shomer, Samy Faddel, Huangjie Gong, Transfer Learning in Deep Learning Models for Building Load Forecasting: Case of Limited Data , in IEEE SoutheastCon, 2023. 2023 IEEE Copyright Notice. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprintingrepublishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Transfer Learning in Deep Learning Models for Building Load Forecasting: Case of Limited Data AbstractPrecise load forecasting in buildings could increase the bill savings potential and facilitate optimized strategies for power generation planning. With the rapid evolution of computer science, data-driven techniques, in particular the Deep Learning models, have become a promising solution for the load forecasting problem. These models have showed accurate forecasting results; however, they need abundance amount of historical data to maintain the performance. Considering the new buildings and buildings with low resolution measuring equipment, it is difficult to get enough historical data from them, leading to poor forecasting performance. In order to adapt Deep Learning models for buildings with limited and scarce data, this paper proposes a Building-to-Building Transfer Learning framework to overcome the problem and enhance the performance of Deep Learning models. The transfer learning approach was applied to a new technique known as Transformer model due to its efficacy in capturing data trends. The performa nce of the algorithm was tested on a large commercial building with limited data. The result showed that the proposed approach improved the forecasting accuracy by 56.8 compared to the case of conventional deep learning where training from scratch is used . The paper also compared the proposed Transformer model to other sequential deep learning models such as Long-short Term Memory (LSTM) and Recurrent Neural Network (RNN). The accuracy of the transformer model outperformed other models by reducing the root mean square error to 0.009, compared to LSTM with 0.011 and RNN with 0.051. Keywords Deep Learning , Transfer Learning , Load Forecasting, Transformer, Sequential Models I. INTRODUCTION As a result of population growth and ongoing technological and economic developments, the number of commercial buildings has increased by 6 from 2012 to 2018 [1]. Newer buildings seem to be bigger in size and have modern electronic devices and more sophisticated equipment, which led to more power consumpti on and energy bills. In 2018, the U.S. has consumed more energy than ever before recording 101.3 quadrillion Btu, up 4 from 2017 [2]. In the following year 2019, U.S. has represented 17 of the energy consumption out of the whole world consumption [3]. Therefore, it is essential for respective parties involved in energy management such as governments and companies to improve the power consumption efficiency. Load-forecasting of commercial buil dings plays a crucial role in increasing energy schedule efficiency, and has become a serious topic that promoted a substantial degree of research. The principal obstacles arise from the fact that there is not a specific factor that controls the energy consumption in commercial buildings, instead there are multiple factors affecting the change of the consumed power. Those factors could affect the forecasting either in a direct way or indirect way, including weather conditions, building size, building type, electronic equipment and human activities, etc. [4]. Since forecasting is a key in improving the energy efficiency issue in commercial buildings, it is necessary to understand different types of load-forecasting. According to the spectrum of time intervals, load forecasting problems are classified into three types: short -term load forecasting (STLF) [5], medium-term load forecasting (MTLF) and long-term load forecasting (LTLF) [6]. This paper focuses on MTLF given the adopted time interval of load forecasting . MTLF has been an active research area which is highly discussed in the literature since it delive rs valuable information for both planning and operation [7]-[10]. MTLF can be assumed from one week to several months and has a goal of making a systematically effective operational plan and scheduling energy for both power generation plants and distribution utilities. In the past decades, it has been observed that improving the accuracy of energy forecasting has become an active issue. It was discussed in many studies using conventional statistical techniques and Artificial Intelligence (AI) base d approaches. Traditional statistical techniques usually involve auto-regressive integrated moving average (ARIMA), linear regression, and exponential smoothing [11], [1 2]. Statistical methods are fast and easy to apply because they rely on linear function s to process the relationships between the historical and forecasted data. As the time-series load forecasting is a non-linear problem, these models are not always forecast ing the load satisfactorily. This issue motivated the development of AI -based models as they are non-linear models which apply non-linear functions to forecast the load demand. Driven by the suitability and efficiency of AI techniques, they have become common in the literature to solve arduous load forecasting problems [1 3], [1 4]. AI -models can be categorized as Machine Learning and Deep Learning models. In terms of Machine Learning models, since these models have advantages such as simplicity and high computation speed, they are highly discussed in the literature [15], [16]. However, Deep Learning models , have proved to be more accurate in energy forecasting tasks [17]. AI-models depend on a large amount of historical data to predict power consumption [18], [19]. In [20], the authors used Deep Learning to predict humidity, air temperatures and energy behavior of buildings. Menna Nawar1, Moustafa Shomer1, Samy Faddel2, and Huangjie Gong2 1Alexandria Higher Institute of Engineering and Technology 2US Research Center, ABB Inc. Corresponding author: samy.faddel-mohamedus.abb.com The algorithm was trained on a large amount of data. Relying on a large-scale data to predict energy consumption was also used in [21], where the authors used a hybrid Deep Learning model that consists of Lon g Short -Term Memory (LSTM) and Recurrent Neural Network (RNN) in LTLF and trained their model on six years of data points from 2004 to 2010. In [22], the authors adapted a developed deep neural network (DNN) for heating energy consumption forecasting. Alth ough, the model had an acceptable performance when it was compared to simulation model from EnergyPlus, the authors did not consider comparing their model to other popular Deep Learning models that are suitable for time-series forecasting. Transformer model is considered one of the newly suggested models in the literature that was mentioned to have a great potential. In [23], the authors trained the Transformer model on 87,648 sampling data point before applying it in STLF , which makes it impractical for new buildings or those that do not collect data at a frequent basis. In [24], the authors explored transfer of knowledge from another domain, then applied this knowledge in two machine learning models (Random Forrest and Feed Forward Networ k). In [25], Transformer model was trained on a very huge-scale of data (19 years of data points) for wind speed forecasting, which clearly proves that Transformer model is considered one of the most data -hungry models. One of the most recent studies [26], The authors modeled Multi - Layer Perceptron (MLP) and Long Short -Term Memory (LSTM) on a medium-sized office building with data scarcity to predict the building thermal dynamics. They applied both traditional Machine Learning and Transfer Learning. The results of deep analysis leveraged using Transfer Learning and lead to higher accuracy. It is noteworthy that none of the previous work considered the use of Transformer model with Transfer learning to tackle the case of limited date. This paper proposes a medium-term load forecasting methodology that can be used for buildings with scarce historical data . The methodology is based on taking the knowledge and information learnt from buildings data in the same domain and transferring it to the targeted building. The paper also applies the framework of knowledge transfer to the Transformer deep learning model that is known for its ability in capturing trends and relations. To the best of our knowledge, this is the first time to apply transfer learning to the Tran sformer model in energy forecasting domain. Finally, the paper compares the proposed methodology to common deep learning techniques such as RNN and LSTM. The paper is organized as follows: Section 2 introduces the transfer learning and background concepts concerning the transfer learning technique. Section 3 presents deep learning models and discusses the specific models used in this paper. Furthermore, Section 4 shows a case study used to evaluate the proposed method and its results . Finally, our conclusion and future work will be discussed in section 5. II. TRANSFER LEARNING TECHNIQUE A. Sequential Deep Learning Models Deep Learning (DL) has become progressively popular in the field of time-series forecasting. The building-unit of any DL model is neural network. The neural network receives inputs, which can be text, image, video, or time -series data like this paper. Then, the input is processed in one or more hidden layer by using weights. These weights can be adjusted during the training process. Finally, the model comes up with the prediction in the output layer. Sequential DL models are the models that use temporal data as inputs. Time-series data are form of sequence data. The main advantage of sequence models is assuming that all inputs and outputs are dependent of each other. Therefore, these models can work efficiently in energy forecasting since the previous inputs in time-series data are inextricably significant in predicting the energy-consumption output. Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) are popular sequential models used in energy forecasting using time -series data as inputs, while Transformer model is rarely used in this domain. Noteworthy, t he three models can capture the patterns and trends, especially the Transformer . The architecture of each model will be discussed in detail in section III, and based on the patterns and trends, the models can predict accurately the energy consumption value. B. Building-to-Building Transfer Learning Transfer Learning (TL) is a promising approach that can help in enhancing the model performance by applying the prior knowledge gained from a so urce task to a similar or different target task. This approach was inspired by the way humans learn new things. Humans have always benefited from their transfer of knowledge and its usage in other different areas. From computer science point of view, the a im of TL is to leverage a pre-trained models knowledge, then applying this knowledge in performing another task. Fig. 1 illustrates the difference between traditional learning in part (a) and Transfer Learning in part (b). It shows that Transfer Learning depends on transferring the knowledge to a pre-trained model. Hence the complexity of the training process will be much easier. Considering the load forecasting problem, it is customary using massive amount of historical data to train an AI -based model to predict the energy consumption. Thus, relying on a large data has become unavoidable, which is impracticable in the case of newly constructed buildings. Unlike TL case, it is available to use scarce historical data to train the DL model as long as it is possible to take advantage of previous knowledge gained from an older task or another source task. Given that the DL models are always data-hungry, the main advantage of TL in these models is that it makes the use of small amount of data for training not only possible, but also efficient. Nevertheless, other advantages of TL are faster training, better model-initializing and higher learning rate for training. From a theoretical perspective of TL, the source (first task) and target tasks (other tasks) could be from the same or different domains. When it comes to load forecasting field, it is more popular in the literature to apply TL between two different domains. This paper demonstrates for the first -time applying building -to-building Transfer Learning in MTLF. The pre-trained models used in this study are initially trained for a load forecasting task for an old building. Then, th e prior knowledge gained from training on previous data, is used to enhance the accuracy of MTLF for new or other commercial buildings with scarce data. Briefly, TL in this paper is mainly used to acquire the knowledge from some old buildings to improve modellingforecasting efficiency on new buildings, with limited data. III. DEEP LEARNING MODELS A. RNN and LSTM Models RNNs have the same general principle of ANNs. While traditional ANNs can only deal with the input data individually and process the relationships between inputs and outputs without linking each output to the preceding. The main difference that RNNs can add ress the sequential data by using internal memory and assume that every output is related to the previous state. Once the output of the RNN is generated, it is copied and returned to the system as an input, hence the name Recurrent of the RNN comes from. Fig. 2 illustrates the structure of RNN, where X and O indicate the input and the output through the time t. Hidden layers are represented by h. U and V denote the weight m atrices between inputs and hidden layers, hidden layers and outputs. W represents t he weight metric between hidden layers in several time stamps. Although RNN is capable of processing sequential data, it fails to extract long -term information after many repeated iterations, because RNN suffers from vanishing gradient problem (this means that the internal memory of RNN cannot keep tracking of the gradient after multiple loops). To overcome this problem, LSTM networks were proposed in [ 27]. LSTMs are considered modified RNNs, however LSTMs are more effective in time -series forecasting for t he long -term dependency than RNNs. The structure of LSTM is similar to RNN, with an addition hidden state, it is also called LSTM cell as shown in Fig. 3 (where c denotes the computations of weights). This cell works as a separate memory for the network, and it is responsible for remembering the long -term information in LSTMs. This cell contains three gates; forget, input and output gate. The forget gate decides wh ich state in the cell can be forgotten, as this information is no longer necessary for the next state prediction. The input gate is responsible for adding or updating the internal cell with the new information. The output gate selects which part should be addressed as an output among all of the information in LSTM cell. The gates in LSTM are represented by the following equations: 𝑖𝑡  𝜎(𝑤𝑖 [ℎ𝑡1, 𝑥𝑡]  𝑏𝑖) (1) 𝑓𝑡  𝜎(𝑤𝑓[ℎ𝑡1, 𝑥𝑡]  𝑏𝑓) (2) 𝑜𝑡  𝜎(𝑤𝑜[ℎ𝑡1, 𝑥𝑡]  𝑏𝑜) (3) (a) (b) (a) (b) Fig. 1 The difference between traditional learning (a) and transfer learning (b). Fig. 2 Fundamental topology of RNN where: 𝑖𝑡 is the input gate, 𝑓𝑡 is the forget gate and 𝑜𝑡 is the output gate of the network. Sigmoid function is represented by 𝜎, 𝑤 denotes the weight for the respective gates and ℎ𝑡1 is the previous output at timestep 𝑡  1 . The current input is represented by 𝑥𝑡 and 𝑏 indicates the biases for the respective gates. Although LSTM has overcome the obstacle of vanishing gradient in RNN, this model created other challenges. Since relying on the LSTM cell including the three gates, the information from previous steps has to go through a long sequence of computations. This problem leads to difficulty in training LSTM due to a very long gradient path. This recursion of the sequence will cause information loss eventually. As mentioned earlier, RNN and LSTM suffer from forgetting the information in the long -term, since both models process the inputs sequentially. Hence, the need for a new model addressing the inputs in parallel rather than sequentially. B. Attention mechanism and Transformer Model The solution to the long -term dependency problem was solved in 2014 and 2015 by [28], [29]. These pioneering papers proposed a technique called the Attention Mechanism. Attention Mechanism is the backbone of the Transformer model, illustrated in [ Fig. 4], Transformers are currently the state -of- the-art solution for many problems such as computer vision and natural language processing. It consists of two parts, encoder and decoder, where each block contains one or more attention layer followed by a multi -layer perceptron (MLP). Using this technique dramatically improved the quality of any sequence - related tasks and used later in the time -series forecasting domain. The attention allows the model to concentrate on only the important and relevant subsets in the long sequences of the input. Concretely, the model has to decide by learning on its own which information from the past steps are relevant for encoding the available input, and then takes the encoded information and decodes it to representative features that is used for forecasting. The core of attention mechanism is assuming that the data source contains elements that can be represented as Key (K) and Value (V). Another element; Query (Q) is associated with the target data, where: KT  R3x5 , V  R5x3 and Q  R4x3. There are three steps to a pply the attention mechanism [27]. First, calculating the similarity score between the key vector and the query vector according to ( 4). Secondly in ( 5), converting the similarity score into weights, then arranging these weights in a probability distribution. Finally, (6) represents calculating the attention value by weighted summation of all the resulted coefficients. The reader is referred to [27] for more information. 𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦(𝑄𝑉, 𝑘𝑖)  𝑄𝑉𝑘𝑖 𝑄𝑉𝑘𝑖 (4) 𝑊𝑖  𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦𝑖 ) 𝑓𝑜𝑟 𝑖  1, 2, 3,  , 𝑛 (5) 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 𝑆𝑐𝑜𝑟𝑒(𝑄, 𝐾, 𝑉)   𝑤𝑖 𝑛 𝑖1  𝑉 (6) In 2017, a new DL model, called Transformer, was proposed based on the attention mechanism [30]. Transformer was rapidly used in many different areas such as translation, image classification and time-series forecasting, and it overcame state- of-the-art models in these areas. In comparison, the main advantage to the Transformers architecture is that it allows the model to access the input data directly using parallel computation, not sequentially as the case of RNN and LSTM. Processing the inputs in parallel avoids the recursion and the iterations, which leads to reduction in the training time and the probability of losing information in the long dependencies. The Transformer does not depend on th e previous hidden states to capture the patterns in order to predict the output. Instead, it processes a batch of input data as a one unit learning positional embeddings to encode the relationships between each observation and search for dependencies and patterns in the time -series data. Positional embedding is a technique that was introduced to replace recurrence by using weights that can encode the information related to a specific position of a certain input , then the transformer decodes the information and transforms it into prediction for the next time - step. IV. CASE STUDY This paper used hourly collected data from two buildings over time interval of one year starting from 1 January 2016 to 1 January 2017. The data were adopted from American Society of Heating , Refrigerating and Air -Conditioning Engineers (ASHRAE) [31]. The proposed models , Transformer, LSTM and RNN, were trained on a subset of that data , representing only 20 of the total samples , which counted up to 336 data samples. The time window used in the experimentations is 6 hours. Fig. 3 Simple architecture of LSTM (a), the internal state or the cell of LSTM and its gates (b). Fig. 4 The Architecture of the Transformer Model Fig. 5 The load consumption over one week for one building including weekend and workdays Fig. 6 The comparison of the three models The dataset contained 14 features, including information about weather conditions like temperature, cloud coverage, precipitation depth and others. It also included buildings metadata like buildings sizes and primary usage. To improve the performance of the model , some engineered features were added like day-of-the-week, seasons, and day-of-the-month, to account for the vacations, holidays and seasonal trends. This paper focuses on educational buildings as they captured most of buildings types by 38. An example of the pattern of the load consumption over one week for one building is shown in Fig. 5 According to the floor count, the number of floors for the two buildings is equal to five floors, representing a large building type Finally, the data used in the study had many features. Some of these features were repre sentative and informative for the modeling process, while others where not relevant to the task or might harm the prediction. After analyzing the data, it was decided to go with the chosen featurism while ditching the rest. An example for the neglected data is the sea level pressure as it is almost constant across all data points. Also, the wind direction as it does not affect the temperature, but it is an indication for the presence of the wind. This would force the model to learn this correlation, instead of learning about the desired task. As well as, engineering the features to give more information to the proposed model, such as the day, month, and hour. It is crucial to determine If the day is a weekend or not, because the usage of electricity highly depends on this feature. The model can also learn to combine information from multiple features together. The results will first show how the transfer learning can improve the accuracy of the deep learning model , enabling forecasting in building with limited data. Then, the paper will compare the proposed transfer transformer model to other sequential models. A. Transfer-Learning Effect on the Transformer model This section aims to see the effect of applying building-to- building Transfer Learning on improving the performance of the Transformer model which were trained on limited data . In theory, transfer learning takes the weights of the pre -trained model and uses them as a starting point to train from, which increases the speed and probability of better convergence to reach high performance . Since it is the first time to apply this type of Transfer Learning to the Transformer model in the domain of energy forecasting and train it on limited data to forecast the load demand, it is essential to evaluate the impact of applying Transfer Learning . Considering the case of large commercial buildings, after adapting building -to-building Transfer Learning the performance of the Transformer model has been improved by 56.8  in terms o f mean squared error (MSE), and almost 34  in terms of root mean square error (RMSE) and mean absolute percentage error (MAPE) as shown in Table. I. B. Forecasting the Load of Large Buildings To evaluate the performance of the proposed model compared to the traditional sequential models such as the RNN and the LSTM, the three models were trained for 15 epochs to prevent models overfitting and make it easier to measure the performance of the models under the same training period. The same large building was used as the source of data for the three models. Also, transfer learning was applied to all of them to ensure apple to apple comparison. The results are presented in Fig. 6 and Table. II. The figure shows that the RNN model has the tendency to overestimate the load for both during heavy and light loading conditions compared to the other two models. The LSTM shows close performance to the transformer model though the transformer is better in following the trend, resulting in a better accuracy as given in Table. II. This result is because the Transformer is built for capturing the patterns, trends, and relations between data points. Regardless of the improvement in the accuracy, the th ree models do not seem to perform well under light loading conditions for the case of weekends as shown after hour 125 in Fig. 6. This will be investigated more in the future. However, typically load forecasting for weekends in commercial buildings is not of high importance since the load is very low and there are not any occupants in the building. TABLE I. IMPACT OF TRANSFER LEARNING IN TRANSFORMERS PERFORMANCE IN CASE OF LARGE BUILDINGS Type of Metric Transformer model Before Transfer Learning After Transfer Learning MSE 0.021 0.009 RMSE 0.146 0.096 MAPE 0.311 0.203 TABLE II. EVALUATION METRICES OF THE THREE MODELS IN LOAD - DEMAND FORECASTING OF LARGE BUILDINGS Metric Transformer LSTM RNN MSE 0.009 0.011 0.051 RMSE 0.096 0.106 0.227 MAPE 0.203 0.217 0.471 V. CONCLUSIONS This paper proposed building-to-building transfer learning for energy load forecasting, which aims for enhancing the performance of the Deep Learning models used currently in the field; by harnessing the knowledge lea rnt from buildings with enough data, and use it to boost the accuracy of predictions for buildings with scarce data. To validate the approach , a case study for a large building was presented, where the data used in our experiments was representing only 2.5 months generated hourly. The paper compared the performance of the mostly used Deep Learning models in time -series forecasting before and after building-to-building transfer learning. This work was the first in utilizing transfer learning and applying it to the Transformer model in the field of energy load forecasting, where the performance gain achieved by the Transformer according to MSE was 56.8 after applying our method. Future work will apply the same methodologies to different building domains and sizes to investigate the different outcomes. More research will focus into finding the limitations of this approach, mainly the least amount of data used with transfer learning to increase the performance. REFERENCES [1] Eia.gov, 2022. [Online]. Available: https:www.eia.govconsumptioncommercialdata2018pdfCBECS_2 018_Building_Characteristics_Flipbook.pdf. [Accessed: 15- Feb- 2022]. [2] Eia.gov, 2022. [Online]. Available: https:www.eia.govtodayinenergydetail.php?id39092. [Accessed: 15- Feb- 2022]. [3] Frequently Asked Questions (FAQs) - U.S. Energy Information Administration (EIA), Eia.gov, 2022. [Online]. Available: https:www.eia.govtoolsfaqsfaq.php?id87t1. [Accessed: 15- Feb- 2022]. [4] Pinzon, P. Vergara, L. da Silva and M. Rider, Optimal Management of Energy Consumption and Comfort for Smart Buildings Operating in a Microgrid, IEEE Transactions on Smart Grid, vol. 10, no. 3, pp. 3236 - 3247, 2019. Available: 10.1109tsg.2018.2822276. [5] T. Yalcinoz and U. Eminoglu, Short -term and medium -term power distribution load forecasting by neural networks, Energy Conversion and Management, vol. 46, no. 9 -10, pp. 1393 -1405, 2005. Available: 10.1016j.enconman.2004.07.005. [6] H. A l-Hamadi and S. Soliman, Long -termmid-term electric load forecasting based on short-term correlation and annual growth, Electric Power Systems Research, vol. 74, no. 3, pp. 353 -361, 2005. Available: 10.1016j.epsr.2004.10.015 [7] M. Ghiassi, D. K. Zimbra, a nd H. Saidane, Medium term system load forecasting with a dynamic artificial neural network model, Electric Power Systems Research, vol. 76, no. 5, pp. 302 316, Mar. 2006, doi: 10.1016j.epsr.2005.06.010. [8] N. Ayub et al., Big Data Analytics for Short and Medium-Term Electricity Load Forecasting Using an AI Techniques Ensembler, Energies, vol. 13, no. 19, Art. no. 19, Jan. 2020, doi: 10.3390en13195193. [9] L. Han, Y. Peng, Y. Li, B. Yong, Q. Zhou, and L. Shu, Enhanced Deep Networks for Short -Term and Mediu m-Term Load Forecasting, IEEE Access, vol. 7 , pp. 40454055, 2019 , doi: 10.1109ACCESS.2018.2888978. [10] L. Han, Y. Peng, Y. Li, B. Yong, Q. Zhou, and L. Shu, Enhanced Deep Networks for Short -Term and Medium -Term Load Forecasting, IEEE Access, vol. 7 , pp. 40454055, 2019 , doi: 10.1109ACCESS.2018.2888978. [11] P. S. Kalekar, Time series Forecasting using Holt -Winters Exponential Smoothing, Kanwal Rekhi School of Information Technology, p. 13, 2004. [12] I. Mpawenimana, A. Pegatoquet, V. Roy, L. Rodriguez, and C. Belleudy, A comparative study of LSTM and ARIMA for energy load prediction with enhanced data preprocessing, in 2020 IEEE Sensors Applications Symposium (SAS), Mar. 2020, pp. 1 6. doi: 10.1109SAS48726.2020.9220021. [13] E. Mocanu, P. H. Nguyen, M. Gibescu, and W. L. Kling, Deep learning for estimating building energy consumption, Sustainable Energy, Grids and Networks, vol. 6, pp. 91 99, Jun. 2016, doi: 10.1016j.segan.2016.02.005. [14] C. Fan, Y. Sun, Y. Zhao, M. Song, and J. Wang, Deep learn ing-based feature engineering methods for improved building energy prediction, Applied Energy, vol. 240, pp. 35 45, Apr. 2019, doi: 10.1016j.apenergy.2019.02.052. [15] M. Hambali, Akinyemi, M. Oladunjoye, and Y. N., Electric Power Load Forecast Using Decision Tree Algorithms, vol. 7, pp. 2942, Jan. 2017. [16] Y. Fu, Z. Li, H. Zhang, and P. Xu, Using Support Vector Machine to Predict Next Day Electricity Load of Public Buildings with Sub-metering Devices, Procedia Engineering, vol. 121, pp. 10161022, Jan. 2015, doi: 10.1016j.proeng.2015.09.097. [17] N. G. Paterakis, E. Mocanu, M. Gibescu, B. Stappers, and W. van Alst, Deep learning versus traditional machine learning methods for aggregated energy demand prediction, in 2017 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe), Sep. 2017, pp. 16. doi: 10.1109ISGTEurope.2017.8260289.. [18] C. Yang, Q. Cheng, P. Lai, J. Liu, and H. Guo, Data -Driven Modeling for Energy Consumption Estimation: Applications, in Green Energy and Technology, 2018, pp. 10571068. doi: 10.1007978-3-319-62575-1_72. [19] A. Gonzalez-Vidal, A. P. Ramallo -Gonzalez, F. Terroso -Saenz, and A. Skarmeta, Data driven modeling for energy consumption prediction in smart buildings, in 2017 IEEE International Conference on Big Data (Big Data), Boston, MA, Dec. 2017, pp. 4562 4569. doi: 10.1109BigData.2017.8258499. [20] V. J. Mawson and B. R. Hughes, Deep learning techniques for energy forecasting and condition monitoring in the manufacturing sector, Energy and Buildings, vol. 217, p. 10996 6, Jun. 2020, doi: 10.1016j.enbuild.2020.109966. [21] R. K. Agrawal, F. Muchahary, and M. M. Tripathi, Long term load forecasting with hourly predictions based on long -short-term-memory networks, in 2018 IEEE Texas Power and Energy Conference (TPEC), Feb. 2018, pp. 16. doi: 10.1109TPEC.2018.8312088. [22] .S. Lee et al., Deep Neural Network Approach for Prediction of Heating Energy Consumption in Old Houses, Energies, vol. 14, no. 1, Art. no. 1, Jan. 2021, doi: 10.3390en14010122. [23] Z. Zhao et al., Short-Term Load Forecasting Based on the Transformer Model, Information, vol. 12, no. 12, Art. no. 12, Dec. 2021, doi: 10.3390info12120516. [24] M. Jain, K. Gupta, A. Sathanur, V. Chandan, and M. M. Halappanavar, Transfer-Learnt Models for Predicting Electr icity Consumption in Buildings with Limited and Sparse Field Data, in 2021 American Control Conference (ACC), May 2021, pp. 2887 2894. doi: 10.23919ACC50511.2021.9483228. Model [25] Huijuan Wu, Keqilao Meng, Daoerji Fan, Zhanqiang Zhang, Qing Liu, Multistep Short -Term Wind Speed Forecasting Using Transformer, Energy, Vol. 276, Dec. 2022, doi: 10.1016j.energy.2022.125231. [26] G. Pinto, R. Messina, H. Li, T. Hong, M. S. Piscitelli, and A. Capozzoli, Sharing is caring: An extensive analysis of parameter -based transfer learning for the prediction of building thermal dynamics, Energy and Buildings, vol. 276, p. 112530, Dec. 2022, doi: 10.1016j.enbuild.2022.112530. [27] S. Hochreiter and J. Schmidhuber, Long Short -Term Memory, Neural Computation, vol. 9, no. 8, pp. 1735 1780, Nov. 1997, doi: 10.1162neco.1997.9.8.1735. [28] D. Bahdanau, K. Cho, and Y. Bengio, Neural Machine Translation by Jointly Learning to Align and Translate, undefined, 2015, Accessed: Mar. 03, 2022. [Online]. Available: https:www.semanticscholar.orgpaperNeural-Machine-Translation-by- Jointly-Learning-to-Bahdanau- Chofa72afa9b2cbc8f0d7b05d52548906610ffbb9c5 [29] T. Luong, H. Pham, and C. D. Manning, Effective Approaches to Attention-based Neural Machine Translation, in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, Sep. 2015, pp. 14121421. doi: 10.18653v1D15-1166. [30] A. Vaswani et al., Attention is All you Need, in Advances in Neural Information Processing Systems, 2017, vol. 30. Accessed: Mar. 01, 2022. [Online]. Available: https:proceedings.neurips.ccpaper2017hash3f5ee243547dee91fbd05 3c1c4a845aa-Abstract.html [31] Ashrae - Great Energy Predictor III, Kaggle. [Online]. Available: https:www.kaggle.comcashrae-energy-prediction. [Accessed: 17-Mar- 2022]. View publication stats",
    "page_start": null,
    "page_end": null,
    "word_count": 5323,
    "created_at": "2025-08-18T06:38:39",
    "updated_at": "2025-08-18T06:38:39"
  },
  {
    "id": "fdbf2235467c42ff93f494973befd01f",
    "doc_id": "079fdd1933be460a9a53c7246bb31729",
    "doc_name": "A_Comprehensive_Survey_on_Transfer_Learning_2.pdf",
    "heading": "Document",
    "content": "arXiv:1911.02685v3 [cs.LG] 23 Jun 2020 1 A Comprehensive Survey on T ransfer Learning Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Y ongchun Zh u, Hengshu Zhu, Senior Member, IEEE, Hui Xiong, Fellow, IEEE, and Qing He AbstractT ransfer learning aims at improving the performance of tar get learners on target domains by transferring the knowledg e contained in different but related source domains. In this w ay , the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide applicati on prospects, transfer learning has become a popular and pro mising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent a dvances in transfer learning. Due to the rapid expansion of t he transfer learning area, it is both necessary and challenging to compr ehensively review the relevant studies. This survey attemp ts to connect and systematize the existing transfer learning researches , as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way , which may help rea ders have a better understanding of the current research sta tus and ideas. Unlike previous surveys, this survey paper reviews m ore than forty representative transfer learning approache s, especially homogeneous transfer learning approaches, from the perspe ctives of data and model. The applications of transfer learn ing are also brieﬂy introduced. In order to show the performance of diffe rent transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-2 1578, and Ofﬁce-31. And the experimental results demonstrate the imp ortance of selecting appropriate transfer learning models for different applications in practice. Index Terms T ransfer learning, machine learning, domain adaptation, interpretation.  1 I NTRODUCTION A LT H O UGH traditional machine learning technology has achieved great success and has been successfully ap- plied in many practical applications, it still has some limi ta- tions for certain real-world scenarios. The ideal scenario of machine learning is that there are abundant labeled trainin g instances, which have the same distribution as the test data . However , collecting sufﬁcient training data is often expen - sive, time-consuming, or even unrealistic in many scenario s. Semi-supervised learning can partly solve this problem by relaxing the need of mass labeled data. T ypically , a semi- supervised approach only requires a limited number of labeled data, and it utilizes a large amount of unlabeled data to improve the learning accuracy . But in many cases, unlabeled instances are also difﬁcult to collect, which usu - ally makes the resultant traditional models unsatisfactor y . T ransfer learning, which focuses on transferring the knowledge across domains, is a promising machine learning methodology for solving the above problem. The concept about transfer learning may initially come from educationa l psychology . According to the generalization theory of tran s- fer , as proposed by psychologist C.H. Judd, learning to transfer is the result of the generalization of experience. It is possible to realize the transfer from one situation to anoth er ,  Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Y ongchun Zh u, and Qing He are with the Key Laboratory of Intelligent Inform ation Processing of Chinese Academy of Sciences (CAS), Institute of Computing T echnology, CAS, Beijing 100190, China and the University o f Chinese Academy of Sciences, Beijing 100049, China.  Hengshu Zhu is with Baidu Inc., No. 10 Shangdi 10th Street, Ha idian District, Beijing, China.  Hui Xiong is with Rutgers, the State University of New Jersey , 1 Washington Park, Newark, New Jersey, USA.  Zhiyuan Qi is with the equal contribution to the ﬁrst author .  Fuzhen Zhuang and Zhiyuan Qi are corresponding authors, zhu ang- fuzhenict.ac.cn and qizhyuangmail.com. Fig. 1. Intuitive examples about transfer learning. as long as a person generalizes his experience. According to this theory , the prerequisite of transfer is that there needs to be a connection between two learning activities. In practice, a person who has learned the violin can learn the piano faster than others, since both the violin and the piano are musical instruments and may share some common knowledge. Fig. 1 shows some intuitive examples about transfer learning. Inspired by human beings capabilities to transfer knowledge across domains, transfer learning aims to leverage knowledge from a related domain (called source domain) to improve the learning performance or minimize the number of labeled examples required in a target domain. It is worth mentioning that the transferred knowledge does not always bring a positive impact on new tasks. If there is little in common between domains, knowledge transfer could be unsuccessful. For example, learning to ride a bicy- cle cannot help us learn to play the piano faster . Besides, th e similarities between domains do not always facilitate lear n- ing, because sometimes the similarities may be misleading. For example, although Spanish and French have a close re- lationship with each other and both belong to the Romance 2 group of languages, people who learn Spanish may experi- ence difﬁculties in learning French, such as using the wrong vocabulary or conjugation. This occurs because previous successful experience in Spanish can interfere with learni ng the word formation, usage, pronunciation, conjugation, et c., in French. In the ﬁeld of psychology , the phenomenon that previous experience has a negative effect on learning new tasks is called negative transfer [1]. Similarly , in the tra nsfer learning area, if the target learner is negatively affected by the transferred knowledge, the phenomenon is also termed as negative transfer [2], [3]. Whether negative transfer wi ll occur may depend on several factors, such as the relevance between the source and the target domains and the learners capacity of ﬁnding the transferable and beneﬁcial part of th e knowledge across domains. In [3], a formal deﬁnition and some analyses of negative transfer are given. Roughly speaking, according to the discrepancy between domains, transfer learning can be further divided into two categories, i.e., homogeneous and heterogeneous transfer learning [4]. Homogeneous transfer learning approaches ar e developed and proposed for handling the situations where the domains are of the same feature space. In homogeneous transfer learning, some studies assume that domains differ only in marginal distributions. Therefore, they adapt the d o- mains by correcting the sample selection bias [5] or covaria te shift [6]. However , this assumption does not hold in many cases. For example, in sentiment classiﬁcation problem, a word may have different meaning tendencies in different domains. This phenomenon is also called context feature bias [7]. T o solve this problem, some studies further adapt the conditional distributions. Heterogeneous transfer le arn- ing refers to the knowledge transfer process in the situatio ns where the domains have different feature spaces. In additio n to distribution adaptation, heterogeneous transfer learn ing requires feature space adaptation [7], which makes it more complicated than homogeneous transfer learning. The survey aims to give readers a comprehensive un- derstanding about transfer learning from the perspectives of data and model. The mechanisms and the strategies of transfer learning approaches are introduced to allow readers grasp how the approaches work. And a number of the existing transfer learning researches are connected an d systematized. Speciﬁcally , over forty representative tra nsfer learning approaches are introduced. Besides, we conduct experiments to demonstrate on which dataset a transfer learning model performs well. In this survey , we focus more on homogeneous transfer learning. Some interesting transfer learning topics are no t covered in this survey , such as reinforcement transfer lear n- ing [8], lifelong transfer learning [9], and online transfe r learning [10]. The rest of this survey are organized into seven sections. Section 2 clariﬁes the difference between transfer learning and other related machine learning tech- niques. Section 3 introduces the notations used in this sur- vey and the deﬁnitions about transfer learning. Sections 4 and 5 interpret transfer learning approaches from the data and the model perspectives, respectively . Section 6 intro- duces some applications of transfer learning. Experiments are conducted and the results are provided in Section 7. The last section concludes this survey . The main contributions of this survey are summarized below .  Over forty representative transfer learning approaches are introduced and summarized, which can give read- ers a comprehensive overview about transfer learning.  Experiments are conducted to compare different trans- fer learning approaches. The performance of over twenty different approaches is displayed intuitively and then analyzed, which may be instructive for the readers to select the appropriate ones in practice. 2 R ELATED WORK Some areas related to transfer learning are introduced. The connections and difference between them and transfer learn - ing are clariﬁed. Semi-Supervised Learning [11]: Semi-supervised learning is a machine learning task and method that lies between supervised learning (with completely labeled instances) and unsupervised learning (without any labeled instances) . T ypically , a semi-supervised method utilizes abundant un- labeled instances combined with a limited number of la- beled instances to train a learner . Semi-supervised learn- ing relaxes the dependence on labeled instances, and thus reduces the expensive labeling costs. Note that, in semi- supervised learning, both the labeled and unlabeled in- stances are drawn from the same distribution. In contrast, i n transfer learning, the data distributions of the source and the target domains are usually different. Many transfer learn- ing approaches absorb the technology of semi-supervised learning. The key assumptions in semi-supervised learning , i.e., smoothness, cluster , and manifold assumptions, are a lso made use of in transfer learning. It is worth mentioning that semi-supervised transfer learning is a controversial term. The reason is that the concept of whether the label information is available in transfer learning is ambiguous because both the source and the target domains can be involved. Multi-View Learning [12]: Multi-view learning focuses on the machine learning problems with multi-view data. A view represents a distinct feature set. An intuitive exampl e about multiple views is that a video object can be described from two different viewpoints, i.e., the image signal and the audio signal. Brieﬂy , multi-view learning describes an object from multiple views, which results in abundant in- formation. By properly considering the information from al l views, the learner s performance can be improved. There are several strategies adopted in multi-view learning such as subspace learning, multi-kernel learning, and co-train ing [13], [14]. Multi-view techniques are also adopted in some transfer learning approaches. For example, Zhang et al. pro- posed a multi-view transfer learning framework, which im- poses the consistency among multiple views [15]. Y ang and Gao incorporated multi-view information across different domains for knowledge transfer [16]. The work by Feuz and Cook introduces a multi-view transfer learning approach for activity learning, which transfers activity knowledge between heterogeneous sensor platforms [17]. Multi-T ask Learning [18]: The thought of multi-task learn- ing is to jointly learn a group of related tasks. T o be more speciﬁc, multi-task learning reinforces each task by taking advantage of the interconnections between task, i.e ., considering both the inter-task relevance and the inter-ta sk 3 difference. In this way , the generalization of each task is e n- hanced. The main difference between transfer learning and multi-task learning is that the former transfer the knowl- edge contained in the related domains, while the latter transfer the knowledge via simultaneously learning some related tasks. In other words, multi-task learning pays equ al attention to each task, while transfer learning pays more attention to the target task than to the source task. There ar e some commons and associations between transfer learning and multi-task learning. Both of them aim to improve the performance of learners via knowledge transfer . Besides, they adopt some similar strategies for constructing models , such as feature transformation and parameter sharing. Note that some existing studies utilize both the transfer learn- ing and the multi-task learning technologies. For example, the work by Zhang et al. employs multi-task and transfer learning techniques for biological image analysis [19]. Th e work by Liu et al. proposes a framework for human action recognition based on multi-task learning and multi-source transfer learning [20]. 3 O VERVIEW In this section, the notations used in this survey are listed for convenience. Besides, some deﬁnitions and categorization s about transfer learning are introduced, and some related surveys are also provided. 3.1 Notation For convenience, a list of symbols and their deﬁnitions are shown in T able 1. Besides, we use to represent the norm and superscript T to denote the transpose of a vectormatrix. 3.2 Deﬁnition In this subsection, some deﬁnitions about transfer learnin g are given. Before giving the deﬁnition of transfer learning , let us review the deﬁnitions of a domain and a task. Deﬁnition 1. (Domain) A domain Dis composed of two parts, i.e., a feature space Xand a marginal distribution P (X). In other words, D {X, P (X)}. And the symbol X denotes an instance set, which is deﬁned as X  {xxi X , i  1,  , n }. Deﬁnition 2. (T ask) A task T consists of a label space Yand a decision function f, i.e., T  {Y, f }. The decision function f is an implicit one, which is expected to be learned from the sample data. Some machine learning models actually output the pre- dicted conditional distributions of instances. In this cas e, f(xj )  {P (ykxj )yk Y, k  1 ,  , Y}. In practice, a domain is often observed by a number of instances withwithout the label information. For ex- ample, a source domain DS corresponding to a source task TS is usually observed via the instance-label pairs, i.e., DS  {(x, y )xi  XS , y i  YS, i  1 ,  , n S}; an observation of the target domain usually consists of a number of unlabeled instances andor limited number of labeled instances. Deﬁnition 3. (T ransfer Learning) Given somean observation(s) corresponding to mS  N source domain(s) and task(s) T ABLE 1 Notations. Symbol Deﬁnition n Number of instances m Number of domains D Domain T T ask X Feature space Y Label space x Feature vector y Label X Instance set Y Label set corresponding to X S Source domain T T arget domain L Labeled instances U Unlabeled instances H Reproducing kernel Hilbert space θ MappingCoefﬁcient vector α W eighting coefﬁcient β W eighting coefﬁcient λ T radeoff parameter δ ParameterError b Bias B Boundary parameter N IterationKernel number f Decision function L Loss function η Scale parameter G Graph Φ Nonlinear mapping σ Monotonically increasing function Ω Structural risk κ Kernel function K Kernel matrix H Centering matrix C Covariance matrix d Document w W ord z Class variable z Noise D Discriminator G Generator S Function M Orthonormal bases Θ Model parameters P Probability E Expectation Q Matrix variable R Matrix variable W Mapping matrix (i.e., {(DSi , TSi )i  1 ,  , m S}), and somean observa- tion(s) about mT  N target domain(s) and task(s) (i.e., {(DTj , TTj )j  1 ,  , m T }), transfer learning utilizes the knowledge implied in the source domain(s) to improve the performance of the learned decision functions fTj (j  1,  , m T ) on the target domain(s). The above deﬁnition, which covers the situation of multi- source transfer learning, is an extension of the one present ed in the survey [2]. If mS equals 1, the scenario is called single- source transfer learning. Otherwise, it is called multi-so urce transfer learning. Besides, mT represents the number of the transfer learning tasks. A few studies focus on the setting that mT 2 [21]. The existing transfer learning studies pay more attention to the scenarios where mT  1 (especially where mS  mT  1 ). It is worth mentioning that the observation of a domain or a task is a concept with broad 4 Transfer Learning Problem Categorization Solution Categorization Homogeneous Transfer Learning Heterogeneous Transfer Learning Inductive Transfer Learning Transductive Transfer Learning Unsupervised Transfer Learning Instance-Based Approach Feature-Based Approach Parameter-Based Approach Relational-Based Approach Symmetric Transformation Asymmetric Transformation Label-Setting-Based Categorization Space-Setting-Based Categorization Fig. 2. Categorizations of transfer learning. sense, which is often cemented into a labeledunlabeled instance set or a pre-learned model. A common scenario is that we have abundant labeled instances or have a well- trained model on the source domain, and we only have limited labeled target-domain instances. In this case, the resources such as the instances and the model are actually the observations, and the goal of transfer learning is to lea rn a more accurate decision function on the target domain. Another term commonly used in the transfer learning area is domain adaptation. Domain adaptation refers to the process that adapting one or more source domains to trans- fer knowledge and improve the performance of the target learner [4]. T ransfer learning often relies on the domain adaptation process, which attempts to reduce the differenc e between domains. 3.3 Categorization of T ransfer Learning There are several categorization criteria of transfer lear ning. For example, transfer learning problems can be divided into three categories, i.e., transductive, inductive, and un- supervised transfer learning [2]. The complete deﬁnitions of these three categories are presented in [2]. These three categories can be interpreted from a label-setting aspect. Roughly speaking, transductive transfer learning refers t o the situations where the label information only comes from the source domain. If the label information of the target- domain instances is available, the scenario can be catego- rized into inductive transfer learning. If the label inform a- tion is unknown for both the source and the target domains, the situation is known as unsupervised transfer learning. Another categorization is based on the consistency between the source and the target feature spaces and label spaces. If XS  XT and YS  YT , the scenario is termed as homogeneous transfer learning. Otherwise, if XS  XT orand YS  YT , the scenario is referred to as heteroge- neous transfer learning. According to the survey [2], the transfer learning ap- proaches can be categorized into four groups: instance- based, feature-based, parameter-based, and relational-b ased approaches. Instance-based transfer learning approaches are mainly based on the instance weighting strategy . Feature- based approaches transform the original features to create a new feature representation; they can be further divided int o two subcategories, i.e., asymmetric and symmetric feature - based transfer learning. Asymmetric approaches transform the source features to match the target ones. In contrast, symmetric approaches attempt to ﬁnd a common latent feature space and then transform both the source and the target features into a new feature representation. The parameter-based transfer learning approaches transfer th e knowledge at the modelparameter level. Relational-based transfer learning approaches mainly focus on the problems in relational domains. Such approaches transfer the logica l relationship or rules learned in the source domain to the target domain. For better understanding, Fig. 2 shows the above-mentioned categorizations of transfer learning. Some surveys are provided for the readers who want to have a more complete understanding of this ﬁeld. The survey by Pan and Y ang [2], which is a pioneering work, cat- egorizes transfer learning and reviews the research progre ss before 2010. The survey by W eiss et al. introduces and summarizes a number of homogeneous and heterogeneous transfer learning approaches [4]. Heterogeneous transfer learning is specially reviewed in the survey by Day and Khoshgoftaar [7]. Some surveys review the literatures re- lated to speciﬁc themes such as reinforcement learning [8], computational intelligence [22], and deep learning [23], [ 24]. Besides, a number of surveys focus on speciﬁc application scenarios including activity recognition [25], visual cat ego- rization [26], collaborative recommendation [27], comput er vision [24], and sentiment analysis [28]. Note that the organization of this survey does not strictly follow the above-mentioned categorizations. In the next two sections, transfer learning approaches are interprete d from the data and the model perspectives. Roughly speak- ing, data-based interpretation covers the above-mentione d instance-based and feature-based transfer learning ap- proaches, but from a broader perspective. Model-based interpretation covers the above-mentioned parameter-bas ed approaches. Since there are relatively few studies concern - ing relational-based transfer learning and the representa tive approaches are well introduced in [2], [4], this survey does not focus on relational-based approaches. 5 Covariance ... Geometric Structure Cluster Structure ... Data-Based Interpretation Objective Measurement Type Distribution Adaptation Data Property PreservationAdjustment Marginal Distribution Adaptation Conditional Distribution Adaptation Kullback-Leibler Divergence Maximum Mean Discrepancy Jensen-Shannon Divergence ... Statistical Property Strategy Instance Weighting Feature Transformation Feature Clustering Feature Alignment Feature Augmentation Feature Reduction Joint Distribution Adaptation Feature Replication ... Feature Encoding Bregman Divergence Feature Stacking Mean Manifold Structure ... Feature Mapping Estimation Method ... Heuristic Method Space Adaptation Feature Space Adaptation Label Space Adaptation Spectral Feature Alignment Subspace Feature Alignment ... ... Statistic Feature Alignment Feature Selection Fig. 3. Strategies and the objectives of the transfer learni ng approaches from the data perspective. 4 D ATA-BASED INTERPRETAT ION Many transfer learning approaches, especially the data- based approaches, focus on transferring the knowledge via the adjustment and the transformation of data. Fig. 3 shows the strategies and the objectives of the approaches from the data perspective. As shown in Fig. 3, space adaptation is one of the objectives. This objective is required to be sat- isﬁed mostly in heterogeneous transfer learning scenarios . In this survey , we focus more on homogeneous transfer learning, and the main objective in this scenario is to reduc e the distribution difference between the source-domain and the target-domain instances. Furthermore, some advanced approaches may attempt to preserve the data properties in the adaptation process. There are generally two strategies for realizing the objectives from the data perspective, i.e ., in- stance weighting and feature transformation. In this secti on, some related transfer learning approaches are introduced i n proper order according to the strategies shown in Fig. 3. 4.1 Instance Weighting Strategy Let us ﬁrst consider a simple scenario in which a large number of labeled source-domain and a limited number of target-domain instances are available and domains diffe r only in marginal distributions (i.e., P S(X)  P T (X) and P S(Y X)  P T (Y X)). For example, suppose we need to build a model to diagnose cancer in a speciﬁc region where the elderly are the majority . Limited target-domain instances are given, and relevant data are available from another region where young people are the majority . Di- rectly transferring all the data from another region may be unsuccessful, since the marginal distribution differen ce exists, and the elderly have a higher risk of cancer than younger people. In this scenario, it is natural to consider adapting the marginal distributions. A simple idea is to assign weights to the source-domain instances in the loss function. The weighting strategy is based on the following equation [5]: E(x,y )P T [L(x, y ; f)]  E(x,y )P S [ P T (x, y ) P S(x, y ) L(x, y ; f) ]  E(x,y )P S [ P T (x) P S(x) L(x, y ; f) ] . Therefore, the general objective function of a learning tas k can be written as [5]: min f 1 nS nS  i1 βiL ( f(xS i ), y S i )  Ω( f), where βi (i  1 , 2,  , n S) is the weighting parameter . The theoretical value of βi is equal to P T (xi)P S(xi). However , this ratio is generally unknown and is difﬁcult to be obtained by using the traditional methods. Kernel Mean Matching (KMM) [5], which is proposed by Huang et al. , resolves the estimation problem of the above unknown ratios by matching the means between the source- domain and the target-domain instances in a Reproducing Kernel Hilbert Space (RKHS), i.e., arg min β i[0,B ]             1 nS nS  i1 βiΦ( xS i )  1 nT nT  j1 Φ( xT j )             2 H s.t. 1 nS nS  i1 βi 1 δ, where δ is a small parameter , and B is a parameter for con- straint. The above optimization problem can be converted 6 into a quadratic programming problem by expanding and using the kernel trick. This approach to estimating the rati os of distributions can be easily incorporated into many exist - ing algorithms. Once the weight βi is obtained, a learner can be trained on the weighted source-domain instances. There are some other studies attempting to estimate the weights. For example, Sugiyama et al. proposed an approach termed Kullback-Leibler Importance Estimation Procedure (KLIEP) [6]. KLIEP depends on the minimization of the Kullback-Leibler (KL) divergence and incorporates a built-in model selection procedure. Based on the studies of weight estimation, some instance-based transfer learni ng frameworks or algorithms are proposed. For example, Sun et al. proposed a multi-source framework termed 2-Stage W eighting Framework for Multi-Source Domain Adaptation (2SW-MDA) with the following two stages [29]. 1. Instance Weighting : The source-domain instances are as- signed with weights to reduce the marginal distribution difference, which is similar to KMM. 2. Domain Weighting : W eights are assigned to each source domain for reducing the conditional distribution differ- ence based on the smoothness assumption [30]. Then, the source-domain instances are reweighted based on the instance weights and the domain weights. These reweighted instances and the labeled target-domain in- stances are used to train the target classiﬁer . In addition to directly estimating the weighting param- eters, adjusting weights iteratively is also effective. Th e key is to design a mechanism to decrease the weights of the instances that have negative effects on the target learner . A representative work is T rAdaBoost [31], which is a framework proposed by Dai et al . This framework is an extension of AdaBoost [32]. AdaBoost is an effective boosting algorithm designed for traditional machine learn - ing tasks. In each iteration of AdaBoost, a learner is traine d on the instances with updated weights, which results in a weak classiﬁer . The weighting mechanism of instances ensures that the instances with incorrect classiﬁcation ar e given more attention. Finally , the resultant weak classiﬁe rs are combined to form a strong classiﬁer . T rAdaBoost ex- tends the AdaBoost to the transfer learning scenario; a new weighting mechanism is designed to reduce the impact of the distribution difference. Speciﬁcally , in T rAdaBoost, the labeled source-domain and labeled target-domain instance s are combined as a whole, i.e., a training set, to train the weak classiﬁer . The weighting operations are different for the source-domain and the target-domain instances. In each iteration, a temporary variable δ, which measures the classi- ﬁcation error rate on the labeled target-domain instances, is calculated. Then, the weights of the target-domain instanc es are updated based on δ and the individual classiﬁcation results, while the weights of the source-domain instances a re updated based on a designed constant and the individual classiﬁcation results. For better understanding, the form ulas used in the k-th iteration ( k  1 ,  , N ) for updating the weights are presented repeatedly as follows [31]: βS k,i  βS k1,i (1   2 ln nSN )fk (xS i )yS i (i  1 ,  , n S), βT k,j  βT k1,j (δk (1 δk))fk (xT j )yT j (j  1 ,  , n T ). Note that each iteration forms a new weak classiﬁer . The ﬁnal classiﬁer is constructed by combining and ensembling half the number of the newly resultant weak classiﬁers through voting scheme. Some studies further extend T rAdaBoost. The work by Y ao and Doretto [33] proposes a Multi-Source T rAdaBoost (MsT rAdaBoost) algorithm, which mainly has the following two steps in each iteration. 1. Candidate Classiﬁer Construction : A group of candi- date weak classiﬁers are respectively trained on the weighted instances in the pairs of each source domain and the target domain, i.e., DSi DT (i  1 ,  , m S). 2. Instance Weighting : A classiﬁer (denoted by j and trained on DSj DT ) which has the minimal classi- ﬁcation error rate δ on the target domain instances is selected, and then is used for updating the weights of the instances in DSj and DT . Finally , the selected classiﬁers from each iteration are co m- bined to form the ﬁnal classiﬁer . Another parameter-based algorithm, i.e., T askT rAdaBoost, is also proposed in the work [33], which is introduced in Section 5.3. Some approaches realize instance weighting strategy in a heuristic way . For example, Jiang and Zhai proposed a general weighting framework [34]. There are three terms in the frameworks objective function, which are designed to minimize the cross-entropy loss of three types of instances . The following types of instances are used to construct the target classiﬁer .  Labeled T arget-domain Instance : The classiﬁer should mini- mize the cross-entropy loss on them, which is actually a standard supervised learning task.  Unlabeled T arget-domain Instance : These instances true con- ditional distributions P (yxT,U i ) are unknown and should be estimated. A possible solution is to train an auxiliary classiﬁer on the labeled source-domain and target-domain instances to help estimate the conditional distributions o r assign pseudo labels to these instances.  Labeled Source-domain Instance : The authors deﬁne the weight of xS,L i as the product of two parts, i.e., α i and βi. The weight βi is ideally equal to P T (xi)P S(xi), which can be estimated by non-parametric methods such as KMM or can be set uniformly in the worst case. The weight α i is used to ﬁlter out the source-domain instances that differ greatly from the target domain. A heuristic method can be used to produce the value of α i, which contains the following three steps. 1. Auxiliary Classiﬁer Construction : An auxiliary classiﬁer trained on the labeled target-domain instances are used to classify the unlabeled source-domain instances. 2. Instance Ranking : The source-domain instances are ranked based on the probabilistic prediction results. 3. Heuristic Weighting ( βi): The weights of the top- k source-domain instances with wrong predictions are set to zero, and the weights of others are set to one. 4.2 Feature T ransformation Strategy Feature transformation strategy is often adopted in featur e- based approaches. For example, consider a cross-domain text classiﬁcation problem. The task is to construct a targe t 7 T ABLE 2 Metrics Adopted in T ransfer Learning. Measurement Related Algorithms Maximum Mean Discrepancy [35] [36] [37] [38] [39]    Kullback-Leibler Divergence [40] [41] [42] [43] [44]    Jensen-Shannon Divergence [45] [46] [47] [48] [49]    Bregman Divergence [50] [51] [52] [53] [54]    Hilbert-Schmidt Independence Criterion [55] [36] [56] [57] [58]   classiﬁer by using the labeled text data from a related domain. In this scenario, a feasible solution is to ﬁnd the common latent features (e.g., latent topics) through fea- ture transformation and use them as a bridge to transfer knowledge. Feature-based approaches transform each orig- inal feature into a new feature representation for knowl- edge transfer . The objectives of constructing a new feature representation include minimizing the marginal and the conditional distribution difference, preserving the prop er- ties or the potential structures of the data, and ﬁnding the correspondence between features. The operations of featur e transformation can be divided into three types, i.e., featu re augmentation, feature reduction, and feature alignment. B e- sides, feature reduction can be further divided into severa l types such as feature mapping, feature clustering, feature selection, and feature encoding. A complete feature trans- formation process designed in an algorithm may consist of several operations. 4.2.1 Distribution Difference Metric One primary objective of feature transformation is to reduc e the distribution difference of the source and the target do- main instances. Therefore, how to measure the distribution difference or the similarity between domains effectively i s an important issue. The measurement termed Maximum Mean Discrepancy (MMD) is widely used in the ﬁeld of transfer learning, which is formulated as follows [35]: MMD(XS, X T )              1 nS nS  i1 Φ( xS i )  1 nT nT  j1 Φ( xT j )             2 H . MMD can be easily computed by using kernel trick. Brieﬂy , MMD quantiﬁes the distribution difference by calculating the distance of the mean values of the instances in a RKHS. Note that the above-mentioned KMM actually produces the weights of instances by minimizing the MMD distance between domains. T able. 2 lists some commonly used metrics and the related algorithms. In addition to T able. 2, there are some other measurement criteria adopted in transfer learning, including W asserstein distance [59], [60], central moment discrepancy [61], etc. Some studies focus on optimizing and improving the existing measurements. T ake MMD as an example. Gretton et al. proposed a multi-kernel version of MMD, i.e., MK-MMD [62], which takes advantage of multiple kernels. Besides, Y an et al. proposed a weighted version of MMD [63], which attempts to address the issue of class weight bias. 4.2.2 Feature Augmentation Feature augmentation operations are widely used in fea- ture transformation, especially in symmetric feature-bas ed approaches. T o be more speciﬁc, there are several ways to realize feature augmentation such as feature replication a nd feature stacking. For better understanding, we start with a simple transfer learning approach which is established based on feature replication. The work by Daum  e proposes a simple domain adap- tation method, i.e., Feature Augmentation Method (F AM) [64]. This method transforms the original features by sim- ple feature replication. Speciﬁcally , in single-source tr ansfer learning scenario, the feature space is augmented to three times its original size. The new feature representation con - sists of general features, source-speciﬁc features, and ta rget- speciﬁc features. Note that, for the transformed source- domain instances, their target-speciﬁc features are set to zero. Similarly , for the transformed target-domain instan ces, their source-speciﬁc features are set to zero. The new featu re representation of F AM is presented as follows: Φ S(xS i )  xS i , xS i , 0, Φ T (xT j )  xT j , 0, xT j , where Φ S and Φ T denote the mappings to the new feature space from the source and the target domain, respectively . The ﬁnal classiﬁer is trained on the transformed labeled instances. It is worth mentioning that this augmentation method is actually redundant. In other words, augmenting the feature space in other ways (with fewer dimensions) may be able to produce competent performance. The supe- riority of F AM is that its feature expansion has an elegant form, which results in some good properties such as the generalization to multi-source scenarios. An extension of F AM is proposed in [65] by Daum  e et al. , which utilizes the unlabeled instances to further facilitate the knowledg e transfer process. However , F AM may not work well in handling het- erogeneous transfer learning tasks. The reason is that di- rectly replicating features and padding zero vectors are le ss effective when the source and the target domains have different feature representations. T o solve this problem, Li et al. proposed an approach termed Heterogeneous Feature Augmentation (HF A) [66], [67]. The feature representation of HF A is presented below: Φ S(xS i )  W SxS i , xS i , 0T , Φ T (xT j )  W T xT j , 0S, xT j , where W SxS i and W T xT j have the same dimension; 0S and 0T denote the zero vectors with the dimensions of xS and xT , respectively . HF A maps the original features into a common feature space, and then performs a feature stacking operation. The mapped features, original features, and zer o elements are stacked in a particular order to produce a new feature representation. 4.2.3 Feature Mapping In the ﬁeld of traditional machine learning, there are many feasible mapping-based methods of extracting fea- tures such as Principal Component Analysis (PCA) [68] and Kernelized-PCA (KPCA) [69]. However , these methods mainly focus on the data variance rather than the distribu- tion difference. In order to solve the distribution differe nce, 8 some feature extraction methods are proposed for transfer learning. Let us ﬁrst consider a simple scenario where there is little difference in the conditional distributions of th e do- mains. In this case, the following simple objective functio n can be used to ﬁnd a mapping for feature extraction: min Φ ( DIST(XS, X T ; Φ)  λΩ(Φ) )  ( V AR(XS XT ; Φ) ) , where Φ is a low-dimensional mapping function, DIST () represents a distribution difference metric, Ω(Φ) is a regular- izer controlling the complexity of Φ , and V AR () represents the variance of instances. This objective function aims to ﬁnd a mapping function Φ that minimizes the marginal distribution difference between domains and meanwhile makes the variance of the instances as large as possible. The objective corresponding to the denominator can be opti- mized in several ways. One possible way is to optimize the objective of the numerator with a variance constraint. For example, the scatter matrix of the mapped instances can be enforced as an identity matrix. Another way is to optimize the objective of the numerator in a high-dimensional featur e space at ﬁrst. Then, a dimension reduction algorithm such as PCA or KPCA can be performed to realize the objective of the denominator . Further , ﬁnding the explicit formulation of Φ( ) is non- trivial. T o solve this problem, some approaches adopt linea r mapping technique or turn to the kernel trick. In general, there are three main ideas to deal with the above optimiza- tion problems.  (Mapping Learning  Feature Extraction) A possible way is to ﬁnd a high-dimensional space at ﬁrst where the objec- tives are met by solving a kernel matrix learning problem or a transformation matrix ﬁnding problem. Then, the high-dimensional features are compacted to form a low- dimensional feature representation. For example, once the kernel matrix is learned, the principal components of the implicit high-dimensional features can be extracted to construct a new feature representation based on PCA.  (Mapping Construction  Mapping Learning) Another way is to map the original features to a constructed high- dimensional feature space, and then a low-dimensional mapping is learned to satisfy the objective function. For example, a kernel matrix can be constructed based on a selected kernel function at ﬁrst. Then, the transfor- mation matrix can be learned, which projects the high- dimensional features into a common latent subspace.  (Direct Low-dimensional Mapping Learning) It is usu- ally difﬁcult to ﬁnd a desired low-dimensional mapping directly . However , if the mapping is assumed to satisfy certain conditions, it may be solvable. For example, if the low-dimensional mapping is restricted to be a linear one, the optimization problem can be easily solved. Some approaches also attempt to match the conditional distributions and preserve the structures of the data. T o achieve this, the above simple objective function needs to incorporate new terms orand constraints. For example, the following general objective function is a possible choice: min Φ µDIST(XS, X T ; Φ)  λ1Ω GEO (Φ)  λ2Ω(Φ)  (1 µ)DIST(Y SXS, Y T XT ; Φ) , s.t. Φ( X)T HΦ( X)  I, with H  I (1n ) Rnn, where µ is a parameter balancing the marginal and the conditional distribution difference [70], Ω GEO(Φ) is a reg- ularizer controlling the geometric structure, Φ( X) is the matrix whose rows are the instances from both the source and the target domains with the extracted new feature representation, H is the centering matrix for constructing the scatter matrix, and the constraint is used to maximize the variance. The last term in the objective function denote s the measurement of the conditional distribution differenc e. Before the further discussion about the above objective function, it is worth mentioning that the label information of the target-domain instances is often limited or even unknown. The lack of the label information makes it difﬁcult to estimate the distribution difference. In order to solve this problem, some approaches resort to the pseudo-label strategy , i.e., assigning pseudo labels to the unlabeled ta rget- domain instances. A simple method of realizing this is to train a base classiﬁer to assign pseudo labels. By the way , there are some other methods of providing pseudo labels such as co-training [71], [72] and tri-training [73] , [74]. Once the pseudo-label information is complemented, the conditional distribution difference can be measured. F or example, MMD can be modiﬁed and extended to measure the conditional distribution difference. Speciﬁcally , fo r each label, the source-domain and the target-domain instances that belong to the same class are collected, and the estima- tion expression of the conditional distribution differenc e is given by [38]: Y k1             1 nS k nS k i1 Φ( xS i )  1 nT k nT k j1 Φ( xT j )             2 H , where nS k and nT k denote the numbers of the instances in the source and the target domains with the same label Yk, respectively . This estimation actually measures the class - conditional distribution (i.e., P (xy)) difference to approx- imate the conditional distribution (i.e., P (yx)) difference. Some studies improve the above estimation. For example, the work by W ang et al. uses a weighted method to ad- ditionally solve the class imbalance problem [70]. For bett er understanding, the transfer learning approaches that are t he special cases of the general objective function presented i n the previous paragraph are detailed as follows.  (µ  1 and λ1  0 ) The objective function of Maximum Mean Discrepancy Embedding (MMDE) is given by [75]: min K MMD(XS, X T ; Φ)  λ1 nS  nT  ij Φ( xi) Φ( xj )2 s.t. (xi k-NN(xj )) (xj k-NN(xi)), Φ( xi) Φ( xj )2  xi xj 2, (xi, xj XS XT ), where k-NN(x) represents the k nearest neighbors of the instance x. The authors design the above objective func- tion motivated by Maximum V ariance Unfolding (MVU) [76]. Instead of employing a scatter matrix constraint, the constraints and the second term of this objective function aim to maximize the distance between instances as well as preserve local geometry . The desired kernel matrix K can be learned by solving a Semi-Deﬁnite Programming (SDP) [77] problem. After obtaining the kernel matrix, 9 PCA is applied to it, and then the leading eigenvectors are selected to help construct a low-dimensional feature representation.  (µ  1 and λ1  0 ) The work by Pan et al. proposes an approach termed T ransfer Component Analysis (TCA) [36], [78]. TCA adopts MMD to measure the marginal dis- tribution difference and enforces the scatter matrix as the constraint. Different from MMDE that learns the kernel matrix and then further adopts PCA, TCA is a uniﬁed method that just needs to learn a linear mapping from an empirical kernel feature space to a low-dimensional fea- ture space. In this way , it avoids solving the SDP problem, which results in relatively low computational burden. The ﬁnal optimization problem can be easily solved via eigen- decomposition. TCA can also be extended to utilize the label information. In the extended version, the scatter matrix constraint is replaced by a new one that balances the label dependence (measured by HSIC) and the data variance. Besides, a graph Laplacian regularizer [30] is also added to preserve the geometry of the manifold. Sim- ilarly , the ﬁnal optimization problem can also be solved by eigen-decomposition.  (µ  0 . 5 and λ1  0 ) Long et al. proposed an ap- proach termed Joint Distribution Adaptation (JDA) [38]. JDA attempts to ﬁnd a transformation matrix that maps the instances to a low-dimensional space where both the marginal and the conditional distribution difference are minimized. T o realize it, the MMD metric and the pseudo- label strategy are adopted. The desired transformation matrix can be obtained by solving a trace optimization problem via eigen-decomposition. Further , it is obvious that the accuracy of the estimated pseudo labels affects the performance of JDA. In order to improve the labeling quality , the authors adopt the iterative reﬁnement oper- ations. Speciﬁcally , in each iteration, JDA is performed, and then a classiﬁer is trained on the instances with the extracted features. Next, the pseudo labels are updated based on the trained classiﬁer . After that, JDA is per- formed repeatedly with the updated pseudo labels. The iteration ends when convergence occurs. Note that JDA can be extended by utilizing the label and structure infor- mation [79], clustering information [80], various statist ical and geometrical information [81], etc.  (µ (0, 1) and λ1  0 ) The paper by W ang et al. proposes an approach termed Balanced Distribution Adaptation (BDA) [70], which is an extension of JDA. Different from JDA which assumes that the marginal and the conditional distributions have the same importance in adaptation, BDA attempts to balance the marginal and the condi- tional distribution adaptation. The operations of BDA are similar to JDA. In addition, the authors also proposed the W eighted BDA (WBDA). In WBDA, the conditional distribution difference is measured by a weighted version of MMD to solve the class imbalance problem. It is worth mentioning that some approaches transform the features into a new feature space (usually of a high dimension) and train an adaptive classiﬁer simultaneously . T o realize this, the mapping function of the features and the decision function of the classiﬁer need to be associated. On e possible way is to deﬁne the following decision function: f(x)  θ Φ( x)b, where θ denotes the classiﬁer parameter; b denotes the bias. In light of the representer theorem [82], the parameter θ can be deﬁned as θ   n i1 α iΦ( xi), and thus we have f(x)  n i1 α iΦ( xi) Φ( x)  b  n i1 α iκ(xi, x)  b, where κ denotes the kernel function. By using the kernel matrix as the bridge, the regularizers designed for the map- ping function can be incorporated into the classiﬁer s obje c- tive function. In this way , the ﬁnal optimization problem is usually about the parameter (e.g., α i) or the kernel function. For example, the paper by Long et al. proposes a general framework termed Adaptation Regularization Based T rans- fer Learning (ARTL) [39]. The goals of ARTL are to learn the adaptive classiﬁer , to minimize the structural risk, to jointly reduce the marginal and the conditional distributi on difference, and to maximize the manifold consistency be- tween the data structure and the predictive structure. The authors also proposed two speciﬁc algorithms under this framework based on different loss functions. In these two algorithms, the coefﬁcient matrix for computing MMD and the graph Laplacian matrix for manifold regularization are constructed at ﬁrst. Then, a kernel function is selected to construct the kernel matrix. After that, the classiﬁer lear ning problem is converted into a parameter (i.e., α i) solving problem, and the solution formula is also given in [39]. In ARTL, the choice of the kernel function affects the performance of the ﬁnal classiﬁer . In order to construct a robust classiﬁer , some studies turn to kernel learning. For example, the paper by Duan et al. proposes a uni- ﬁed framework termed Domain T ransfer Multiple Kernel Learning (DTMKL) [83]. In DTMKL, the kernel function is assumed to be a linear combination of a group of base kernels, i.e., κ(xi, xj )   N k1 βkκk(xi, xj ). DTMKL aims to minimize the distribution difference, the classiﬁcatio n error , etc., simultaneously . The general objective functi on of DTMKL can be written as follows: min β k,f σ ( MMD(XS, X T ; κ) )  λΩ L(βk, f ), where σ is any monotonically increasing function, f is the decision function with the same deﬁnition as the one in ARTL, and Ω L(βk, f ) is a general term representing a group of regularizers deﬁned on the labeled instances such as the ones for minimizing the classiﬁcation error and controllin g the complexity of the resultant model. The authors devel- oped an algorithm to learn the kernel and the decision function simultaneously by using the reduced gradient de- scent method [84]. In each iteration, the weight coefﬁcient s of base kernels are ﬁxed to update the decision function at ﬁrst. Then, the decision function is ﬁxed to update the weight coefﬁcients. Note that DTMKL can incorporate many existing kernel methods. The authors proposed two speciﬁc algorithms under this framework. The ﬁrst one implements the framework by using hinge loss and Support V ector Machine (SVM). The second one is an extension of the ﬁrst one with an additional regularizer utilizing pseudo- label information, and the pseudo labels of the unlabeled instances are generated by using base classiﬁers. 10 4.2.4 Feature Clustering Feature clustering aims to ﬁnd a more abstract feature representation of the original features. Although it can be regarded as a way of feature extraction, it is different from the above-mentioned mapping-based extraction. For example, some transfer learning approaches implic- itly reduce the features by using the co-clustering tech- nique, i.e., simultaneously clustering both the columns an d rows of (or say , co-cluster) a contingency table based on the information theory [85]. The paper by Dai et al. [41] proposes an algorithm termed Co-Clustering Based Clas- siﬁcation (CoCC), which is used for document classiﬁca- tion. In a document classiﬁcation problem, the transfer learning task is to classify the target-domain documents (represented by a document-to-word matrix) with the help of the labeled source document-to-word data. CoCC re- gards the co-clustering technique as a bridge to transfer the knowledge. In CoCC algorithm, both the source and the target document-to-word matrices are co-clustered. Th e source document-to-word matrix is co-clustered to generat e the word clusters based on the known label information, and these word clusters are used as constraints during the co-clustering process of the target-domain data. The co- clustering criterion is to minimize the loss in mutual infor - mation, and the clustering results are obtained by iteratio n. Each iteration contains the following two steps. 1. Document Clustering : Each row of the target document- to-word matrix is re-ordered based on the objective function for updating the document clusters. 2. Word Clustering : The word clusters are adjusted to min- imize the joint mutual-information loss of the source and the target document-word matrices. After several times of iterations, the algorithm converges , and the classiﬁcation results are obtained. Note that, in CoCC, the word clustering process implicitly extracts the word features to form uniﬁed word clusters. Dai et al. also proposed an unsupervised clustering ap- proach, which is termed as Self-T aught Clustering (STC) [42]. Similar to CoCC, this algorithm is also a co-clusterin g- based one. However , STC does not need the label infor- mation. STC aims to simultaneously co-cluster the source- domain and the target-domain instances with the assump- tion that these two domains share the same feature clusters in their common feature space. Therefore, two co-clusterin g tasks are separately performed at the same time to ﬁnd the shared feature clusters. Each iteration of STC has the following steps. 1. Instance Clustering : The clustering results of the source- domain and the target domain instances are updated to minimize their respective loss in mutual information. 2. Feature Clustering : The feature clusters are updated to minimize the joint loss in mutual information. When the algorithm converges, the clustering results of the target-domain instances are obtained. Different from the above-mentioned co-clustering-based ones, some approaches extract the original features into co n- cepts (or topics). In the document classiﬁcation problem, t he concepts represent the high-level abstractness of the word s (e.g., word clusters). In order to introduce the concept-ba sed transfer learning approaches easily , let us brieﬂy review t he Latent Semantic Analysis (LSA) [86], the Probabilistic LSA (PLSA) [87], and the Dual-PLSA [88].  LSA: LSA is an approach to mapping the document-to- word matrix to a low-dimensional space (i.e., a latent se- mantic space) based on the SVD technique. In short, LSA attempts to ﬁnd the true meanings of the words. T o realize this, SVD technique is used to reduce the dimensionality , which can remove the irrelevant information and ﬁlter out the noise information from the raw data.  PLSA: PLSA is developed based on a statistical view of LSA. PLSA assumes that there is a latent class variable z, which reﬂects the concept, associating the document d and the word w. Besides, d and w are independently con- ditioned on the concept z. The diagram of this graphical model is presented as follows: d P (dizk)  P (zk )  z P (wj zk) w, where the subscripts i, j and k represent the indexes of the document, the word, and the concept, respectively . PLSA constructs a Bayesian network, and the parameters are estimated by using the Expectation-Maximization (EM) algorithm [89].  Dual-PLSA: The Dual-PLSA is an extension of PLSA. This approach assumes there are two latent variables zd and zw associating the documents and the words. Speciﬁcally , the variables zd and zw reﬂect the concepts behind the documents and the words, respectively . The diagram of the Dual-PLSA is provided below: d P (dizd k1 ) zd P (zd k1 ,z w k2 ) zw P (wj zw k2 ) w. The parameters of the Dual-PLSA can also be obtained based on the EM algorithm. Some concept-based transfer learning approaches are established based on PLSA. For example, the paper by Xue et al. proposes a cross-domain text classiﬁcation approach termed T opic-Bridged Probabilistic Latent Semantic Analy - sis (TPLSA) [90]. TPLSA, which is an extension of PLSA, assumes that the source-domain and the target-domain instances share the same mixing concepts of the words. Instead of performing two PLSAs for the source domain and the target domain separately , the authors merge those two PLSAs as an integrated one by using the mixing concept z as a bridge, i.e., each concept has some probabilities to produ ce the source-domain and the target-domain documents. The diagram of TPLSA is provided below:    dS dT տ ւ P (dS i zk)        P (dT i zk) z P (zkwj ) w. Note that PLSA does not require the label information. In order to exploit the label information, the authors add the concept constraints, which include must-link and cannot- link constraints, as the penalty terms in the objective function of TPLSA. Finally , the objective function is iter- atively optimized to obtain the classiﬁcation results (i.e ., arg maxzP (zdT i )) by using the EM algorithm. The work by Zhuang et al. proposes an approach termed Collaborative Dual-PLSA (CD-PLSA) for multi-domain text classiﬁcation ( mS source domains and mT target domains) 11 [91], [92]. CD-PLSA is an extension of Dual-PLSA. Its dia- gram is shown below: P (Dk0 )  D  P (dizd k1 , Dk0 )  d zd P (zd k1 ,z w k2 ) zw  P (wj zw k2 , Dk0 )  w ց ր , where 1  k0  mS  mT denotes the domain index. The domain Dconnects both the variables d and w, but is independent of the variables zd and zw. The label in- formation of the source-domain instances is utilized by initializing the value P (dizd k1 , Dk0 ) (k0  1 ,  , m S). Due to the lack of the target-domain label information, the value P (dizd k1 , Dk0 ) (k0  mS  1 ,  , m S  mT ) can be initialized based on any supervised classiﬁer . Similarl y , the authors adopt the EM algorithm to ﬁnd the param- eters. Through the iterations, all the parameters in the Bayesian network are obtained. Thus, the class label of the i-th document in a target domain (denoted by Dk) can be predicted by computing the posterior probabilities, i.e ., arg maxzd P (zddi, Dk). Zhuang et al. further proposed a general framework that is termed as Homogeneous-Identical-Distinct-Concep t Model (HIDC) [93]. This framework is also an extension of Dual-PLSA. HIDC is composed of three generative models, i.e., identical-concept, homogeneous-concept, and disti nct- concept models. These three graphical models are presented below: Identical-Concept Model: D d zd ց ր zw IC w, Homogeneous-Concept Model: ր ց D d zd ց ր zw HC w, Distinct-Concept Model: ր ց ց D d zd ց ր zw DC w . The original word concept zw is divided into three types, i.e., zw IC, zw HC , and zw DC. In the identical-concept model, the word distributions only rely on the word concepts, and the word concepts are independent of the domains. However , in the homogeneous-concept model, the word distributions also depend on the domains. The difference between the identical and the homogeneous concepts is that zw IC is di- rectly transferable, while zw HC is the domain-speciﬁc transfer- able one that may have different effects on the word distri- butions for different domains. In the distinct-concept mod el, zw DC is actually the nontransferable domain-speciﬁc one, which may only appear in a speciﬁc domain. The above- mentioned three models are combined as an integrated one, i.e., HIDC. Similar to other PLSA-related algorithms, HIDC also uses EM algorithm to get the parameters. 4.2.5 Feature Selection Feature selection is another kind of operation for feature reduction, which is used to extract the pivot features. The pivot features are the ones that behave in the same way in different domains. Due to the stability of these features , they can be used as the bridge to transfer the knowledge. For example, Blitzer et al. proposed an approach termed Structural Correspondence Learning (SCL) [94]. Brieﬂy , SC L consists of the following steps to construct a new feature representation. 1. Feature Selection : SCL ﬁrst performs feature selection operations to obtain the pivot features. 2. Mapping Learning : The pivot features are utilized to ﬁnd a low-dimensional common latent feature space by using the structural learning technique [95]. 3. Feature Stacking : A new feature representation is con- structed by feature augmentation, i.e., stacking the original features with the obtained low-dimensional features. T ake the part-of-speech tagging problem as an example. The selected pivot features should occur frequently in source and target domains. Therefore, determiners can be included in pivot features. Once all the pivot features are deﬁned and selected, a number of binary linear classiﬁers whose function is to predict the occurrence of each pivot feature a re constructed. Without losing generality , the decision func tion of the i-th classiﬁer , which is used to predict the i-th pivot feature, can be formulated as fi(x)  sign(θi x), where x is assumed to be a binary feature input. And the i-th classiﬁer is trained on all the instances excluding the features deriv ed from the i-th pivot feature. The following formula can be used to estimate the i-th classiﬁer s parameters, i.e., θi  arg min θ 1 n n j1 L(θ xj , Rowi(xj ))  λθ2, where Row i(xj ) denotes the true value of the unlabeled instance xj in terms of the i-th pivot feature. By stacking the obtained parameter vectors as column elements, a matrix W is obtained. Next, based on singular value decomposition (SVD), the top- k left singular vectors, which are the prin- cipal components of the matrix W , are taken to construct the transformation matrix W . At last, the ﬁnal classiﬁer is trained on the labeled instances in an augmented feature space, i.e., ([xL i ; W T xL i ]T , y L i ). 4.2.6 Feature Encoding In addition to feature extraction and selection, feature en - coding is also an effective tool. For example, autoencoders , which are often adopted in deep learning area, can be used for feature encoding. An autoencoder consists of an encoder and a decoder . The encoder tries to produce a more abstract representation of the input, while the decoder aims to map back that representation and to minimize the reconstructio n error . Autoencoders can be stacked to build a deep learning architecture. Once an autoencoder completes the training process, another autoencoder can be stacked at the top of it. The newly added autoencoder is then trained by using the encoded output of the upper-level autoencoder as its input. In this way , deep learning architectures can thus be constructed. Some transfer learning approaches are developed based on autoencoders. For example, the paper by Glorot et al. proposes an approach termed Stacked Denoising Autoen- coder (SDA) [96]. The denoising autoencoder , which can enhance the robustness, is an extension of the basic one [97] . This kind of autoencoder contains a randomly corrupting mechanism that adds noise to the input before mapping. For 12 example, an input can be corrupted or partially destroyed by adding a masking noise or Gaussian noise. The denoising autoencoder is then trained to minimize the denoising re- construction error between the original clean input and the output. The SDA algorithm proposed in the paper mainly encompasses the following steps. 1. Autoencoder T raining : The source-domain and target- domain instances are used to train a stack of denoising autoencoders in a greedy layer-by-layer way . 2. Feature Encoding  Stacking: A new feature representa- tion is constructed by stacking the encoding output of intermediate layers, and the features of the instances are transformed into the obtained new representation. 3. Learner T raining : The target classiﬁer is trained on the transformed labeled instances. Although the SDA algorithm has excellent performance for feature extraction, it still has some drawbacks such as high computational and parameter-estimation cost. In orde r to shorten the training time and to speed up traditional SDA algorithms, Chen et al. proposed a modiﬁed version of SDA, i.e., Marginalized Stacked Linear Denoising Au- toencoder (mSLDA) [98], [99]. This algorithm adopts linear autoencoders and marginalizes the randomly corrupting step in a closed form. It may seem that linear autoencoders are too simple to learn complex features. However , the authors observe that linear autoencoders are often sufﬁcie nt to achieve competent performance when encountering high dimensional data. The basic architecture of mSLDA is a single-layer linear autoencoder . The corresponding singl e- layer mapping matrix W (augmented with a bias column for convenience) should minimize the expected squared reconstruction loss function, i.e., W  arg min W 1 2n n i1 EP (xix) [ xi W xi2] , where xi denotes the corrupted version of the input xi. The solution of W is given by [98], [99]: W  ( n i1 xiE[xi]T )( n i1 E [ xi xT i ] ) 1 . When the corruption strategy is determined, the above for- mulas can be further expanded and simpliﬁed into a speciﬁc form. Note that, in order to insert nonlinearity , a nonlinea r function is used to squash the output of each autoencoder after we obtain the matrix W in a closed form. Then, the next linear autoencoder is stacked to the current one in a similar way to SDA. In order to deal with high dimensional data, the authors also put forward an extension approach to further reduce the computational complexity . 4.2.7 Feature Alignment Note that feature augmentation and feature reduction mainly focus on the explicit features in a feature space. In contrast, in addition to the explicit features, feature alignment also focuses on some implicit features such as the statistic features and the spectral features. Therefor e, feature alignment can play various roles in the feature transformation process. For example, the explicit feature s can be aligned to generate a new feature representation, or the implicit features can be aligned to construct a satisﬁed feature transformation. There are several kinds of features that can be aligned, which includes subspace features, spectral features, and statistic features. T ake the subspace feature alignment as an example. A typical approach mainly has the following steps. 1. Subspace Generation : In this step, the instances are used to generate the respective subspaces for the source and the target domains. The orthonormal bases of the source and the target domain subspaces are then obtained, which are denoted by MS and MT , respectively . These bases are used to learn the shift between the subspaces. 2. Subspace Alignment : In the second step, a mapping, which aligns the bases MS and MT of the subspaces, is learned. And the features of the instances are pro- jected to the aligned subspaces to generate new feature representation. 3. Learner T raining: Finally , the target learner is trained on the transformed instances. For example, the work by Fernando et al. proposes an approach termed Subspace Alignment (SA) [100]. In SA, the subspaces are generated by performing PCA; the bases MS and MT are obtained by selecting the leading eigenvectors. Then, a transformation matrix W is learned to align the subspaces, which is given by [100]: W  arg min W MSW MT 2 F  MT SMT , where  F denotes the Frobenius norm. Note that the matrix W aligns MS with MT , or say , transforms the source subspace coordinate system into the target subspace coor- dinate system. The transformed low-dimensional source- domain and target-domain instances are given by XSMSW and XT MT , respectively . Finally , a learner can be trained on the resultant transformed instances. In light of SA, a number of transfer learning approaches are established. For example, the paper by Sun and Saenko proposes an approach that aligns both the subspace bases and the distributions [101], which is termed as Subspace Distribution Alignment between T wo Subspaces (SDA-TS). In SDA-TS, the transformation matrix W is formulated as W  MT SMT Q, where Q is a matrix used to align the distribution difference. The transformation matrix W in SA is a special case of the one in SDA-TS by setting Q to an identity matrix. Note that SA is a symmetrical feature-base d approach, while SDA-TS is an asymmetrical one. In SDA- TS, the labeled source-domain instances are projected to th e source subspace, then mapped to the target subspace, and ﬁnally mapped back to the target domain. The transformed source-domain instances are formulated as XSMSW M T T . Another representative subspace feature alignment ap- proach is Geodesic Flow Kernel (GFK) [102], which is pro- posed by Gong et al . GFK is closely related to a previous ap- proach termed Geodesic Flow Subspaces (GFS) [103]. Before introducing GFK, let us review the steps of GFS at ﬁrst. GFS is inspired by incremental learning. Intuitively , utilizi ng the information conveyed by the potential path between two domains may be beneﬁcial to the domain adaptation. GFS generally takes the following steps to align features. 13 1. Subspace Generation : GFS ﬁrst generates two subspaces of the source and the target domains by performing PCA, respectively . 2. Subspace Interpolation : The two obtained subspaces can be viewed as two points on the Grassmann manifold [104]. A ﬁnite number of the interpolated subspaces are generated between these two subspaces based on the geometric properties of the manifold. 3. Feature Projection  Stacking: The original features are transformed by stacking the corresponding projections from all the obtained subspaces. Despite the usefulness and superiority of GFS, there is a problem about how to determine the number of the interpo- lated subspaces. GFK resolves this problem by integrating inﬁnite number of the subspaces located on the geodesic curve from the source subspace to the target one. The key of GFK is to construct an inﬁnite-dimensional feature space that incorporating the information of all the subspaces lyi ng on the geodesic ﬂow . In order to compute the inner product in the resultant inﬁnite-dimensional space, the geodesic- ﬂow kernel is deﬁned and derived. In addition, a subspace- disagreement measure is proposed to select the optimal dimensionality of the subspaces; a rank-of-domain metric is also proposed to select the optimal source domain when multi-source domains are available. Statistic feature alignment is another kind of feature alignment. For example, Sun et al. proposed an approach termed Co-Relation Alignment (CORAL) [105]. CORAL constructs the transformation matrix of the source feature s by aligning the second-order statistic features, i.e., the co- variance matrices. The transformation matrix W is given by [105]: W  arg min W W T CS W CT 2 F , where C denotes the covariance matrix. Note that, com- pared to the above subspace-based approaches, CORAL avoids subspace generation as well as projection and is very easy to implement. Some transfer learning approaches are established based on spectral feature alignment. In traditional machine lear n- ing area, spectral clustering is a clustering technique bas ed on graph theory . The key of this technique is to utilize the spectrum, i.e., eigenvalues, of the similarity matrix t o reduce the dimension of the features before clustering. The similarity matrix is constructed to quantitatively assess the relative similarity of each pair of datavertices. On the basis of spectral clustering and feature alignment, Spectr al Feature Alignment (SF A) [106] is proposed by Pan et al . SF A is an algorithm for sentiment classiﬁcation. This algorith m tries to identify the domain-speciﬁc words and domain- independent words in different domains, and then aligns these domain-speciﬁc word features to construct a low- dimensional feature representation. SF A generally contai ns the following ﬁve steps. 1. Feature Selection : In this step, feature selection operations are performed to select the domain- independentpivot features. The paper presents three strategies to select domain-independent features. These strategies are based on the occurrence frequency of words, the mutual information between features and labels [107], and the mutual information between fea- tures and domains, respectively . 2. Similarity Matrix Construction : Once the domain-speciﬁc and the domain-independent features are identiﬁed, a bipartite graph is constructed. Each edge of this bipar- tite graph is assigned with a weight that measures the co-occurrence relationship between a domain-speciﬁc word and a domain-independent word. Based on the bipartite graph, a similarity matrix is then constructed. 3. Spectral Feature Alignment : In this step, a spectral clus- tering algorithm is adapted and performed to align domain-speciﬁc features [108], [109]. Speciﬁcally , based on the eigenvectors of the graph Laplacian, a feature alignment mapping is constructed, and the domain- speciﬁc features are mapped into a low-dimensional feature space. 4. Feature Stacking : The original features and the low- dimensional features are stacked to produce the ﬁnal feature representation. 5. Learner T raining : The target learner is trained on the labeled instances with the ﬁnal feature representation. There are some other spectral transfer learning ap- proaches. For example, the work by Ling et al. proposes an approach termed Cross-Domain Spectral Classiﬁer (CDSC) [110]. The general ideas and steps of this approach are presented as follows. 1. Similarity Matrix Construction : In the ﬁrst step, two similarity matrices are constructed corresponding to the whole instances and the target-domain instances, respectively . 2. Spectral Feature Alignment : An objective function is de- signed with respect to a graph-partition indicator vec- tor; a constraint matrix is constructed, which contains pair-wise must-link information. Instead of seeking the discrete solution of the indicator vector , the solution is relaxed to be continuous, and the eigen-system problem corresponding to the objective function is solved to construct the aligned spectral features [111]. 3. Learner T raining: A traditional classiﬁer is trained on the transformed instances. T o be more speciﬁc, the objective function has a form of the generalized Rayleigh quotient, which aims to ﬁnd the optimal graph partition that respects the label informatio n with small cut-size [112], to maximize the separation of the target-domain instances, and to ﬁt the constraints of the pair-wise property . After eigen-decomposition, the la st eigenvectors are selected and combined as a matrix, and then the matrix is normalized. Each row of the normalized matrix represents a transformed instance. 5 M ODEL -BASED INTERPRETATIO N T ransfer learning approaches can also be interpreted from the model perspective. Fig. 4 shows the corresponding strategies and the objectives. The main objective of a trans fer learning model is to make accurate prediction results on the target domain, e.g., classiﬁcation or clustering results. Note that a transfer learning model may consist of a few sub- modules such as classiﬁers, extractors, or encoders. These sub-modules may play different roles, e.g., feature adapta - tion or pseudo label generation. In this section, some relat ed 14 Model-Based Interpretation Objective Domain Adaptation ... Strategy Model Ensemble Model Selection Parameter Sharing Prediction Making Parameter Control Deep Learning Technique Parameter Restriction Voting Strategy Weighting Strategy ... ... Traditional Deep Learning Adversarial Deep Learning ... Model Control Consensus Regularizer Domain-Dependent Regularizer ... Pseudo Label Generation Fig. 4. Strategies and objectives of the transfer learning a pproaches from the model perspective. transfer learning approaches are introduced in proper orde r according to the strategies shown in Fig. 4. 5.1 Model Control Strategy From the perspective of model, a natural thought is to directly add the model-level regularizers to the learner s objective function. In this way , the knowledge contained in the pre-obtained source models can be transferred into the target model during the training process. For example, the paper by Duan et al. proposes a general framework termed Domain Adaptation Machine (DAM) [113], [114], which is designed for multi-source transfer learning. The goal of DAM is to construct a robust classiﬁer for the target domain with the help of some pre-obtained base classiﬁers that are respectively trained on multiple source domains. The objective function is given by: min fT LT,L (fT )  λ1Ω D (fT )  λ2Ω( fT ), where the ﬁrst term represents the loss function used to min- imize the classiﬁcation error of the labeled target-domain in- stances, the second term denotes different regularizers, a nd the third term is used to control the complexity of the ﬁnal decision function fT . Different types of the loss functions can be adopted in LT,L (fT ) such as the square error or the cross-entropy loss. Some transfer learning approaches can be regarded as the special cases of this framework to some extent.  (Consensus Regularizer) The work by Luo et al. proposes a framework termed Consensus Regularization Frame- work (CRF) [115], [116]. CRF is designed for multi-source transfer learning with no labeled target-domain instances . The framework constructs mS classiﬁers corresponding to each source domain, and these classiﬁers are required to reach mutual consensuses on the target domain. The objective function of each source classiﬁer , denoted by fS k (with k  1 ,  , m S), is similar to that of DAM, which is presented below: min fS k  nSk  i1 log P (ySk i xSk i ; fS k )  λ2Ω( fS k ) λ1 nT,U  i1  yj Y S ( 1 mS mS  k01 P (yj xT,U i ; fS k0 ) ) , where fS k denotes the decision function corresponding to the k-th source domain, and S(x)  x log x. The ﬁrst term is used to quantify the classiﬁcation error of the k-th classiﬁer on the k-th source domain, and the last term is the consensus regularizer in the form of cross- entropy . The consensus regularizer can not only enhance the agreement of all the classiﬁers, but also reduce the uncertainty of the predictions on the target domain. The authors implement this framework based on the logistic regression. A difference between DAM and CRF is that DAM explicitly constructs the target classiﬁer , while CRF makes the target predictions based on the reached consen- sus from the source classiﬁers.  (Domain-dependent Regularizer) Fast-DAM is a speciﬁc algorithm of DAM [113]. In light of the manifold assump- tion [30] and the graph-based regularizer [117], [118], Fast-DAM designs a domain-dependent regularizer . The objective function is given by: min fT nT,L  j1 ( fT (xT,L j ) yT,L j ) 2  λ2Ω( fT ) λ1 mS  k1 βk nT,U  i1 ( fT (xT,U i ) fS k (xT,U i ) ) 2 , where fS k (k  1 , 2,  , m S) denotes the pre-obtained source decision function for the k-th source domain and βk represents the weighting parameter that is determined by the relevance between the target domain and the k- th source domain and can be measured based on the MMD metric. The third term is the domain-dependent regularizer , which transfers the knowledge contained in 15 the source classiﬁer motivated by domain dependence. In [113], the authors also introduce and add a new term to the above objective function based on ε-insensitive loss function [119], which makes the resultant model have high computational efﬁciency .  (Domain-dependent Regularizer  Universum Regular- izer) Univer-DAM is an extension of the Fast-DAM [114]. Its objective function contains an additional regularizer , i.e., Universum regularizer . This regularizer usually uti - lizes an additional dataset termed Universum where the instances do not belong to either the positive or the negative class [120]. The authors treat the source-domain instances as the Universum for the target domain, and the objective function of Univer-DAM is presented as follows: min fT nT,L  j1 ( fT (xT,L j ) yT,L j ) 2  λ2 nS  j1 ( fT (xS j ) ) 2 λ1 mS  k1 βk nT,U  i1 ( fT (xT,U i ) fS k (xT,U i ) ) 2  λ3Ω( fT ). Similar to Fast-DAM, the ε-insensitive loss function can also be utilized [114]. 5.2 Parameter Control Strategy The parameter control strategy focuses on the parameters of models. For example, in the application of object categoriz a- tion, the knowledge from known source categories can be transferred into target categories via object attributes s uch as shape and color [121]. The attribute priors, i.e., probabil istic distribution parameters of the image features correspondi ng to each attribute, can be learned from the source domain and then used to facilitate learning the target classiﬁer . The parameters of a model actually reﬂect the knowledge learned by the model. Therefore, it is possible to transfer t he knowledge at the parametric level. 5.2.1 Parameter Sharing An intuitive way of controlling the parameters is to directl y share the parameters of the source learner to the target learner . Parameter sharing is widely employed especially in the network-based approaches. For example, if we have a neural network for the source task, we can freeze (or say , share) most of its layers and only ﬁnetune the last few layers to produce a target network. The network-based approaches are introduced in Section 5.4. In addition to network-based parameter sharing, matrix- factorization-based parameter sharing is also workable. F or example, Zhuang et al. proposed an approach for text clas- siﬁcation, which is referred to as Matrix T ri-Factorizatio n Based Classiﬁcation Framework (MT rick) [122]. The au- thors observe that, in different domains, different words o r phrases sometimes express the same or similar connotative meaning. Thus, it is more effective to use the concepts be- hind the words rather than the words themselves as a bridge to transfer the knowledge in source domains. Different from PLSA-based transfer learning approaches that utilize the concepts by constructing Bayesian networks, MT rick attempts to ﬁnd the connections between the document classes and the concepts conveyed by the word clusters through matrix tri-factorization. These connections are c on- sidered to be the stable knowledge that is supposed to be transferred. The main idea is to decompose a document-to- word matrix into three matrices, i.e., document-to-cluste r , connection, and cluster-to-word matrices. Speciﬁcally , b y performing the matrix tri-factorization operations on the source and the target document-to-word matrices respec- tively , a joint optimization problem is constructed, which is given by min Q,R,W XS QSRW S2  λ1XT QT RW T 2 λ2QS QS2 s.t. Normalization Constraints , where X denotes the document-to-word matrix, Q denotes the document-to-cluster matrix, R represents the transfor- mation matrix from document clusters to word clusters, W denotes the cluster-to-word matrix, nd denotes the number of the documents, and QS represents the label matrix. The matrix QS is constructed based on the class information of the source-domain documents. If the i-th document belongs to the k-th class, QS [i,k ]  1 . In the above objective function, the matrix R is actually the shared parameter . The ﬁrst term aims to tri-factorize the source document-to-word matrix, and the second term decomposes the target document-to- word matrix. The last term incorporates the source-domain label information. The optimization problem is solved base d on the alternating iteration method. Once the solution of QT is obtained, the class index of the k-th target-domain instance is the one with the maximum value in the k-th row of QT . Further , Zhuang et al. extended MT rick and proposed an approach termed T riplex T ransfer Learning (T riTL) [123] . MT rick assumes that the domains share the similar con- cepts behind their word clusters. In contrast, T riTL as- sumes that the concepts of these domains can be further divided into three types, i.e., domain-independent, trans fer- able domain-speciﬁc, and nontransferable domain-speciﬁc concepts, which is similar to HIDC. This idea is motivated by Dual T ransfer Learning (DTL), where the concepts are assumed to be composed of the domain-independent ones and the transferable domain-speciﬁc ones [124]. The objec- tive function of T riTL is provided as follows: min Q,R,W mS mT  k1 Xk Qk [ RDI RTD RND k ]   W DI W TD k W ND k  2 s.t. Normalization Constraints , where the deﬁnitions of the symbols are similar to those of MT rick and the subscript k denotes the index of the domains with the assumption that the ﬁrst mS domains are the source domains and the last mT domains are the target do- mains. The authors proposed an iterative algorithm to solve the optimization problem. And in the initialization phase, W DI and W TD k are initialized based on the clustering results of the PLSA algorithm, while W UT k is randomly initialized; the PLSA algorithm is performed on the combination of the instances from all the domains. There are some other approaches developed based on matrix factorization. W ang et al. proposed a transfer learn- ing framework for image classiﬁcation [125]. W ang et al. 16 proposed a softly associative approach that integrates two matrix tri-factorizations into a joint framework [126]. Do et al. utilized matrix tri-factorization to discover both the implicit and the explicit similarities for cross-domain re c- ommendation [127]. 5.2.2 Parameter Restriction Another parameter-control-type strategy is to restrict th e parameters. Different from the parameter sharing strategy that enforces the models share some parameters, parame- ter restriction strategy only requires the parameters of th e source and the target models to be similar . T ake the approaches to category learning as examples. The category-learning problem is to learn a new decision function for predicting a new category (denoted by the (k  1) -th category) with only limited target-domain in- stances and k pre-obtained binary decision functions. The function of these pre-obtained decision functions is to pre - dict which of the k categories an instance belongs to. In order to solve the category-learning problem, T ommasi et al. proposed an approach termed Single-Model Knowledge T ransfer (SMKL) [128]. SMKL is based on Least-Squares SVM (LS-SVM). The advantage of LS-SVM is that LS-SVM transforms inequality constraints to equality constraint s and has high computational efﬁciency; its optimization is equi v- alent to solving a linear equation system problem instead of a quadratic programming problem. SMKL selects one of the pre-obtained binary decision functions, and transfers the knowledge contained in its parameters. The objective function is given by min f 1 2      θ β θ       2  λ 2 nT,L  j1 ηj ( f(xT,L j ) yT,L j ) 2 , where f(x)  θ Φ( x)  b, β is the weighting parameter controlling the transfer degree, θ is the parameter of a selected pre-obtained model, and ηj is the coefﬁcient for resolving the label imbalance problem. The kernel param- eter and the tradeoff parameter are chosen based on cross- validation. In order to ﬁnd the optimal weighting parameter , the authors refer to an earlier work [129]. In [129], Cawley proposed a model selection mechanism for LS-SVM, which is based on the leave-one-out cross-validation method. The superiority of this method is that the leave-one-out error for each instance can be obtained in a closed form without performing the real cross-validation experiment. Motivat ed by Cawleys work, the generalization error can be easily estimated to guide the parameter setting in SMKL. T ommasi et al. further extended SMKL by utilizing all the pre-obtained decision functions. In [130], an approach tha t is referred to as Multi-Model Knowledge T ransfer (MMKL) is proposed. Its objective function is presented as follows : min f 1 2          θ  k i1 βiθi           2  λ 2 nT,L  j1 ηj ( f(xT,L j ) yT,L j ) 2 , where θi and βi are the model parameter and the weighting parameter of the i-th pre-obtained decision function, respec- tively . The leave-one-out error can also be obtained in a closed form, and the optimal value of βi (i  1 , 2,  , k ) is the one that maximizes the generalization performance. 5.3 Model Ensemble Strategy In sentiment analysis applications related to product re- views, data or models from multiple product domains are available and can be used as the source domains [131]. Com- bining data or models directly into a single domain may not be successful because the distributions of these domain s are different from each other . Model ensemble is another commonly used strategy . This strategy aims to combine a number of weak classiﬁers to make the ﬁnal predictions. Some previously mentioned transfer learning approaches already adopted this strategy . For example, T rAdaBoost and MsT rAdaBoost ensemble the weak classiﬁers via voting and weighting, respectively . In this subsection, several typi cal ensemble-based transfer learning approaches are introduc ed to help readers better understand the function and the appliance of this strategy . As mentioned in Section 4.1, T askT rAdaBoost, which is an extension of T rAdaBoost for handling multi-source scenarios, is proposed in the paper [33]. T askT rAdaBoost mainly has the following two stages. 1. Candidate Classiﬁer Construction : In the ﬁrst stage, a group of candidate classiﬁers are constructed by per- forming AdaBoost on each source domain. Note that, for each source domain, each iteration of AdaBoost re- sults in a new weak classiﬁer . In order to avoid the over- ﬁtting problem, the authors introduced a threshold to pick the suitable classiﬁers into the candidate group. 2. Classiﬁer Selection and Ensemble : In the second stage, a revised version of AdaBoost is performed on the target- domain instances to construct the ﬁnal classiﬁer . In each iteration, an optimal candidate classiﬁer which has the lowest classiﬁcation error on the labeled target-domain instances is picked out and assigned with a weight based on the classiﬁcation error . Then, the weight of each target-domain instance is updated based on the performance of the selected classiﬁer on the target do- main. After the iteration process, the selected classiﬁers are ensembled to produce the ﬁnal predictions. The difference between the original AdaBoost and the sec- ond stage of T askT rAdaBoost is that, in each iteration, the former constructs a new candidate classiﬁer on the weighted target-domain instances, while the latter selects one pre- obtained candidate classiﬁer which has the minimal clas- siﬁcation error on the weighted target-domain instances. The paper by Gao et al. proposes another ensemble- based framework that is referred to as Locally W eighted En- semble (L WE) [132]. L WE focuses on the ensemble process of various learners; these learners could be constructed on different source domains, or be built by performing differe nt learning algorithms on a single source domain. Different from T askT rAdaBoost that learns the global weight of each learner , the authors adopted the local-weight strategy , i. e., assigning adaptive weights to the learners based on the loca l manifold structure of the target-domain test set. In L WE, a learner is usually assigned with different weights when classifying different target-domain instances. Speciﬁca lly , the authors adopt a graph-based approach to estimate the weights. The steps for weighting are outlined below . 1. Graph Construction : For the i-th source learner , a graph GT Si is constructed by using the learner to classify the 17 target-domain instances in the test set; if two instances are classiﬁed into the same class, they are connected in the graph. Another graph GT is constructed for the target-domain instances as well by performing a clustering algorithm. 2. Learner Weighting : The weight of the i-th learner for the j-th target-domain instance xT j is proportional to the similarity between the instances local structures in GT Si and GT . And the similarity can be measured by the percentage of the common neighbors of xT j in these two graphs. Note that this weighting scheme is based on the clustering- manifold assumption, i.e., if two instances are close to eac h other in a high-density region, they often have similar labels. In order to check the validity of this assumption for the task, the target task is tested on the source-domain training set(s). Speciﬁcally , the clustering quality of th e training set(s) is quantiﬁed and checked by using a metric such as purity or entropy . If the clustering quality is not satisfactory , uniform weights are assigned to the learners instead. Besides, it is intuitive that if the measured struc ture similarity is particularly low for every learner , weightin g and combining these learners seems unwise. Therefore, the authors introduce a threshold and compare it to the average similarity . If the similarity is lower than the threshold, t he label of xT j is determined by the voting scheme among its reliable neighbors, where the reliable neighbors are the on es whose label predictions are made by the combined classiﬁer . The above-mentioned T askT rAdaBoost and L WE ap- proaches mainly focus on the ensemble process. In con- trast, some studies focus more on the construction of weak learners. For example, Ensemble Framework of Anchor Adapters (ENCHOR) [133] is a weighting ensemble frame- work proposed by Zhuang et al . An anchor is a speciﬁc instance. Different from T rAdaBoost which adjusts weights of instances to train and produce a new learner iteratively , ENCHOR constructs a group of weak learners via using dif- ferent representations of the instances produced by anchor s. The thought is that the higher similarity between a certain instance and an anchor , the more likely the feature of that in - stance remains unchanged relative to the anchor , where the similarity can be measured by using the cosine or Gaussian distance function. ENCHOR contains the following steps. 1. Anchor Selection : In this step, a group of anchors are selected. These anchors can be selected based on some rules or even randomly . In order to improve the ﬁ- nal performance of ENCHOR, the authors proposed a method of selecting high-quality anchors [133]. 2. Anchor-based Representation Generation : For each anchor and each instance, the feature vector of an instance is directly multiplied by a coefﬁcient that measures the distance from the instance to the anchor . In this way , each anchor produces a new pair of anchor-adapted source and target instance sets. 3. Learner T raining and Ensemble : The obtained pairs of instance sets can be respectively used to train learners. Then, the resultant learners are weighted and combined to make the ﬁnal predictions. The framework ENCHOR is easy to be realized in a parallel manner in that the operations performed on each anchor are independent. 5.4 Deep Learning T echnique Deep learning methods are particularly popular in the ﬁeld of machine learning. Many researchers utilize the deep learning techniques to construct transfer learning models . For example, the SDA and the mSLDA approaches men- tioned in Section 4.2.6 utilize the deep learning technique s. In this subsection, we speciﬁcally discuss the deep-learni ng- related transfer learning models. The deep learning ap- proaches introduced are divided into two types, i.e., non- adversarial (or say , traditional) ones and adversarial one s. 5.4.1 T raditional Deep Learning As said earlier , autoencoders are often used in deep learnin g area. In addition to SDA and mSLDA, there are some other reconstruction-based transfer learning approaches. For e x- ample, the paper by Zhuang et al. proposes an approach termed T ransfer Learning with Deep Autoencoders (TLDA) [44], [134]. TLDA adopts two autoencoders for the source and the target domains, respectively . These two autoen- coders share the same parameters. The encoder and the decoder both have two layers with activation functions. The diagram of the two autoencoders is presented as follows: XS (W1,b 1) QS (W2,b 2)  Softmax Regression RS ( ˆW2, ˆb2) QS ( ˆW1, ˆb1) XS,  KL Divergence  XT (W1,b 1) QT (W2,b 2)  Softmax Regression RT ( ˆW2, ˆb2) QT ( ˆW1, ˆb1) XT . There are several objectives of TLDA, which are listed as follows. 1. Reconstruction Error Minimization : The output of the de- coder should be extremely close to the input of encoder . In other words, the distance between XS and XS as well as the distance between XT and XT should be minimized. 2. Distribution Adaptation : The distribution difference be- tween QS and QT should be minimized. 3. Regression Error Minimization : The output of the encoder on the labeled source-domain instances, i.e., RS, should be consistent with the corresponding label information Y S. Therefore, the objective function of TLDA is given by min Θ LREC (X, X)  λ1KL(QSQT )  λ2Ω( W, b, ˆW , ˆb) λ3LREG (RS, Y S), where the ﬁrst term represents the reconstruction error , KL() represents the KL divergence, the third term controls the complexity , and the last term represents the regression error . TLDA is trained by using a gradient descent method. The ﬁnal predictions can be made in two different ways. The ﬁrst way is to directly use the output of the encoder to make predictions. And the second way is to treat the autoencoder as a feature extractor , and then train the target classiﬁer on the labeled instances with the feature representation produced by the encoder s ﬁrst-layer output. 18 In addition to the reconstruction-based domain adapta- tion, discrepancy-based domain adaptation is also a popula r direction. In earlier research, the shallow neural network s are tried to learn the domain-independent feature repre- sentation [135]. It is found that the shallow architectures often make it difﬁcult for the resultant models to achieve excellent performance. Therefore, many studies turn to uti - lize deep neural networks. Tzeng et al. [136] added a single adaptation layer and a discrepancy loss to the deep neural network, which improves the performance. Further , Long et al. performed multi-layer adaptation and utilized multi- kernel technique, and they proposed an architecture termed Deep Adaptation Networks (DAN) [137]. For better understanding, let us review DAN in detail. DAN is based on AlexNet [138] and its architecture is presented below [137]. full  6th RS 6 full  7th RS 7 full  8th RS 8 ( f(XS) ) XS XT conv  1st QS 1 QT 1 conv   QS 5 QT 5   ր ց  MK-MMD   MK-MMD   MK-MMD  Five Convolutional Layers full  6th RT 6 full  7th RT 7 full  8th RT 8 ( f(XT ) )    Three Fully Connected Layers In the above network, the features are ﬁrst extracted by ﬁve convolutional layers in a general-to-speciﬁc manner . Next , the extracted features are fed into one of the two fully connected networks switched by their original domains. These two networks both consist of three fully connected layers that are specialized for the source and the target domains. DAN has the following objectives. 1. Classiﬁcation Error Minimization : The classiﬁcation error of the labeled instances should be minimized. The cross-entropy loss function is adopted to measure the prediction error of the labeled instances. 2. Distribution Adaptation : Multiple layers, which include the representation layers and the output layer , can be jointly adapted in a layer-wise manner . Instead of using the single-kernel MMD to measure the distribution difference, the authors turn to MK-MMD. The authors adopt the linear-time unbiased estimation of MK-MMD to avoid numerous inner product operations [62]. 3. Kernel Parameter Optimization : The weighting parame- ters of the multiple kernels in MK-MMD should be optimized to maximize the test power [62]. The objective function of the DAN network is given by: min Θ max κ nL  i1 L ( f(xL i ), y L i )  λ 8 l6 MK-MMD(RS l , R T l ; κ), where l denotes the index of the layer . The above opti- mization is actually a minimax optimization problem. The maximization of the objective function with respect to the kernel function κ aims to maximize the test power . After this step, the subtle difference between the source and the targe t domains are magniﬁed. This train of thought is similar to the Generative Adversarial Network (GAN) [139]. In the training process, the DAN network is initialized by a pre- trained AlexNet [138]. There are two categories of param- eters that should be learned, i.e., the network parameters and the weighting parameters of the multiple kernels. Given that the ﬁrst three convolutional layers output the general features and are transferable, the authors freeze them and ﬁne-turn the last two convolutional layers and the two fully connected layers [140]. The last fully connected layer (or s ay , the classiﬁer layer) is trained from scratch. Long et al. further extended the above DAN approach and proposed the DAN framework [141]. The new charac- teristics are summarized as follows. 1. Regularizer Adding : The framework introduces an ad- ditional regularizer to minimize the uncertainty of the predicted labels of the unlabeled target-domain in- stances, which is motivated by entropy minimization criterion [142]. 2. Architecture Generalizing : The DAN framework can be applied to many other architectures such as GoogLeNet [143] and ResNet [144]. 3. Measurement Generalizing : The distribution difference can be estimated by other metrics. For example, in addition to MK-MMD, the authors also present the Mean Embedding test for distribution adaptation [145]. The objective function of the DAN framework is given by: min Θ max κ nL  i1 L ( f(xL i ), y L i )  λ1 lend llstrt DIST(RS l , R T l )  λ2 nT,U  i1  yj Y S ( P (yj f(xT,U i )) ) , where lstrt and lend denote the boundary indexes of the fully connected layers for adapting the distributions. There are some other impressive works. For example, Long et al. constructed residual transfer networks for do- main adaptation, which is motivated by deep residual learn- ing [146]. Besides, another work by Long et al. proposes the Joint Adaptation Network (JAN) [147], which adapts the joint distribution difference of multiple layers. Sun a nd Saenko extended CORAL for deep domain adaptation and proposed an approach termed Deep CORAL (DCORAL), in which the CORAL loss is added to minimize the feature covariance [148]. Chen et al. realized that the instances with the same label should be close to each other in the feature space, and they not only add the CORAL loss but also add an instance-based class-level discrepancy loss [149]. Pan et al. constructed three prototypical networks (corresponding to DS, DT and DS DT ) and incorporated the thought of multi-model consensus. They also adopt pseudo-label strategy and adapt both the instance-level and class-level discrepancy [150]. Kang et al. proposed the Contrastive Adaptation Network (CAN), which is based on the dis- crepancy metric termed contrastive domain discrepancy [151]. Zhu et al. aimed to adapt the extracted multiple fea- ture representations and proposed the Multi-Representati on Adaptation Network (MRAN) [152]. Deep learning technique can also be used for multi- source transfer learning. For example, the work by Zhu et al. proposes a framework that is referred to as Multiple Feature Spaces Adaptation Network (MFSAN) [153]. The architec- ture of MFSAN consists of a common-feature extractor , mS domain-speciﬁc feature extractors, and mS domain-speciﬁc 19 classiﬁers. The corresponding schematic diagram is shown below . XS 1  XS k  XS mS XT Common  Extractor QS 1 QS k QS mS QT Domain-Speciﬁc   Extractors RS 1 RS k  RS mS RT 1 RT k  RT mS Domain-Speciﬁc  Classiﬁers ˆY S 1  ˆY S k  ˆY S mS ˆY T 1  ˆY T k  ˆY T mS In each iteration, MFSAN has the following steps. 1. Common Feature Extraction : For each source domain (denoted by DSk with k  1 ,  , m S), the source- domain instances (denoted by XS k ) are separately input to the common-feature extractor to produce instances in a common latent feature space (denoted by QS k ). Similar operations are also performed on the target-domain instances (denoted by XT ), which produces QT . 2. Speciﬁc Feature Extraction : For each source domain, the extracted common features QS k is fed to the k- th domain-speciﬁc feature extractor . Meanwhile, QT is fed to all the domain-speciﬁc feature extractors, which results in RT k with k  1 ,  , m S. 3. Data Classiﬁcation : The output of the k-th domain- speciﬁc feature extractor is input to the k-th classiﬁer . In this way , mS pairs of the classiﬁcation results are predicted in the form of probability . 4. Parameter Updating : The parameters of the network are updated to optimize the objective function. There are three objectives in MFSAN, i.e., classiﬁcation error minimization, distribution adaptation, and consens us regularization. The objective function is given by: min Θ mS  i1 L( ˆY S i , Y S i )  λ1 mS  i1 MMD(RS i , R T i )  λ2 mS  ij   ˆY T i ˆY T j   , where the ﬁrst term represents the classiﬁcation error of th e labeled source-domain instances, the second term measures the distribution difference, and the third term measures the discrepancy of the predictions on the target-domain instances. 5.4.2 Adversarial Deep Learning The thought of adversarial learning can be integrated into deep-learning-based transfer learning approaches. As men - tioned above, in the DAN framework, the network Θ and the kernel κ play a minimax game, which reﬂects the thought of adversarial learning. However , the DAN frame- work is a little different from the traditional GAN-based methods in terms of the adversarial matching. In the DAN framework, there is only a few parameters to be optimized in the max game, which makes the optimization easier to achieve equilibrium. Before introducing the adversaria l transfer learning approaches, let us brieﬂy review the orig i- nal GAN framework and the related work. The original GAN [139], which is inspired by the two- player game, is composed of two models, a generator G and a discriminator D . The generator produces the counterfeits of the true data for the purpose of confusing the discrimina- tor and making the discriminator produce wrong detection. The discriminator is fed with the mixture of the true data and the counterfeits, and it aims to detect whether a data is the true one or the fake one. These two models actually play a two-player minimax game, and the objective function is as follows: min G max D ExPtrue [log D (x)]  EzPz [log (1 D (G (z)))], where z represents the noise instances (sampled from a certain noise distribution) used as the input of the generat or for producing the counterfeits. The entire GAN can be trained by using the back-propagation algorithm. When the two-player game achieves equilibrium, the generator can produce almost true-looking instances. Motivated by GAN, many transfer learning approaches are established based on the assumption that a good feature representation contains almost no discriminative informa - tion about the instances original domains. For example, th e work by Ganin et al. proposes a deep architecture termed Domain-Adversarial Neural Network (DANN) for domain adaptation [154], [155]. DANN assumes that there is no labeled target-domain instance to work with. Its architec- ture consists of a feature extractor , a label predictor , and a domain classiﬁer . The corresponding diagram is as follows. Label  Predictor ˆY S,L ˆY T,U XS,L XT,U Feature  Extractor  QS,L QT,U } Domain  Classiﬁer ˆS ˆT (Domain Label ) The feature extractor acts like the generator , which aims to produce the domain-independent feature representation fo r confusing the domain classiﬁer . The domain classiﬁer plays the role like the discriminator , which attempts to detect whether the extracted features come from the source domain or the target domain. Besides, the label predictor produces the label prediction of the instances, which is trained on th e extracted features of the labeled source-domain instances , i.e., QS,L . DANN can be trained by inserting a special gra- dient reversal layer (GRL). After the training of the whole system, the feature extractor learns the deep feature of the instances, and the output ˆY T,U is the predicted labels of the unlabeled target-domain instances. There are some other related impressive works. The work by Tzeng et al. proposes a uniﬁed adversarial domain adaptation framework [156]. The work by Shen et al. adopts W asserstein distance for domain adaptation [59]. Hoffman et al. adopted cycle-consistency loss to ensure the structural and semantic consistency [157]. Long et al. proposed the Conditional Domain Adversarial Network (CDAN), which utilizes a conditional domain discriminator to assist adve r- sarial adaptation [158]. Zhang et al. adopted a symmetric design for the source and the target classiﬁers [159]. Zhao et al. utilized domain adversarial networks to solve the multi- source transfer learning problem [160]. Y u et al. proposed a dynamic adversarial adaptation network [161]. Some approaches are designed for some special scenar- ios. T ake the partial transfer learning as an example. The partial transfer learning approaches are designed for the s ce- nario that the target-domain classes are less than the sourc e- domain classes, i.e., YS  YT . In this case, the source- domain instances with different labels may have different 20 importance for domain adaptation. T o be more speciﬁc, the source-domain and the target-domain instances with the same label are more likely to be potentially associated. How - ever , since the target-domain instances are unlabeled, how to identify and partially transfer the important informati on from the labeled source-domain instances is a critical issu e. The paper by Zhang et al. proposes an approach for partial domain adaptation, which is called Impor- tance W eighted Adversarial Nets-Based Domain Adaptation (IW ANDA) [162]. The architecture of IW ANDA is different from that of DANN. DANN adopts one common feature extractor based on the assumption that there exists a com- mon feature space where QS,L and QT,U have the similar distribution. However , IW ANDA uses two domain-speciﬁc feature extractors for the source and the target domains, respectively . Speciﬁcally , IW ANDA consists of two feature extractors, two domain classiﬁers, and one label predictor . The diagram of IW ANDA is presented below . Label  Predictor ˆY S,L ˆY T,U XS,L Source Feature  Extractor  QS,L XT,U T arget Feature  Extractor QT,U  } β S   ˆY T,U 2nd Domain  Classiﬁer ˆS2 ˆT2 1st Domain  Classiﬁer ˆS1 ˆT1 W eight    Function β S Before training, the source feature extractor and the label predictor are pre-trained on the labeled source-domain in- stances. These two components are frozen in the training process, which means that only the target feature extractor and the domain classiﬁers should be optimized. In each iteration, the above network is optimized by taking the following steps. 1. Instance Weighting : In order to solve the partial transfer issue, the source-domain instances are assigned with weights based on the output of the ﬁrst domain clas- siﬁer . The ﬁrst domain classiﬁer is fed with QS,L and QT,U , and then outputs the probabilistic predictions of their domains. If a source domain instance is predicted with a high probability of belonging to the target do- main, this instance is highly likely to associate with the target domain. Thus, this instance is assigned with a larger weight and vice versa. 2. Prediction Making : The label predictor outputs the label predictions of the instances. The second classiﬁer pre- dicts which domain an instance belongs to. 3. Parameter Updating : The ﬁrst classiﬁer is optimized to minimize the domain classiﬁcation error . The second classiﬁer plays a minmax game with the target fea- ture extractor . This classiﬁer aims to detect whether a instance is the instance from the target domain or the weighted instance from the source domain, and to reduce the uncertainty of the label prediction ˆY T,U . The target feature extractor aims to confuse the second classiﬁer . These components can be optimized in a similar way to GAN or by inserting a GRL. In addition to IW ANDA, the work by Cao et al. con- structs the selective adversarial network for partial tran sfer learning [163]. There are some other studies related to transfer learning. For example, the work by W ang et al. proposes a minimax-based approach to select high-quality source-domain data [164]. Chen et al. investigated the trans- ferability and the discriminability in the adversarial dom ain adaptation, and proposed a spectral penalization approach to boost the existing adversarial transfer learning method s [165]. 6 A PPLICATIO N In previous sections, a number of representative trans- fer learning approaches are introduced, which have been applied to solving a variety of text-relatedimage-relate d problems in their original papers. For example, MT rick [122 ] and T riTL [123] utilize the matrix factorization technique to solve cross-domain text classiﬁcation problems; the deep- learning-based approaches such as DAN [137], DCORAL [148], and DANN [154], [155] are applied to solving image classiﬁcation problems. Instead of focusing on the general text-related or image-related applications, in this secti on, we mainly focus on the transfer learning applications in speci ﬁc areas such as medicine, bioinformatics, transportation, a nd recommender systems. 6.1 Medical Application Medical imaging plays an important role in the medical area, which is a powerful tool for diagnosis. With the de- velopment of computer technology such as machine learn- ing, computer-aided diagnosis has become a popular and promising direction. Note that medical images are gener- ated by special medical equipment, and their labeling often relies on experienced doctors. Therefore, in many cases, it is expensive and hard to collect sufﬁcient training data. T ran s- fer learning technology can be utilized for medical imaging analysis. A commonly used transfer learning approach is to pre-train a neural network on the source domain (e.g., ImageNet, which is an image database containing more than fourteen million annotated images with more than twenty thousand categories [166]) and then ﬁnetune it based on the instances from the target domain. For example, Maqsood et al. ﬁnetuned the AlexNet [138] for the detection of Alzheimer s disease [167]. Their ap- proach has the following four steps. First, the MRI images from the target domain are pre-processed by performing contrast stretching operations. Second, the AlexNet archi tec- ture [138] is pre-trained over ImageNet [166] (i.e., the sou rce domain) as a starting point to learn the new task. Third, the convolutional layers of AlexNet are ﬁxed, and the last three fully connected layers are replaced by the new ones including one softmax layer , one fully connected layer , and one output layer . Finally , the modiﬁed AlexNet is ﬁnetuned by training on the Alzheimer s dataset [168] (i.e., the targ et domain). The experimental results show that the proposed approach achieves the highest accuracy for the multi-class classiﬁcation problem (i.e, Alzheimer s stage detection) . Similarly , Shin et al. ﬁnetuned the pre-trained deep neural network for solving the computer-aided detection problems [169]. Byra et al. utilized the transfer learning tech- nology to help assess knee osteoarthritis [170]. In additio n to imaging analysis, transfer learning has some other applica - tions in the medical area. For example, the work by T ang et 21 al. combines the active learning and the domain adaptation technologies for the classiﬁcation of various medical data [171]. Zeng et al. utilized transfer learning for automatically encoding ICD-9 codes that are used to describe a patients diagnosis [172]. 6.2 Bioinformatics Application Biological sequence analysis is an important task in the bioinformatics area. Since the understanding of some or- ganisms can be transferred to other organisms, transfer learning can be applied to facilitate the biological sequen ce analysis. The distribution difference problem exists sign iﬁ- cantly in this application. For example, the function of som e biological substances may remain unchanged but with the composition changed between two organisms, which may result in the marginal distribution difference. Besides, i f two organisms have a common ancestor but with long evo- lutionary distance, the conditional distribution differe nce would be signiﬁcant. The work by Schweikert et al. uses the mRNA splice site prediction problem as the example to analyze the effectiveness of transfer learning approach es [173]. In their experiments, the source domain contains the sequence instances from a well-studied model organism, i.e ., C. elegans , and the target organisms include two additional nematodes (i.e., C. remanei and P . paciﬁcus ), D. melanogaster , and the plant A. thaliana . A number of transfer learning approaches, e.g., F AM [64] and the variant of KMM [5], are compared with each other . The experimental results show that transfer learning can help improve the classiﬁcation performance. Another widely encountered task in the bioinformatics area is gene expression analysis, e.g., predicting associa tions between genes and phenotypes. In this application, one of the main challenges is the data sparsity problem, since there is usually very little data of the known associations. T ransfer learning can be used to leverage this problem by providing additional information and knowledge. For example, Petegrosso et al. [174] proposed a transfer learn- ing approach to analyze and predict the gene-phenotype associations based on the Label Propagation Algorithm (LP A) [175]. LP A utilizes the Protein-Protein Interaction (PPI) network and the initial labeling to predict the target associations based on the assumption that the genes that are connected in the PPI network should have the similar labels. The authors extended LP A by incorporating multi-task and transfer-learning technologies. First, Human Phenotype O n- tology (HPO), which provides a standardized vocabulary of phenotypic features of human diseases, is utilized to form the auxiliary task. In this way , the associations can be predicted by utilizing phenotype paths and both the linkage knowledge in HPO and in the PPI network; the interacted genes in PPI are more likely to be associated with the same phenotype and the connected phenotypes in HPO are more likely to be associated with the same gene. Second, Gene Ontology (GO), which contains the association information between gene functions and genes, is used as the source domain. Additional regularizers are designed, and the PPI network and the common genes are used as the bridge for knowledge transfer . The gene-GO term and gene-HPO phenotype associations are constructed simultaneously fo r all the genes in the PPI network. By transferring additional knowledge, the predicted gene-phenotype associations can be more reliable. T ransfer learning can also be applied to solving the PPI prediction problems. Xu et al. [176] proposed an approach to transfer the linkage knowledge from the source PPI network to the target one. The proposed approach is based on the collective matrix factorization technique [177], wh ere a factor matrix is shared across domains. 6.3 T ransportation Application One application of transfer learning in the transportation area is to understand the trafﬁc scene images. In this applic a- tion, a challenge problem is that the images taken from a cer- tain location often suffer from variations because of diffe rent weather and light conditions. In order to solve this problem , Di et al. proposed an approach that attempts to transfer the information of the images that were taken from the same location in different conditions [178]. In the ﬁrst step, a p re- trained network is ﬁnetuned to extract the feature represen - tations of images. In the second step, the feature transfor- mation strategy is adopted to construct a new feature rep- resentation. Speciﬁcally , the dimension reduction algori thm (i.e., partial least squares regression [179]) is performe d on the extracted features to generate low-dimension features . Then, a transformation matrix is learned to minimize the domain discrepancy of the dimension-reduced data. Next, the subspace alignment operations are adopted to further reduce the domain discrepancy . Note that, although images under different conditions often have different appearanc es, they often have the similar layout structure. Therefore, in the ﬁnal step, the cross-domain dense correspondences are established between the test image and the retrieved best matching image at ﬁrst, and then the annotations of the best matching image are transferred to the test image via the Markov random ﬁeld model [180], [181]. T ransfer learning can also be applied to the task of driver behavior modeling. In this task, sufﬁcient personalized da ta of each individual driver are usually unavailable. In such situations, transferring the knowledge contained in the hi s- torical data for the newly-involved driver is a promising alternative. For example, Lu et al. proposed an approach to driver model adaptation in lane-changing scenarios [182]. The source domain contains the sufﬁcient data describing the behavior of the source drivers, while the target domain has a few numbers of data about the target driver . In the ﬁrst step, the data from both domains are pre-processed by performing PCA to generate low-dimension features. The authors assume that the source and the target data are from two manifolds. Therefore, in the second step, a manifold alignment approach is adopted for domain adap- tation. Speciﬁcally , the dynamic time warping algorithm [183] is applied to measuring similarity and ﬁnding the corresponding source-domain data point of each target- domain data point. Then, local Procrustes analysis [184] is adopted to align the two manifolds based on the obtained correspondences between data points. In this way , the data from the source domain can be transferred to the target domain. And in the ﬁnal step, a stochastic modeling method (e.g., Gaussian mixture regression [185]) is used to model 22 the behavior of the target driver . The experimental results demonstrate that the transfer learning approach can help the target driver even when few target-domain data are available. Besides, the results also show that when the number of target instances are very small or very large, the superiority of their approach is not obvious. This may because the relationship across domains cannot be found exactly with few target-domain instances, and in the case of sufﬁcient target-domain instances, the necessity of trans fer learning is reduced. Besides, there are some other applications of transfer learning in the transportation area. For example, Liu et al. applied transfer learning to driver poses recognition [186 ]. W ang et al. adopted the regularization technique in transfer learning for vehicle type recognition [187]. T ransfer lear ning can also be utilized for anomalous activity detection [188] , [189], trafﬁc sign recognition [190], etc. 6.4 Recommender-System Application Due to the rapid increase of the amount of information, how to effectively recommend the personalized content for individual users is an important issue. In the ﬁeld of recommender systems, some traditional recommendation methods, e.g., factorization-based collaborative ﬁlteri ng, of- ten rely on the factorization of the user-item interaction matrix to obtain the predictive function. These methods often require a large amount of training data to make accurate recommendations. However , the necessary trainin g data, e.g., the historical interaction data, are often spar se in real-world scenarios. Besides, for new registered users or new items, traditional methods are often hard to make effective recommendations, which is also known as the cold- start problem. Recognizing the above-mentioned problems in recom- mender systems, kinds of transfer learning approaches, e.g ., instance-based and feature-based approaches, have been proposed. These approaches attempt to make use of the data from other recommender systems (i.e., the source domains) to help construct the recommender system in the target domain. Instance-based approaches mainly focus on transferring different types of instances, e.g., rating s, feedbacks, and examinations, from the source domain to the target domain. The work by Pan et al. [191] leverages the uncertain ratings (represented as rating distribution s) of the source domain for knowledge transfer . Speciﬁcally , the source-domain uncertain ratings are used as constraints to help complete the rating matrix factorization task on the target domain. Hu et al. [192] proposed an approach termed transfer meeting hybrid, which extracts the knowledge from unstructured text by using an attentive memory network and selectively transfer the useful information. Feature-based approaches often leverage and transfer the information from a latent feature space. For example, Pan et al. proposed an approach termed Coordinate System T ransfer (CST) [193] to leverage both the user-side and the item-side latent features. The source-domain instances co me from another recommender system, sharing common users and items with the target domain. CST is developed based on the assumption that the principle coordinates, which reﬂect the tastes of users or the factors of items, character - ize the domain-independent structure and are transferable across domains. CST ﬁrst constructs two principle coordi- nate systems, which are actually the latent features of user s and items, by applying sparse matrix tri-factorization on the source-domain data, and then transfer the coordinate systems to the target domain by setting them as constraints. The experimental results show that CST signiﬁcantly out- performs the non-transfer baselines (i.e., average ﬁlling model and latent factorization model) in all data sparsity levels [193]. There are some other studies about cross-domain rec- ommendation [194], [195], [196], [197]. For example, He et al. proposed a transfer learning framework based on the Bayesian neural network [198]. Zhu et al. [199] proposed a deep framework, which ﬁrst generates the user and item feature representations based on the matrix factorization technique, and then employs a deep neural network to learn the mapping of features across domains. Y uan et al. [200] proposed a deep domain adaptation approach based on autoencoders and a modiﬁed DANN [154], [155] to extract and transfer the instances from rating matrices. 6.5 Other Applications Communication Application : In addition to WiFi localiza- tion tasks [2], [36], transfer learning has also been employ ed in wireless-network applications. For example, Bastug et al. proposed a caching mechanism [201]; the knowledge contained in contextual information, which is extracted fr om the interactions between devices, is transferred to the tar get domain. Besides, some studies focus on the energy saving problems. The work by Li et al. proposes an energy saving scheme for cellular radio access networks, which utilizes the transfer-learning expertise [202]. The work by Zhao and Grace applies transfer learning to topology management for reducing energy consumption [203]. Urban-Computing Application : With a large amount of data related to our cities, urban-computing is a promis- ing researching track in directions of trafﬁc monitoring, health care, social security , etc. T ransfer learning has be en applied to alleviate the data scarcity problem in many urban computing applications. For example, Guo et al. [204] proposed an approach for chain store site recommendation, which leverages the knowledge from semantically-relevant domains (e.g., other cities with the same store and other chain stores in the target city) to the target city . W ei et al. [205] proposed a ﬂexible multi-modal transfer learning approach that transfers knowledge from a city that have sufﬁcient multi-model data and labels to the target city to alleviate the data sparsity problem. T ransfer learning has been applied to some recognition tasks such as hand gesture recognition [206], face recogni- tion [207], activity recognition [208], and speech emotion recognition [209]. Besides, transfer-learning expertise has also been incorporated into some other areas such as sen- timent analysis [28], [96], [210], fraud detection [211], s ocial network [212], and hyperspectral image analysis [54], [213 ]. 7 E XPERIMENT T ransfer learning techniques have been successfully appli ed in many real-world applications. In this section, we perfor m 23 experiments to evaluate the performance of some represen- tative transfer learning models 1 [214] of different categories on two mainstream research areas, i.e., object recognition and text classiﬁcation. The datasets are introduced at ﬁrst . Then, the experimental results and further analyses are provided. 7.1 Dataset and Preprocessing Three datasets are studied in the experiments, i.e., Ofﬁce- 31, Reuters-21578, and Amazon Reviews. For simplicity , we focus on the classiﬁcation tasks. The statistical informat ion of the preprocessed datasets is listed in T able 3.  Amazon Reviews 2 [107] is a multi-domain sentiment dataset which contains product reviews taken from Ama- zon.com of four domains (Books, Kitchen, Electronics and DVDs). Each review in the four domains has a text and a rating from zero to ﬁve. In the experiments, the ratios that are less than three are deﬁned as the negative ones, while others are deﬁned as the positive ones. The frequency of each word in all reviews is calculated. Then, the ﬁve thousand words with the highest frequency are selected as the attributes of each review . In this way , we ﬁnally have 1000 positive instances, 1000 negative instances, and about 5000 unlabeled instances in each domain. In the experiments, every two of the four domains are selected to generate twelve tasks.  Reuters-215783 is a dataset for text categorization, which has a hierarchical structure. The dataset contains 5 top categories (Exchanges, Orgs, People, Places, T opics). In out experiment, we use the top three big category Orgs, People and Places to generate three classiﬁcation tasks (Orgs vs People, Orgs vs Places and People vs Places). In each task, the subcategories in the corresponding two categories are separately divided into two parts. Then, the resultant four parts are used as the components to form two domains. Each domain has about 1000 instances, and each instance has about 4500 features. Speciﬁcally , taking the task Orgs vs People as an example, one part from Orgs and one part from People and combined to form the source domain; similarly , the rest two parts form the target domain. Note that the instances in the three categories are all labeled. In order to generate the unlabeled instances, the labeled instances are selected from the dataset, and their labels are ignored.  Ofﬁce-31 [215] is an object recognition dataset which contains thirty-one categories and three domains, i.e., Amazon, W ebcam, and DSLR. These three domains have 2817, 498, and 795 instances, respectively . The images in Amazon are the online e-commerce pictures taken from Amazon.com. The images in W ebcam are the low- resolution pictures taken by web cameras. And the im- ages in DSLR are the high-resolution pictures taken by DSLR cameras. In the experiments, every two of the three domains (with the order considered) are selected as the source and the target domains, which results in six tasks. 1. https:github.comFuzhenZhuangT ransfer-Learning-T oolkit 2. http:www .cs.jhu.edu mdredzedatasetssentiment 3. https:archive.ics.uci.edumldatasetsReuters- 21578T extCategorizationCollection Model .ĺ .ĺ .ĺ( ĺ. ĺ ĺ( ĺ. ĺ ĺ( (ĺ. (ĺ (ĺ HIDC 0.88 0.875 0.88 0.7925 0.81 0.8025 0.7925 0.8175 0.8075 0.8075 0.87 0.87 濃濁濋濆濆濋 TriTL 0.715 0.725 0.6775 0.5725 0.525 0.5775 0.615 0.6125 0. 6 0.625 0.61 0.615 濃濁濉濅濅濈 CD-PLSA 0.7475 0.7225 0.72 0.6075 0.6175 0.6075 0.575 0.61 0 .6425 0.7225 0.745 0.7 濃濁濉濉濋濄 MTrick 0.82 0.835 0.8125 0.7725 0.7475 0.7275 0.755 0.745 0.7 8 0.79 0.7975 0.81 濃濁濊濋濅濊 SFA 0.8525 0.8575 0.8675 0.7825 0.805 0.775 0.7925 0.785 0.7 775 0.84 0.8525 0.84 濃濁濋濄濌濃 mSLDA 0.7975 0.7825 0.7925 0.635 0.645 0.6325 0.6525 0.6675 0.6625 0.7225 0.715 0.7125 濃濁濊濃濄濈 SDA 0.8425 0.7925 0.8025 0.745 0.76 0.765 0.7625 0.7475 0.74 25 0.8175 0.805 0.81 濃濁濊濋濅濊 GFK 0.62 0.6275 0.6325 0.62 0.61 0.6225 0.58 0.565 0.5725 0.6 575 0.65 0.6325 濃濁濉濄濈濋 SCL 0.8575 0.8625 0.8725 0.78 0.785 0.7825 0.7925 0.7925 0.7 825 0.8425 0.8525 0.845 濃濁濋濅濃濉 TCA 0.755 0.755 0.755 0.6475 0.6475 0.65 0.58 0.5825 0.585 0. 7175 0.715 0.7125 濃濁濉濊濈濅 Baseline 0.727 0.709 0.827 0.74 0.728 0.73 0.745 0.772 0.708 0 .84 0.706 0.707 濃濁濊濇濇濌 .ĺ .ĺ .ĺ( ĺ. ĺ ĺ( ĺ. ĺ ĺ( (ĺ. (ĺ (ĺ HIDC TriTL CD-PLSA MTrick SFA mSLDA SDA GFK SCL TCA Baseline Fig. 5. Comparison results on Amazon Reviews. 7.2 Experiment Setting Experiments are conducted to compare some representative transfer learning models. Speciﬁcally , eight algorithms a re performed on the dataset Ofﬁce-31 for solving the object recognition problem. Besides, fourteen algorithms are per - formed and evaluated on the dataset Reuters-21578 for solving the text classiﬁcation problem. In the sentiment classiﬁcation problem, eleven algorithms are performed on Amazon Reviews. The classiﬁcation results are evaluated by accuracy , which is deﬁned as follows: accuracy  {xxi Dtest f(xi)  yi} Dtest  where Dtest denotes the test data and y denotes the truth classiﬁcation label; f(x) represents the predicted classiﬁca- tion result. Note that some algorithms need the base classi- ﬁer . In these cases, an SVM with a linear kernel is adopted as the base classiﬁer in the experiments. Besides, the sourc e- domain instances are all labeled. And for the performed al- gorithms (except T rAdaBoost), the target-domain instance s are unlabeled. Each algorithm was executed three times, and the average results are adopted as our experimental results . The evaluated transfer learning models include: HIDC [93], T riTL [123], CD-PLSA [91], [92], MT rick [122], SF A [106], mSLDA [98], [99], SDA [96], GFK [102], SCL [94], TCA [36], [78], CoCC [41], JDA [38], T rAdaBoost [31], DAN [137], DCORAL [148], MRAN [152], CDAN [158], DANN [154], [155], JAN [147], and CAN [151]. 7.3 Experiment Result In this subsection, we compare over twenty algorithms on three datasets in total. The parameters of all algorithms are set to the default values or the recommended values mentioned in the original papers. The experimental results are presented in T ables 4, 5, and 6 corresponding to Amazon Reviews, Reuters-21578, and Ofﬁce-31, respectively . In or der to allow readers to understand the experimental results more intuitively , three radar maps, i.e., Figs. 5, 6, and 7, a re provided, which visualize the experimental results. In the radar maps, each direction represents a task. The general performance of an algorithm is demonstrated by a polygon 24 T ABLE 3 Statistical information of the preprocessed datasets. Area Dataset Domain Attribute T otal Instances T asks Sentiment Classiﬁcation Amazon Reviews 4 5000 27677 12 T ext Classiﬁcation Reuters-21578 3 4772 6570 3 Object Recognition Ofﬁce-31 3 800 4110 6 Orgs vs Places People vs Places Orgs vs People HIDC 0.7698 0.6945 0.8375 GFK 0.622 0.5417 0.6446 CD-PLSA 0.5624 0.5749 0.7826 MTrick 0.7494 0.6457 0.793 CoCC 0.6704 0.8264 0.7644 SFA 0.7468 0.6768 0.7906 mSLDA 0.5645 0.6064 0.5289 Orgs vs Places People vs Places Orgs vs People HIDC GFK CD-PLSA MTrick CoCC SFA mSLDA Orgs vs Places People vs Places Orgs vs People SDA 0.6603 0.5556 0.5992 濃濁濉濃濈濃 TriTL 0.7338 0.5517 0.7505 濃濁濉濊濋濊 SCL 0.6794 0.5046 0.6694 濃濁濉濄濊濋 TCA 0.7368 0.6065 0.7562 濃濁濉濌濌濋 JDA 0.5694 0.6296 0.7424 濃濁濉濇濊濄 TrAdaBoost 0.7336 0.7052 0.7879 濃濁濊濇濅濅 Baseline 0.6683 0.5198 0.6696 HIDC 0.7698 0.6945 0.8375 濃濁濊濉濊濆 GFK 0.622 0.5417 0.6446 濃濁濉濃濅濋 CD-PLSA 0.5624 0.5749 0.7826 濃濁濉濇濃濃 MTrick 0.7494 0.6457 0.793 濃濁濊濅濌濇 CoCC 0.6704 0.8264 0.7644 濃濁濊濈濆濊 SFA 0.7468 0.6768 0.7906 濃濁濊濆濋濄 mSLDA 0.5645 0.6064 0.5289 濃濁濈濉濉濉 Orgs vs Places People vs Places Orgs vs People SDA TriTL SCL TCA JDA TrAdaBoost Baseline Fig. 6. Comparison results on Reuters-21578. T ABLE 4 Accuracy performance on the Amazon Reviews of four domains: Kitchen (K), Electronics (E), D VDs (D) and Books (B). Model K  D K  B K  E D  K D  B D  E B  K B  D B  E E  K E  D E  B A verage HIDC 0.8800 0.8750 0.8800 0.7925 0.8100 0.8025 0.7925 0.817 5 0.8075 0.8075 0.8700 0.8700 0.8338 T riTL 0.7150 0.7250 0.6775 0.5725 0.5250 0.5775 0.6150 0.61 25 0.6000 0.6250 0.6100 0.6150 0.6225 CD-PLSA 0.7475 0.7225 0.7200 0.6075 0.6175 0.6075 0.5750 0.6 100 0.6425 0.7225 0.7450 0.7000 0.6681 MT rick 0.8200 0.8350 0.8125 0.7725 0.7475 0.7275 0.7550 0.7 450 0.7800 0.7900 0.7975 0.8100 0.7827 SF A 0.8525 0.8575 0.8675 0.7825 0.8050 0.7750 0.7925 0.7850 0 .7775 0.8400 0.8525 0.8400 0.8190 mSLDA 0.7975 0.7825 0.7925 0.6350 0.6450 0.6325 0.6525 0.667 5 0.6625 0.7225 0.7150 0.7125 0.7015 SDA 0.8425 0.7925 0.8025 0.7450 0.7600 0.7650 0.7625 0.7475 0 .7425 0.8175 0.8050 0.8100 0.7827 GFK 0.6200 0.6275 0.6325 0.6200 0.6100 0.6225 0.5800 0.5650 0.5725 0.6575 0.6500 0.6325 0.6158 SCL 0.8575 0.8625 0.8725 0.7800 0.7850 0.7825 0.7925 0.7925 0 .7825 0.8425 0.8525 0.8450 0.8206 TCA 0.7550 0.7550 0.7550 0.6475 0.6475 0.6500 0.5800 0.5825 0.5850 0.7175 0.7150 0.7125 0.6752 Baseline 0.7270 0.7090 0.8270 0.7400 0.7280 0.7300 0.7450 0 .7720 0.7080 0.8400 0.7060 0.7070 0.7449 whose vertices representing the accuracy of the algorithm for dealing with different tasks. T able 4 shows the experimental results on Amazon Re- views. The baseline is a linear classiﬁer trained only on the source domain (here we directly use the results from the paper [107]). Fig. 5 visualizes the results. As shown in Fig. 5, most algorithms are relatively well-performed when the source domain is electronics or kitchen, which indicate s that these two domains may contains more transferable information than the other two domains. In addition, it can be observed that HIDC, SCL, SF A, MT rick and SDA perform well and relatively stable in all the twelve tasks. Meanwhil e, other algorithms, especially mSLDA, CD-PLSA, and T riTL, are relatively unstable; the performance of them ﬂuctuates in a range about twenty percent. T riTL has a relatively high accuracy on the tasks where the source domain is kitchen, but has a relatively low accuracy on other tasks. The algorithms TCA, mSLDA, and CD-PLSA have similar performance on all the tasks with an accuracy about seventy percent on average. Among the well-performed algorithms, HIDC and MT rick are based on feature reduction (feature clustering), while the others are based on feature encoding (SDA), feature alignment (SF A), and feature selection (SCL ). Those strategies are currently the mainstreams of feature- based transfer learning. T able 5 presents the comparison results on Reuter-21578 (here we directly use the results of the baseline and CoCC from papers [78] and [41]). The baseline is a regularized lea st square regression model trained only on the labeled target domain instances [78]. Fig. 6, which has the same structure of Fig. 5, visualizes the performance. For clarity , thirtee n algorithms are divided into two parts that correspond to the two subﬁgures in Fig. 6. It can be observed that most algorithms are relatively well-performed for Orgs vs Place s 25 Method ĺ: ĺ: :ĺ ĺ ĺ :ĺ DAN 0.826 0.977 0.831 0.668 0.666 0.828 DCORAL 0.79 0.98 0.827 0.653 0.645 0.816 MRAN 0.914 0.969 0.998 0.864 0.683 0.709 0.856 CDAN 0.931 0.982 0.898 0.701 0.68 0.865 DANN 0.826 0.978 0.833 0.668 0.661 0.828 JAN 0.854 0.974 0.998 0.847 0.686 0.7 0.843 CAN 0.945 0.991 0.998 0.95 0.78 0.77 0.906 Baseline 0.616 0.954 0.99 0.638 0.511 0.498 0.701166667 ĺ: ĺ: :ĺ ĺ ĺ :ĺ DAN DCORAL MRAN CDAN DANN JAN CAN Baseline Fig. 7. Comparison results on Ofﬁce-31. T ABLE 5 Accuracy performance on the Reuters-21578 of three domains : Orgs, People, and Places. Model Orgs vs Places People vs Places Orgs vs People A verage HIDC 0.7698 0.6945 0.8375 0.7673 T riTL 0.7338 0.5517 0.7505 0.6787 CD-PLSA 0.5624 0.5749 0.7826 0.6400 MT rick 0.7494 0.6457 0.7930 0.7294 CoCC 0.6704 0.8264 0.7644 0.7537 SF A 0.7468 0.6768 0.7906 0.7381 mSLDA 0.5645 0.6064 0.5289 0.5666 SDA 0.6603 0.5556 0.5992 0.6050 GFK 0.6220 0.5417 0.6446 0.6028 SCL 0.6794 0.5046 0.6694 0.6178 TCA 0.7368 0.6065 0.7562 0.6998 JDA 0.5694 0.6296 0.7424 0.6471 T rAdaBoost 0.7336 0.7052 0.7879 0.7422 Baseline 0.6683 0.5198 0.6696 0.6192 T ABLE 6 Accuracy performance on Ofﬁce-31 of three domains: Amazon ( A), Webcam (W), and DSLR (D). Model A  W D  W W  D A  D D  A W  A A verage DAN 0.826 0.977 1.00 0.831 0.668 0.666 0.828 DCORAL 0.790 0.980 1.00 0.827 0.653 0.645 0.816 MRAN 0.914 0.969 0.998 0.864 0.683 0.709 0.856 CDAN 0.931 0.982 1.00 0.898 0.701 0.680 0.865 DANN 0.826 0.978 1.00 0.833 0.668 0.661 0.828 JAN 0.854 0.974 0.998 0.847 0.686 0.700 0.843 CAN 0.945 0.991 0.998 0.950 0.780 0.770 0.906 Baseline 0.616 0.954 0.990 0.638 0.511 0.498 0.701 and Orgs vs People, but poor for People vs Places. This phe- nomenon indicates that the discrepancy between People and Places may be relatively large. T rAdaBoost has a relatively good performance in this experiment because it uses the labels of the instances in the target domain to reduce the impact of the distribution difference. Besides, the algori thms HIDC, SF A, and MT rick have relatively consistent perfor- mance in the three tasks. These algorithms are also well- performed in the previous experiment on Amazon Reviews. In addition, the top two well-performed algorithms in terms of People vs Places are CoCC and T rAdaBoost. In the third experiment, seven deep-learning-based transfer learning models (i.e., DAN, DCORAL, MRAN, CDAN, DANN, JAN, and CAN) and the baseline (i.e., the Alexnet [138], [140] pre-trained on ImageNet [166] and then directly trained on the target domain) are performed on the dataset Ofﬁce-31 (here we directly use the results of CDAN, JAN, CAN, and the baseline from the original papers [137], [147], [151], [158]). The ResNet-50 [144] is used as the back - bone network for all these three models. The experimental results are provided in T able 6 and the average performance is visualized in Fig. 7. As shown in Fig. 7, all of these seven algorithms have excellent performance, especially o n the tasks D W and W D, whose accuracy is very close to one hundred percent. This phenomenon reﬂects the superiority of the deep-learning based approaches, and is consistent with the fact that the difference between W ebcam and DSLR is smaller than that between W ebcamDSLR and Amazon. Clearly , CAN outperforms the other six al- gorithms. In all the six tasks, the performance of DANN is similar to that of DAN, and is better than that of DCORAL, which indicates the effectiveness and the practicability o f incorporating adversarial learning. It is worth mentioning that, in the above experiments, the performance of some algorithms is not ideal. One reason is that we use the default parameter settings provided in the algorithms original papers, which may not be suitable for the dataset we selected. For example, GFK was originally designed for object recognition, and we directly adopt it into text classiﬁcation in the ﬁrst experiment, which turns out to produce an unsatisfactory result (having about sixty - two percent accuracy on average). The above experimental results are just for reference. These results demonstrate t hat some algorithms may not be suitable for the datasets of certain domains. Therefore, it is important to choose the appropriate algorithms as the baselines in the process of re - search. Besides, in practical applications, it is also nece ssary to ﬁnd a suitable algorithm. 8 C ONCLUSION AND FUTURE DIRECTION In this survey paper , we have summarized the mechanisms and the strategies of transfer learning from the perspectiv es of data and model. The survey gives the clear deﬁnitions about transfer learning and manages to use a uniﬁed sym- bol system to describe a large number of representative transfer learning approaches and related works. W e have basically introduced the objectives and strategies in tran sfer learning based on data-based interpretation and model- based interpretation. Data-based interpretation introdu ces the objectives, the strategies, and some transfer learning 26 approaches from the data perspective. Similarly , model- based interpretation introduces the mechanisms and the strategies of transfer learning but from the model level. The applications of transfer learning have also been intro- duced. At last, experiments have been conducted to evaluate the performance of representative transfer learning model s on two mainstream area, i.e., object recognition and text categorization. The comparisons of the models have also been given, which reﬂects that the selection of the transfer learning model is an important research topic as well as a complex issue in practical applications. Several directions are available for future research in the transfer learning area. First, transfer learning techn iques can be further explored and applied to a wider range of applications. And new approaches are needed to solve the knowledge transfer problems in more complex scenarios. For example, in real-world scenarios, sometimes the user- relevant source-domain data comes from another company . In this case, how to transfer the knowledge contained in the source domain while protecting user privacy is an important issue. Second, how to measure the transferability across do - mains and avoid negative transfer is also an important issue . Although there have been some studies on negative transfer , negative transfer still needs further systematic analyses [3]. Third, the interpretability of transfer learning also need s to be investigated further [216]. Finally , theoretical stu dies can be further conducted to provide theoretical support for the effectiveness and applicability of transfer learning. As a popular and promising area in machine learning, transfer learning shows some advantages over traditional machine learning such as less data dependency and less label depen- dency . W e hope our work can help readers have a better understanding of the research status and the research ideas . ACKNOWLEDGME NT S The research work is supported by the National Key Re- search and Development Program of China under Grant No. 2018YFB1004300, the National Natural Science Foundation of China under Grant No. U1836206, U1811461, 61773361, 61836013, and the Project of Y outh Innovation Promotion Association CAS under Grant No. 2017146. REFERENCES [1] D.N. Perkins and G. Salomon, T ransfer of Learning. Oxford, England: Pergamon, 1992. [2] S.J. Pan and Q. Y ang, A survey on transfer learning, IEEE T rans. Knowl. Data Eng. , vol. 22, no. 10, pp. 13451359, Oct. 2010. [3] Z. W ang, Z. Dai, B. Poczos, and J. Carbonell, Characterizing and avoiding negative transfer , in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Long Beach, Jun. 2019, pp. 11293 11302. [4] K. W eiss, T .M. Khoshgoftaar , and D. W ang, A survey of tra nsfer learning, J. Big Data , vol. 3, no. 1, Dec. 2016. [5] J. Huang, A.J. Smola, A. Gretton, K.M. Borgwardt, and B. Sch  olkopf, Correcting sample selection bias by unlabeled data, in Proc. 20th Annual Conference on Neural Information Processing Sy stems, V ancouver , Dec. 2006, pp. 601608. [6] M. Sugiyama, T . Suzuki, S. Nakajima, H. Kashima, P . Bnau, and M. Kawanabe, Direct importance estimation for covariate s hift adaptation, Ann. Inst. Stat. Math. , vol. 60, no. 4, pp. 699746, Dec. 2008. [7] O. Day and T .M. Khoshgoftaar , A survey on heterogeneous trans- fer learning, J. Big Data , vol. 4, no. 1, Dec. 2017. [8] M.E. T aylor and P . Stone, T ransfer learning for reinforc ement learning domains: A survey , J. Mach. Learn. Res. , vol. 10, pp. 1633 1685, Sep. 2009. [9] H.B. Ammar , E. Eaton, J.M. Luna, and P . Ruvolo, Autonomo us cross-domain knowledge transfer in lifelong policy gradie nt rein- forcement learning, in Proc. 24th International Joint Conference on Artiﬁcial Intelligence , Buenos Aires, Jul. 2015, pp. 33453351. [10] P . Zhao and S.C.H. Hoi, OTL: A framework of online transf er learning, in Proc. 27th International Conference on Machine Learning , Haifa, Jun. 2010, pp. 12311238. [11] O. Chapelle, B. Schlkopf, and A. Zien, Semi-supervised Learning . Cambridge: MIT Press, 2010. [12] S. Sun, A survey of multi-view machine learning, Neural Com- put. Appl. , vol. 23, no. 78, pp. 20312038, Dec. 2013. [13] C. Xu, D. T ao, and C. Xu, A survey on multi-view learning , 2013, arXiv:1304.5634v1. [14] J. Zhao, X. Xie, X. Xu, and S. Sun, Multi-view learning ove rview: Recent progress and new challenges, Inf. Fusion , vol. 38, pp. 4354, Nov . 2017. [15] D. Zhang, J. He, Y . Liu, L. Si, and R. Lawrence, Multi-vie w transfer learning with a large margin approach, in Proc. 17th ACM SIGKDD International Conference on Knowledge Discovery an d Data Mining, San Diego, Aug. 2011, pp. 12081216. [16] P . Y ang and W . Gao, Multi-view discriminant transfer l earning, in Proc. 23rd International Joint Conference on Artiﬁcial Int elligence, Beijing, Aug. 2013, pp. 18481854. [17] K.D. Feuz and D.J. Cook, Collegial activity learning b etween heterogeneous sensors, Knowl. Inf. Syst. , vol. 53, pp. 337364, Mar . 2017. [18] Y . Zhang and Q. Y ang, An overview of multi-task learnin g, Natl. Sci. Rev . , vol. 5, no. 1, pp. 3043, Jan. 2018. [19] W . Zhang, R. Li, T . Zeng, Q. Sun, S. Kumar , J. Y e, and S. Ji, De ep model based transfer and multi-task learning for biologica l image analysis, in Proc. 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Sydney , Aug. 2015, pp. 1475 1484. [20] A. Liu, N. Xu, W . Nie, Y . Su, and Y . Zhang, Multi-domain an d multi-task learning for human action recognition, IEEE T rans. Image Process. , vol. 28, no. 2, pp. 853867, Feb. 2019. [21] X. Peng, Z. Huang, X. Sun, and K. Saenko, Domain agnostic learning with disentangled representations, in Proc. 36th Interna- tional Conference on Machine Learning , Long Beach, Jun. 2019, pp. 51025112. [22] J. Lu, V . Behbood, P . Hao, H. Zuo, S. Xue, and G. Zhang, T ra nsfer learning using computational intelligence: A survey , Knowledge- Based Syst. , vol. 80, pp. 1423, May 2015. [23] C. T an, F . Sun, T . Kong, W . Zhang, C. Y ang, and C. Liu, A Surv ey on deep transfer learning, in Proc. 27th International Conference on Artiﬁcial Neural Networks , Rhodes, Oct. 2018, pp. 270279. [24] M. W ang and W . Deng, Deep visual domain adaptation: A survey , Neurocomputing, vol. 312, pp. 135153, Oct. 2018. [25] D. Cook, K.D. Feuz, and N.C. Krishnan, T ransfer learni ng for activity recognition: A survey , Knowl. Inf. Syst. , vol. 36, no. 3, pp. 537556, Sep. 2013. [26] L. Shao, F . Zhu, and X. Li, T ransfer learning for visual c ategoriza- tion: A survey , IEEE T rans. Neural Netw . Learn. Syst. , vol. 26, no. 5, pp. 10191034, May 2015. [27] W . Pan, A survey of transfer learning for collaborativ e recom- mendation with auxiliary data, Neurocomputing, vol. 177, pp. 447 453, Feb. 2016. [28] R. Liu, Y . Shi, C. Ji, and M. Jia, A Survey of sentiment anal ysis based on transfer learning, IEEE Access , vol. 7, pp. 8540185412, Jun. 2019. [29] Q. Sun, R. Chattopadhyay , S. Panchanathan, and J. Y e, A tw o- stage weighting framework for multi-source domain adaptat ion, in Proc. 25th Annual Conference on Neural Information Process ing Systems, Granada, Dec. 2011, pp. 505513. [30] M. Belkin, P . Niyogi, and V . Sindhwani, Manifold regula rization: A geometric framework for learning from labeled and unlabel ed examples, J. Mach. Learn. Res. , vol. 7, pp. 23992434, Nov . 2006. [31] W . Dai, Q. Y ang, G. Xue, and Y . Y u, Boosting for transfer learning, in Proc. 24th International Conference on Machine Learning , Corvalis, Jun. 2007, pp. 193200. [32] Y . Freund and R.E. Schapire, A decision-theoretic gene ralization of on-line learning and an application to boosting, J. Comput. Syst. Sci., vol. 55, no. 1, pp. 119139, Aug. 1997. 27 [33] Y . Y ao and G. Doretto, Boosting for transfer learning w ith multi- ple sources, in Proc. IEEE Conference on Computer V ision and Pattern Recognition, San Francisco, Jun. 2010, pp. 18551862. [34] J. Jiang and C. Zhai, Instance weighting for domain ada ptation in NLP , in Proc. 45th Annual Meeting of the Association of Computation al Linguistics, Prague, Jun. 2007, pp. 264271. [35] K.M. Borgwardt, A. Gretton, M.J. Rasch, H.-P . Kriegel, B. Scholkopf, and A.J. Smola, Integrating structured biologic al data by kernel maximum mean discrepancy , Bioinformatics, vol. 22, no. 14, pp. 4957, Jul. 2006. [36] S.J. Pan, I.W . T sang, J.T . Kwok, and Q. Y ang, Domain adap tation via transfer component analysis, IEEE T rans. Neural Netw . , vol. 22, no. 2, pp. 199210, Feb. 2011. [37] M. Ghifary , W .B. Kleijn, and M. Zhang, Domain adaptive neural networks for object recognition, in Proc. Paciﬁc Rim International Conference on Artiﬁcial Intelligence , Gold Coast, Dec. 2014, pp. 898 904. [38] M. Long, J. W ang, G. Ding, J. Sun, and P .S. Y u, T ransfer fea ture learning with joint distribution adaptation,in Proc. IEEE Interna- tional Conference on Computer V ision , Sydney , Dec. 2013, pp. 2200 2207. [39] M. Long, J. W ang, G. Ding, S.J. Pan, and P .S. Y u, Adaptatio n regularization: A general framework for transfer learning , IEEE T rans. Knowl. Data Eng. , vol. 26, no. 5, pp. 1076-1089, May 2014. [40] S. Kullback and R.A. Leibler , On information and sufﬁci ency , Ann. Math. Statist. , vol. 22, no. 1, pp. 7986, 1951. [41] W . Dai, G.-R. Xue, Q. Y ang, and Y . Y u, Co-clustering bas ed classiﬁcation for out-of-domain documents, in Proc. 13th ACM SIGKDD International Conference on Knowledge Discovery an d Data Mining, San Jose, Aug. 2007, pp. 210219. [42] W . Dai, Q. Y ang, G. Xue, and Y . Y u, Self-taught clusterin g, in Proc. 25th International Conference of Machine Learning , Helsinki, Jul. 2008, pp. 200207. [43] J. Davis and P . Domingos, Deep transfer via second-ord er Markov logic, in Proc. 26th International Conference on Machine Learning, Montreal, Jun. 2009, pp. 217224. [44] F . Zhuang, X. Cheng, P . Luo, S.J. Pan, and Q. He, Supervise d rep- resentation learning: T ransfer learning with deep autoenc oders, in Proc. 24th International Joint Conference on Artiﬁcial Int elligence, Buenos Aires, Jul. 2015, pp. 41194125. [45] I. Dagan, L. Lee, and F . Pereira, Similarity-based meth ods for word sense disambiguation, in Proc. 35th Annual Meeting of the Association of Computational Linguistics and 8th Conferen ce of the European Chapter of the Association for Computational Ling uistics (ACLEACL), Madrid, Jul. 1997, pp. 5663. [46] B. Chen, W . Lam, I. T sang, and T . W ong, Location and scat ter matching for dataset shift in text mining, in Proc. 10th IEEE International Conference on Data Mining , Sydney , Dec. 2010, pp. 773 778. [47] S. Dey , S. Madikeri, and P . Motlicek, Information theore tic clus- tering for unsupervised domain-adaptation, in Proc. IEEE Interna- tional Conference on Acoustics, Speech and Signal Processi ng, Shanghai, Mar . 2016, pp. 55805584. [48] W .-H. Chen, P .-C. Cho, and Y .-L. Jiang, Activity recog nition using transfer learning, Sens. Mater . , vol. 29, no. 7, pp. 897904, Jul. 2017. [49] J. Giles, K.K. Ang, L.S. Mihaylova, and M. Arvaneh, A sub ject- to-subject transfer learning framework based on Jensen-Sha nnon divergence for improving brain-computer interface, in Proc. IEEE International Conference on Acoustics, Speech and Signal P rocessing, Brighton, May 2019, pp. 30873091. [50] L.M. Bregman, The relaxation method of ﬁnding the comm on point of convex sets and its application to the solution of pr oblems in convex programming, USSR Comput. Math. Math. Phys. , vol. 7, no. 3, pp. 200217, 1967. [51] S. Si, D. T ao, and B. Geng, Bregman divergence-based regu lariza- tion for transfer subspace learning, IEEE T rans. Knowl. Data Eng. , vol. 22, no. 7, pp. 929942, Jul. 2010. [52] H. Sun, S. Liu, S. Zhou, and H. Zou, Unsupervised cross-vie w semantic transfer for remote sensing image classiﬁcation,  IEEE Geosci. Remote Sens. Lett. , vol. 13, no. 1, pp. 1317, Jan. 2016. [53] H. Sun, S. Liu, and S. Zhou, Discriminative subspace align ment for unsupervised visual domain adaptation, Neural Process. Lett. , vol. 44, no. 3, pp. 779793, Dec. 2016. [54] Q. Shi, Y . Zhang, X. Liu, and K. Zhao, Regularised transf er learning for hyperspectral image classiﬁcation, IET Comput. V is. , vol. 13, no. 2, pp. 188193, Feb. 2019. [55] A. Gretton, O. Bousquet, A.J. Smola, and B. Schlkopf, Mea suring statistical dependence with Hilbert-Schmidt norms, in Proc. 18th International Conference on Algorithmic Learning Theory , Singapore, Oct. 2005, pp. 6377. [56] H. W ang and Q. Y ang, T ransfer learning by structural an alogy , in Proc. 25th AAAI Conference on Artiﬁcial Intelligence , San Francisco, Aug. 2011, pp. 513518. [57] M. Xiao and Y . Guo, Feature space independent semi-sup ervised domain adaptation via kernel matching, IEEE T rans. Pattern Anal. Mach. Intell. , vol. 37, no. 1, pp. 5466, Jan. 2015. [58] K. Y an, L. Kou, and D. Zhang, Learning domain-invarian t sub- space using domain features and independence maximization , IEEE T . Cybern. , vol. 48, no. 1, pp. 288299, Jan. 2018. [59] J. Shen, Y . Qu, W . Zhang, and Y . Y u, W asserstein distance guided representation learning for domain adaptation, in Proc. 32nd AAAI Conference on Artiﬁcial Intelligence , New Orleans, Feb. 2018, pp. 40584065. [60] C.-Y . Lee, T . Batra, M.H. Baig, and D. Ulbricht, Sliced W asserstein discrepancy for unsupervised domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Long Beach, Jun. 2019, pp. 1028510295. [61] W . Zellinger , T . Grubinger , E. Lughofer , T . Natschlger , and S. Saminger-Platz, Central moment discrepancy (CMD) for doma in- invariant representation learning, in Proc. 5th International Confer- ence on Learning Representations , T oulon, Apr . 2017, pp. 113. [62] A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan , M. Pon- til, K. Fukumizu, and B.K. Sriperumbudur , Optimal kernel ch oice for large-scale two-sample tests, in Proc. 26th Annual Conference on Neural Information Processing Systems , Lake T ahoe, Dec. 2012, pp. 12051213. [63] H. Y an, Y . Ding, P . Li, Q. W ang, Y . Xu, and W . Zuo, Mind the class weight bias: W eighted maximum mean discrepancy for un su- pervised domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Honolulu, Jul. 2017, pp. 22722281. [64] H. Daum  e III, Frustratingly easy domain adaptation,  in Proc. 45th Annual Meeting of the Association for Computational Li nguistics, Prague, Jun. 2007, pp. 256263. [65] H. Daum  e III, A. Kumar , and A. Saha, Co-regularization based semi-supervised domain adaptation, in Proc. 24th Annual Confer- ence on Neural Information Processing Systems , V ancouver , Dec. 2010, pp. 478486. [66] L. Duan, D. Xu, and I.W . T sang, Learning with augmented features for heterogeneous domain adaptation, in Proc. 29th Inter- national Conference on Machine Learning , Edinburgh, Jun. 2012, pp. 18. [67] W . Li, L. Duan, D. Xu, and I.W . T sang, Learning with augm ented features for supervised and semi-supervised heterogeneou s do- main adaptation, IEEE T rans. Pattern Anal. Mach. Intell. , vol. 36, no. 6, pp. 11341148, Jun. 2014. [68] K.I. Diamantaras and S.Y . Kung, Principal Component Neural Net- works. New Y ork: Wiley , 1996. [69] B. Schlkopf, A. Smola, and K. Mller , Nonlinear component anal- ysis as a kernel eigenvalue problem, Neural Comput. , vol. 10, no. 5, pp. 12991319, Jul. 1998. [70] J. W ang, Y . Chen, S. Hao, W . Feng, and Z. Shen, Balanced distribution adaptation for transfer learning, in Proc. 17th IEEE International Conference on Data Mining , New Orleans, Nov . 2017, pp. 11291134. [71] A. Blum and T . Mitchell, Combining labeled and unlabel ed data with co-training, in Proc. 11th Annual Conference on Computational Learning Theory , Madison, Jul. 1998, pp. 92100. [72] M. Chen, K.Q. W einberger , and J.C. Blitzer , Co-traini ng for domain adaptation, in Proc. 25th Annual Conference on Neural Information Processing Systems , Granada, Dec. 2011, pp. 24562464. [73] Z.-H. Zhou and M. Li, T ri-training: Exploiting unlabe led data using three classiﬁers, IEEE T rans. Knowl. Data Eng. , vol. 17, no. 11, pp. 15291541, Nov . 2005. [74] K. Saito, Y . Ushiku, and T . Harada, Asymmetric tri-trai ning for unsupervised domain adaptation, in Proc. 34th International Conference on Machine Learning , Sydney , Aug. 2017, pp. 29882997. [75] S.J. Pan, J.T . Kwok, and Q. Y ang, T ransfer learning via d imen- sionality reduction, in Proc. 23rd AAAI Conference on Artiﬁcial Intelligence, Chicago, Jul. 2008, pp. 677682. [76] K.Q. W einberger , F . Sha, and L.K. Saul, Learning a kernel matrix for nonlinear dimensionality reduction, in Proc. 21st International Conference on Machine Learning , Banff, Jul. 2004, pp. 106113. 28 [77] L. V andenberghe and S. Boyd, Semideﬁnite programming, SIAM Rev . , vol. 38, no. 1, pp. 4995, Mar . 1996. [78] S.J. Pan, I.W . T sang, J.T . Kwok, and Q. Y ang, Domain adap tation via transfer component analysis, in Proc. 21st International Joint Conference on Artiﬁcial Intelligence , Pasadena, Jul. 2009, pp. 1187 1192. [79] C. Hou, Y .H. T sai, Y . Y eh, and Y .F . W ang, Unsupervised d omain adaptation with label and structural consistency , IEEE T rans. Image Process., vol. 25, no. 12, pp. 55525562, Dec. 2016. [80] J. T ahmoresnezhad and S. Hashemi, V isual domain adapta tion via transfer feature learning, Knowl. Inf. Syst. , vol. 50, no. 2, pp. 585605, Feb. 2017. [81] J. Zhang, W . Li, and P . Ogunbona, Joint geometrical and statistical alignment for visual domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Honolulu, Jul. 2017, pp. 51505158. [82] B. Schlkopf, R. Herbrich, and A.J. Smola, A generalized r epre- senter theorem, in Proc. International Conference on Computational Learning Theory , Amsterdam, Jul. 2001, pp. 416426. [83] L. Duan, I.W . T sang, and D. Xu, Domain transfer multipl e kernel learning, IEEE T rans. Pattern Anal. Mach. Intell. , vol. 34, no. 3, pp. 465479, Mar . 2012. [84] A. Rakotomamonjy , F .R. Bach, S. Canu, and Y . Grandvalet, Sim- pleMKL, J. Mach. Learn. Res. , vol. 9, pp. 2491-2521, Nov . 2008. [85] I.S. Dhillon, S. Mallela, and D.S. Modha, Information-th eoretic co-clustering, in Proc. 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , W ashington, Aug. 2003, pp. 8998. [86] S. Deerwester , S.T . Dumais, G.W . Furnas, T .K. Landauer , a nd R. Harshman, Indexing by latent semantic analysis, J. Am. Soc. Inf. Sci., vol. 41, pp. 391407, Sep. 1990. [87] T . Hofmann, Probabilistic latent semantic analysis,  in Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence , Stockholm, Jul. 1999, pp. 289296. [88] J. Y oo and S. Choi, Probabilistic matrix tri-factoriza tion, in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, T aipei, Apr . 2009, pp. 15531556. [89] A. Dempster , N. Laird, and D. Rubin, Maximum likelihoo d from incomplete data via the EM algorithm, J. R. Stat. Soc. - Ser . B , vol. 39, no. 1, pp. 138, 1977. [90] G.-R. Xue, W . Dai, Q. Y ang, and Y . Y u, T opic-bridged PLSA for cross-domain text classiﬁcation, in Proc. 31st Annual International ACM SIGIR Conference on Research and Development in Informa tion Retrieval, Singapore, Jul. 2008, pp. 627634. [91] F . Zhuang, P . Luo, Z. Shen, Q. He, Y . Xiong, Z. Shi, and H. Xio ng, Collaborative Dual-PLSA: Mining distinction and commonal ity across multiple domains for text classiﬁcation, in Proc. 19th ACM International Conference on Information and Knowledge Man agement, T oronto, Oct. 2010, pp. 359368. [92] F . Zhuang, P . Luo, Z. Shen, Q. He, Y . Xiong, Z. Shi, and H. Xio ng, Mining distinction and commonality across multiple domai ns using generative model for text classiﬁcation, IEEE T rans. Knowl. Data Eng. , vol. 24, no. 11, pp. 20252039, Nov . 2012. [93] F . Zhuang, P . Luo, P . Y in, Q. He, and Z. Shi, Concept learn - ing for cross-domain text classiﬁcation: A general probabi listic framework, in Proc. 23rd International Joint Conference on Artiﬁcial Intelligence, Beijing, Aug. 2013, pp. 19601966. [94] J. Blitzer , R. McDonald, and F . Pereira, Domain adapta tion with structural correspondence learning, in Proc. Conference on Empirical Methods in Natural Language Processing , Sydney , Jul. 2006, pp. 120 128. [95] R.K. Ando and T . Zhang, A framework for learning predic tive structures from multiple tasks and unlabeled data, J. Mach. Learn. Res., vol. 6, pp. 18171853, Dec. 2005. [96] X. Glorot, A. Bordes, and Y . Bengio, Domain adaptation for large-scale sentiment classiﬁcation: A deep learning appr oach, in Proc. 28th International Conference on Machine Learning , Bellevue, Jun. 2011, pp. 513520. [97] P . V incent, H. Larochelle, Y . Bengio, and P .-A. Manzago l, Extract- ing and composing robust features with denoising autoencod ers, in Proc. 25th International Conference on Machine Learning , Helsinki, Jul. 2008, pp. 10961103. [98] M. Chen, Z. Xu, K. W einberger , and F . Sha, Marginalized d enois- ing autoencoders for domain adaptation, in Proc. 29th International Conference on Machine Learning , Edinburgh, Jun. 2012, pp. 767774. [99] M. Chen, K.Q. W einberger , Z. Xu, and F . Sha, Marginalizi ng stacked linear denoising autoencoders, J. Mach. Learn. Res. , vol. 16, no. 1, pp. 38493875, Jan. 2015. [100] B. Fernando, A. Habrard, M. Sebban, and T . T uytelaars,  Unsu- pervised visual domain adaptation using subspace alignmen t, in Proc. IEEE International Conference on Computer V ision , Sydney , Dec. 2013, pp. 29602967. [101] B. Sun and K. Saenko, Subspace distribution alignment fo r unsupervised domain adaptation, in Proc. British Machine V ision Conference, Swansea, Sep. 2015, pp. 24.124.10. [102] B. Gong, Y . Shi, F . Sha, and K. Grauman, Geodesic ﬂow kern el for unsupervised domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Providence, Jun. 2012, pp. 20662073. [103] R. Gopalan, Ruonan Li, and R. Chellappa, Domain adapt ation for object recognition: An unsupervised approach, in Proc. IEEE International Conference on Computer V ision , Barcelona, Jun. 2011, pp. 999-1006. [104] M.I. Zelikin, Control Theory and Optimization I in Encyclopaedia of Mathematical Sciences, vol. 86, Berlin: Springer , 2000. [105] B. Sun, J. Feng, and K. Saenko, Return of frustratingly e asy domain adaptation, in Proc. 30th AAAI Conference on Artiﬁcial Intelligence, Phoenix, Feb. 2016, pp. 20582065. [106] S.J. Pan, X. Ni, J.-T . Sun, Q. Y ang, and Z. Chen, Cross-do main sentiment classiﬁcation via spectral feature alignment, in Proc. 19th International Conference on World Wide Web , Raleigh, Apr . 2010, pp. 751760. [107] J. Blitzer , M. Dredze, and F . Pereira, Biographies, b ollywood, boom-boxes and blenders: Domain adaptation for sentiment c lassi- ﬁcation, in Proc. 45th Annual Meeting of the Association of Computa- tional Linguistics , Prague, Jun. 2007, pp. 440447. [108] F .R.K. Chung, Spectral Graph Theory . Providence: American Math- ematical Society , 1997. [109] A.Y . Ng, M.I. Jordan, and Y . W eiss, On spectral cluste ring: Analysis and an algorithm, in Proc. 15th Annual Conference on Neural Information Processing Systems , V ancouver , Dec. 2001, pp. 849- 856. [110] X. Ling, W . Dai, G.-R. Xue, Q. Y ang, and Y . Y u, Spectral d omain- transfer learning, in Proc. 14th ACM SIGKDD International Confer- ence on Knowledge Discovery and Data Mining , Las V egas, Aug. 2008, pp. 488496. [111] S.D. Kamvar , D. Klein, and C.D. Manning, Spectral learn ing, in Proc. 18th International Joint Conference on Artiﬁcial Int elligence, Acapulco, Aug. 2003, pp. 561566. [112] J. Shi and J. Malik, Normalized cuts and image segmenta tion, IEEE T rans. Pattern Anal. Mach. Intell. , vol .22, no. 8, pp. 888905, Aug. 2000. [113] L. Duan, I.W . T sang, D. Xu, and T .-S. Chua, Domain adapt ation from multiple sources via auxiliary classiﬁers, in Proc. 26th In- ternational Conference on Machine Learning , Montreal, Jun. 2009, pp. 289296. [114] L. Duan, D. Xu, and I.W . T sang, Domain adaptation from multiple sources: A domain-dependent regularization appr oach, IEEE T rans. Neural Netw . Learn. Syst. , vol. 23, no. 3, pp. 504518, Mar . 2012. [115] P . Luo, F . Zhuang, H. Xiong, Y . Xiong, and Q. He, T ransf er learn- ing from multiple source domains via consensus regularizat ion, in Proc. 17th ACM Conference on Information and Knowledge Mana gement, Napa V alley , Oct. 2008, pp. 103112. [116] F . Zhuang, P . Luo, H. Xiong, Y . Xiong, Q. He, and Z. Shi, C ross- domain learning from multiple sources: A consensus regular ization perspective, IEEE T rans. Knowl. Data Eng. , vol. 22, no. 12, pp. 1664 1678, Dec. 2010. [117] T . Evgeniou, C.A. Micchelli, and M. Pontil, Learning multiple tasks with kernel methods, J. Mach. Learn. Res. , vol. 6, pp. 615-637, Apr . 2005. [118] T . Kato, H. Kashima, M. Sugiyama, and K. Asai, Multi-ta sk learning via conic programming, in Proc. 21st Annual Conference on Neural Information Processing Systems , V ancouver , Dec. 2007, pp. 737744. [119] A.J. Smola and B. Schlkopf, A tutorial on support vector regres- sion, Stat. Comput. , vol. 14, no. 3, pp. 199222, Aug. 2004. [120] J. W eston, R. Collobert, F . Sinz, L. Bottou, and V . V apni k, Infer- ence with the universum, in Proc. 23rd International Conference on Machine Learning , Pittsburgh, Jun. 2006, pp. 10091016. [121] X. Y u and Y . Aloimonos, Attribute-based transfer lea rning for object categorization with zeroone training example, in Proc. 29 European Conference on Computer V ision , Heraklion, Sep. 2010, pp. 127140. [122] F . Zhuang, P . Luo, H. Xiong, Q. He, Y . Xiong, and Z. Shi, E x- ploiting associations between word clusters and document c lasses for cross-domain text categorization, Stat. Anal. Data Min. , vol. 4, no. 1, pp. 100114, Feb. 2011. [123] F . Zhuang, P . Luo, C. Du, Q. He, Z. Shi, and H. Xiong, T rip lex transfer learning: Exploiting both shared and distinct con cepts for text classiﬁcation, IEEE T . Cybern. , vol. 44, no. 7, pp. 11911203, Jul. 2014. [124] M. Long, J. W ang, G. Ding, W . Cheng, X. Zhang, and W . W ang , Dual transfer learning, in Proc. 12th SIAM International Conference on Data Mining , Anaheim, Apr . 2012, pp. 540551. [125] H. W ang, F . Nie, H. Huang, and C. Ding, Dyadic transfer learn- ing for cross-domain image classiﬁcation, in Proc. International Conference on Computer V ision , Barcelona, Nov . 2011, pp. 551556. [126] D. W ang, C. Lu, J. Wu, H. Liu, W . Zhang, F . Zhuang, and H. Zhang, Softly associative transfer learning for cros s- domain classiﬁcation, IEEE T . Cybern. , to be published. doi: 10.1109TCYB.2019.2891577. [127] Q. Do, W . Liu, J. Fan, and D. T ao, Unveiling hidden impl icit similarities for cross-domain recommendation, IEEE T rans. Knowl. Data Eng. , to be published. doi: 10.1109TKDE.2019.2923904. [128] T . T ommasi and B. Caputo, The more you know , the less you learn: from knowledge transfer to one-shot learning of o bject categories in Proc. British Machine V ision Conference , London, Sep. 2009, pp. 80.180.11. [129] G.C. Cawley , Leave-one-out cross-validation based model selec- tion criteria for weighted LS-SVMs, in Proc. IEEE International Joint Conference on Neural Network , V ancouver , Jul. 2006, pp. 16611668. [130] T . T ommasi, F . Orabona, and B. Caputo, Safety in number s: Learning categories from few examples with multi model know l- edge transfer , in Proc. IEEE Conference on Computer V ision and Pattern Recognition , San Francisco, Jun. 2010, pp. 30813088. [131] C.-K. Lin, Y .-Y . Lee, C.-H. Y u, and H.-H. Chen, Explor ing ensemble of models in taxonomy-based cross-domain sentime nt classiﬁcation, in Proc. 23rd ACM International on Conference on Information and Knowledge Management , Shanghai, Nov . 2014, pp. 12791288. [132] J. Gao, W . Fan, J. Jiang, and J. Han, Knowledge transfe r via mul- tiple model local structure mapping, in Proc. 14th ACM SIGKDD International Conference on Knowledge Discovery and Data M ining, Las V egas, Aug. 2008, pp. 283291. [133] F . Zhuang, P . Luo, S.J. Pan, H. Xiong, and Q. He. Ensembl e of anchor adapters for transfer learning, in Proc. 25th ACM In- ternational on Conference on Information and Knowledge Man agement, Indianapolis, Oct. 2016, pp. 23352340. [134] F . Zhuang, X. Cheng, P . Luo, S.J. Pan, and Q. He, Supervis ed representation learning with double encoding-layer autoe ncoder for transfer learning, ACM T rans. Intell. Syst. T echnol. , vol. 9, no. 2, pp. 117, Jan. 2018. [135] M. Ghifary , W .B. Kleijn, and M. Zhang, Domain adaptiv e neural networks for object recognition, in Proc. 13th Paciﬁc Rim Interna- tional Conference on Artiﬁcial Intelligence , Gold Coast, Dec. 2014, pp. 898904. [136] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T . Darrell , Deep domain confusion: Maximizing for domain invariance,  2014, arXiv:1412.3474v1. [137] M. Long, Y . Cao, J. W ang, and M.I. Jordan, Learning tra nsferable features with deep adaptation networks, in Proc. 32nd International Conference on Machine Learning , Lille, Jul. 2015, pp. 97105. [138] A. Krizhevsky , I. Sutskever , and G.E. Hinton, Imagene t classi- ﬁcation with deep convolutional neural networks, in Proc. 26th Annual Conference on Neural Information Processing System s, Lake T ahoe, Dec. 2012, pp. 10971105. [139] I. Goodfellow , J. Pouget-Abadie, M. Mirza, B. Xu, D. W a rde- Farley , S. Ozair , A. Courville, and Y . Bengio, Generative ad ver- sarial nets, in Proc. 28th Annual Conference on Neural Information Processing Systems , Montreal, Dec. 2014, pp. 26722680. [140] J. Y osinski, J. Clune, Y . Bengio, and H. Lipson, How tr ansferable are features in deep neural networks? in Proc. 28th Annual Confer- ence on Neural Information Processing Systems , Montreal, Dec. 2014, pp. 33203328. [141] M. Long, Y . Cao, Z. Cao, J. W ang, and M.I. Jordan, T rans - ferable representation learning with deep adaptation netw orks, IEEE T rans. Pattern Anal. Mach. Intell. , to be published. doi: 10.1109TP AMI.2018.2868685. [142] Y . Grandvalet and Y . Bengio, Semi-supervised learnin g by en- tropy minimization, in Proc. 18th Annual Conference on Neural Information Processing Systems , V ancouver , Dec. 2004, pp. 529536. [143] C. Szegedy , W . Liu, Y . Jia, P . Sermanet, S. Reed, D. Anguelo v , D. Erhan, V . V anhoucke, and A. Rabinovich, Going deeper wit h convolutions, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Boston, Jun. 2015, pp. 19. [144] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learnin g for image recognition, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Las V egas, Jun. 2016, pp. 770778. [145] K.P . Chwialkowski, A. Ramdas, D. Sejdinovic, and A. Gre tton, Fast two-sample testing with analytic representations of probabil- ity measures, in Proc. 29th Annual Conference on Neural Information Processing Systems , Montreal, Dec. 2015, pp. 19811989. [146] M. Long, H. Zhu, J. W ang, and M.I. Jordan, Unsupervise d do- main adaptation with residual transfer networks, in Proc. 30th An- nual Conference on Neural Information Processing Systems , Barcelona, Dec. 2016, pp. 136144. [147] M. Long, H. Zhu, J. W ang, and M.I. Jordan, Deep transfe r learning with joint adaptation networks, in Proc. 34th International Conference on Machine Learning , Sydney , Aug. 2017, pp. 22082217. [148] B. Sun and K. Saenko, Deep CORAL: Correlation alignment for deep domain adaptation, in Proc. European Conference on Computer V ision Workshops , Amsterdam, Oct. 2016, pp. 443450. [149] C. Chen, Z. Chen, B. Jiang, and X. Jin, Joint domain ali gnment and discriminative feature learning for unsupervised deep domain adaptation, in Proc. 33rd AAAI Conference on Artiﬁcial Intelligence , Honolulu, Jan. 2019, pp. 32963303. [150] Y . Pan, T . Y ao, Y . Li, Y . W ang, C.-W . Ngo, and T . Mei, T ra ns- ferrable prototypical networks for unsupervised domain ad ap- tation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition, Long Beach, Jun. 2019, pp. 22392247. [151] G. Kang, L. Jiang, Y . Y ang, and A.G. Hauptmann, Contra stive adaptation network for unsupervised domain adaptation, i n Proc. IEEE Conference on Computer V ision and Pattern Recognition , Long Beach, Jun. 2019, pp. 48934902. [152] Y . Zhu, F . Zhuang, J. W ang, J. Chen, Z. Shi, W . Wu, and Q. He , Multi-representation adaptation network for cross-doma in image classiﬁcation, Neural Netw . , vol. 119. pp. 214221, Nov . 2019. [153] Y . Zhu, F . Zhuang, and D. W ang, Aligning domain-speci ﬁc dis- tribution and classiﬁer for cross-domain classiﬁcation fr om multiple sources, in Proc. 33rd AAAI Conference on Artiﬁcial Intelligence , Honolulu, Jan. 2019, pp. 59895996. [154] Y . Ganin and V . Lempitsky , Unsupervised domain adapt ation by backpropagation, in Proc. 32nd International Conference on Machine Learning, Lille, Jul. 2015, pp. 11801189. [155] Y . Ganin, E. Ustinova, H. Ajakan, P . Germain, H. Laroch elle, F .Laviolette, M. Marchand, and V . Lempitsky , Domain-adve rsarial training of neural networks, J. Mach. Learn. Res. , vol. 17, pp. 135, Apr . 2016. [156] E. Tzeng, J. Hoffman, K. Saenko, and T . Darrell, Advers arial discriminative domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Honolulu, Jul. 2017, pp. 29622971. [157] J. Hoffman, E. Tzeng, T . Park, J.-Y . Zhu, P . Isola, K. Sae nko, A.A. Efros, and T . Darrell, CyCADA: Cycle-consistent adve rsarial domain adaptation, in Proc. 35th International Conference on Machine Learning, Stockholm, Jul. 2018, pp. 19942003. [158] M. Long, Z. Cao, J. W ang, and M.I. Jordan, Conditional adver- sarial domain adaptation, in Proc. 32nd Annual Conference on Neural Information Processing Systems , Montreal, Dec. 2018, pp. 16401650. [159] Y . Zhang, H. T ang, K. Jia, and M. T an, Domain-symmetri c net- works for adversarial domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Long Beach, Jun. 2019, pp. 50315040. [160] H. Zhao, S. Zhang, G. Wu, J.M.F . Moura, J.P . Costeira, an d G.J. Gordon, Adversarial multiple source domain adaptation, in Proc. 32nd Annual Conference on Neural Information Processing Sy stems, Montreal, Dec. 2018, pp. 85598570. [161] C. Y u, J. W ang, Y . Chen, and M. Huang, T ransfer learnin g with dynamic adversarial adaptation network, in Proc. 19th IEEE International Conference on Data Mining , Beijing, Nov . 2019, pp. 19. [162] J. Zhang, Z. Ding, W . Li, and P . Ogunbona, Importance w eighted adversarial nets for partial domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Salt Lake City , Jun. 2018, pp. 81568163. 30 [163] Z. Cao, M. Long, J. W ang, and M.I. Jordan, Partial tran sfer learn- ing with selective adversarial networks, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Salt Lake City , Jun. 2018, pp. 27242732. [164] B. W ang, M. Qiu, X. W ang, Y . Li, Y . Gong, X. Zeng, J. Huang , B. Zheng, D. Cai, and J. Zhou, A minimax game for instance based selective transfer learning, in Proc. 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Anchorage, Aug. 2019, pp. 3443. [165] X. Chen, S. W ang, M. Long, and J. W ang, T ransferability vs. discriminability: Batch spectral penalization for advers arial domain adaptation, in Proc. 36th International Conference on Machine Learn- ing, Long Beach, Jun. 2019, pp. 10811090. [166] J. Deng, W . Dong, R. Socher , L.-J. Li, K. Li, and L. Fei-Fe i, ImageNet: A large-scale hierarchical image database, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Miami, Jun. 2009, pp. 248255. [167] M. Maqsood, F . Nazir , U. Khan, F . Aadil, H. Jamal, I. Meh mood, and O. Song, T ransfer learning assisted classiﬁcation and d etection of Alzheimer s disease stages using 3D MRI scans, Sensors, vol. 19, no. 11, pp. 119, Jun. 2019. [168] D.S. Marcus, A.F . Fotenos, J.G. Csernansky , J.C. Morri s, and R.L. Buckner , Open access series of imaging studies: Longitudi nal MRI data in nondemented and demented older adults, J. Cogn. Neurosci. , vol. 22, no. 12, pp. 26772684, Dec. 2010. [169] H.-C. Shin, H.R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Y ao, D. Mollura, and R.M. Summers, Deep convolutional neura l networks for computer-aided detection: CNN architectures , dataset characteristics and transfer Learning, IEEE T rans. Med. Imaging , vol. 35, no. 5, pp. 12851298, May 2016. [170] M. Byra, M. Wu, X. Zhang, H. Jang, Y .-J. Ma, E.Y . Chang, S. Shah, and Jiang Du, Knee menisci segmentation and relaxomet ry of 3D ultrashort echo time cones MR imaging using attention UNet with transfer learning, Magn. Reson. Med. , Sep. 2019, doi: 10.1002mrm.27969. [171] X. T ang, B. Du, J. Huang, Z. W ang, and L. Zhang, On combi ning active and transfer learning for medical data classiﬁcatio n, IET Comput. V is. , vol. 13, no. 2, pp. 194205, Feb. 2019. [172] M. Zeng, M. Li, Z. Fei, Y . Y u, Y . Pan, and J. W ang, Automa tic ICD-9 coding via deep transfer learning, Neurocomputing, vol. 324, pp. 4350, Jan. 2019. [173] G. Schweikert, G. Ratsch, C. Widmer , and B. Scholkopf, A n empirical analysis of domain adaptation algorithms for gen omic sequence analysis, in Proc. 22nd Annual Conference on Neural Infor- mation Processing Systems , V ancouver , Dec. 2008, pp. 14331440. [174] R. Petegrosso, S. Park, T .H. Hwang, and R. Kuang, T rans fer learning across ontologies for phenome-genome associatio n predic- tion, Bioinformatics, vol. 33, no. 4, pp. 529536, Feb. 2017. [175] T . Hwang and R. Kuang, A heterogeneous label propagat ion al- gorithm for disease gene discovery , in Proc. 10th SIAM International Conference on Data Mining , Columbus, Apr . 2010, pp. 583594. [176] Q. Xu, E.W . Xiang, and Q. Y ang, Protein-protein inter action prediction via collective matrix factorization, in Proc. IEEE Interna- tional Conference on Bioinformatics and Biomedicine , Hong Kong, Dec. 2010, pp. 6267. [177] A.P . Singh and G.J. Gordon, Relational learning via co llective matrix factorization, in Proc. 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Las V egas, Aug. 2008, pp. 650658. [178] S. Di, H. Zhang, C. Li, X. Mei, D. Prokhorov , and H. Ling,  Cross- domain trafﬁc scene understanding: A dense correspondence -based transfer learning approach, IEEE T rans. Intell. T ransp. Syst. , vol. 19, no. 3, pp. 745757, Mar . 2018. [179] H. Abdi, Partial least squares regression and projec tion on latent structure regression (PLS Regression), Wiley Interdiscip. Rev .- Comput. Statist. , vol. 2, no. 1, pp. 97106, Jan. 2010. [180] S.D. Pietra, V .D. Pietra, and J. Lafferty , Inducing fe atures of random ﬁelds, IEEE T rans. Pattern Anal. Mach. Intell. , vol. 19, no. 4, pp. 380393, Apr . 1997. [181] C. Liu, J. Y uen, and A. T orralba, Nonparametric scene parsing via label transfer , IEEE T rans. Pattern Anal. Mach. Intell. , vol. 33, no. 12, pp. 23682382, Dec. 2011. [182] C. Lu, F . Hu, D. Cao, J. Gong, Y . Xing, and Z. Li, T ransfe r learning for driver model adaptation in lane-changing scen arios using manifold alignment, IEEE T rans. Intell. T ransp. Syst. , to be published. doi: 10.1109TITS.2019.2925510. [183] D.J. Berndt and J. Clifford, Using dynamic time warpi ng to ﬁnd patterns in time series, in Proc. Knowledge Discovery in Databases Workshop, Seattle, Jul. 1994, pp. 359370. [184] N. Makondo, M. Hiratsuka, B. Rosman, and O. Hasegawa,  A non-linear manifold alignment approach to robot learning f rom demonstrations, J. Robot. Mechatron. , vol. 30, no. 2, pp. 265281, Apr . 2018. [185] P . Angkititrakul, C. Miyajima, and K. T akeda, Modeli ng and adaptation of stochastic driver-behavior model with appli cation to car following, in Proc. IEEE Intelligent V ehicles Symposium (IV) , Baden-Baden, Jun. 2011, pp. 814819. [186] Y . Liu, P . Lasang, S. Pranata, S. Shen, and W . Zhang, Drive r pose estimation using recurrent lightweight network and virtua l data augmented transfer learning, IEEE T rans. Intell. T ransp. Syst. , vol. 20, no. 10, pp. 38183831, Oct. 2019. [187] J. W ang, H. Zheng, Y . Huang, and X. Ding, V ehicle type r ecog- nition in surveillance images from labeled web-nature data using deep transfer learning, IEEE T rans. Intell. T ransp. Syst. , vol. 19, no. 9, pp. 29132922, Sep. 2018. [188] K. Gopalakrishnan, S.K. Khaitan, A. Choudhary , and A. A grawal, Deep convolutional neural networks with transfer learnin g for computer vision-based data-driven pavement distress dete ction, Constr . Build. Mater . , vol. 157, pp. 322330, Dec. 2017. [189] S. Bansod and A. Nandedkar , T ransfer learning for vide o anomaly detection, J. Intell. Fuzzy Syst. , vol. 36, no. 3, pp. 1967 1975, Mar . 2019. [190] G. Rosario, T . Sonderman, and X. Zhu, Deep transfer lea rning for trafﬁc sign recognition, in Proc. IEEE International Conference on Information Reuse and Integration , Salt Lake City , Jul. 2018, pp. 178185. [191] W . Pan, E.W . Xiang, and Q. Y ang, T ransfer learning in c ollabora- tive ﬁltering with uncertain ratings, in Proc. 26th AAAI Conference on Artiﬁcial Intelligence , T oronto, Jul. 2012, pp. 662668. [192] G. Hu, Y . Zhang, and Q. Y ang, T ransfer meets hybrid: A synthetic approach for cross-domain collaborative ﬁlteri ng with text, in Proc. 28th International Conference on World Wide Web , San Francisco, May 2019, pp. 28222829. [193] W . Pan, E.W . Xiang, N.N. Liu, and Q. Y ang, T ransfer lea rning in collaborative ﬁltering for sparsity reduction, in Proc. 24th AAAI Conference on Artiﬁcial Intelligence , Atlanta, Jul. 2010, pp. 230235. [194] W . Pan and Q. Y ang, T ransfer learning in heterogeneou s col- laborative ﬁltering domains, Artif. Intell. , vol. 197, pp. 3955, Apr . 2013. [195] F . Zhuang, Y . Zhou, F . Zhang, X. Ao, X. Xie, and Q. He, Seq uen- tial transfer learning: Cross-domain novelty seeking trai t mining for recommendation, in Proc. 26th International Conference on World Wide Web Companion , Perth, Apr . 2017, pp. 881882. [196] J. Zheng, F . Zhuang, and C. Shi, Local ensemble across m ultiple sources for collaborative ﬁltering, in Proc. 26th ACM International on Conference on Information and Knowledge Management , Singapore, Nov . 2017, pp. 24312434. [197] F . Zhuang, J. Zheng, J. Chen, X. Zhang, C. Shi, and Q. He, T ransfer collaborative ﬁltering from multiple sources vi a consen- sus regularization, Neural Netw . , vol. 108, pp. 287295, Dec. 2018. [198] J. He, R. Liu, F . Zhuang, F . Lin, C. Niu, and Q. He, A gene ral cross-domain recommendation framework via Bayesian neura l net- work, in Proc. 18th IEEE International Conference on Data Mining , Singapore, Nov . 2018, pp. 10011006. [199] F . Zhu, Y . W ang, C. Chen, G. Liu, M.A. Orgun, and J. Wu, A deep framework for cross-domain and cross-system recommendati ons, in Proc. 27th International Joint Conference on Artiﬁcial Int elligence, Stockholm, Jul. 2018, pp. 37113717. [200] F . Y uan, L. Y ao, and B. Benatallah, DARec: Deep domain adap- tation for cross-domain recommendation via transferring r ating patterns, in Proc. 29th International Joint Conference on Artiﬁcial Intelligence, Macao, Aug. 2019, pp. 42274233. [201] E. Bastug, M. Bennis, and M. Debbah, A transfer learni ng approach for cache-enabled wireless networks, in Proc. 13th Inter- national Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks , Mumbai, May 2015, pp. 161166. [202] R. Li, Z. Zhao, X. Chen, J. Palicot, and H. Zhang, T ACT: A transfer actor-critic learning framework for energy savin g in cel- lular radio access networks, IEEE T rans. Wirel. Commun. , vol. 13, no. 4, pp. 20002011, Apr . 2014. [203] Q. Zhao and D. Grace, T ransfer learning for QoS aware t opology management in energy efﬁcient 5G cognitive radio networks,  in 31 Proc. 1st International Conference on 5G for Ubiquitous Con nectivity, Akaslompolo, Nov . 2014, pp. 152157. [204] B. Guo, J. Li, V .W . Zheng, Z. W ang, and Z. Y u, Citytrans fer: T ransferring inter- and intra-city knowledge for chain sto re site recommendation based on multi-source urban data, in Proc. ACM on Interactive, Mobile, Wearable and Ubiquitous T echnolog ies, Jan. 2018, pp. 123. [205] Y . W ei, Y . Zheng, and Q. Y ang, T ransfer knowledge betw een cities, in Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , San Francisco, Aug. 2016, pp. 19051914. [206] U. Cote-Allard, C.L. Fall, A. Drouin, A. Campeau-Leco urs, C. Gosselin, K. Glette, F . Laviolette, and B. Gosselin, Deep l earn- ing for electromyographic hand gesture signal classiﬁcati on using transfer learning, IEEE T rans. Neural Syst. Rehabil. Eng. , vol. 27, no. 4, pp. 760771, Apr . 2019. [207] C. Ren, D. Dai, K. Huang, and Z. Lai, T ransfer learning of structured representation for face recognition, IEEE T rans. Image Process., vol. 23, no. 12, pp. 54405454, Dec. 2014. [208] J. W ang, Y . Chen, L. Hu, X. Peng, and P .S. Y u, Stratiﬁed tr ans- fer learning for cross-domain activity recognition, in Proc. IEEE International Conference on Pervasive Computing and Commu nications, Athens, Mar . 2018, pp. 110. [209] J. Deng, Z. Zhang, E. Marchi, and B. Schuller , Sparse autoencoder-based feature transfer learning for speech em otion recognition, in Proc. Humaine Association Conference on Affective Computing and Intelligent Interaction , Geneva, Sep. 2013, pp. 511 516. [210] D. Xi, F . Zhuang, G. Zhou, X. Cheng, F . Lin, and Q. He, Do main adaptation with category attention network for deep sentim ent analysis, in Proc. The Web Conference , T aipei, Apr . 2020, pp. 3133 3139. [211] Y . Zhu, D. Xi, B. Song, F . Zhuang, S. Chen, X. Gu, and Q. He, Modeling users behavior sequences with hierarchical exp lainable network for cross-domain fraud detection, in Proc. The Web Con- ference, T aipei, Apr . 2020, pp. 928938. [212] J. T ang, T . Lou, J. Kleinberg, and S. Wu, T ransfer learn ing to infer social ties across heterogeneous networks, ACM T rans. Inf. Syst., vol. 34, no. 2, pp. 143, Apr . 2016. [213] L. Zhang, L. Zhang, D. T ao, and X. Huang, Sparse transfe r man- ifold embedding for hyperspectral target detection, IEEE T rans. Geosci. Remote Sensing , vol. 52, no. 2, pp. 10301043, Feb. 2014. [214] F . Zhuang, K. Duan, T . Guo, Y . Zhu, D. Xi, Z. Qi, and Q. He, T ransfer learning toolkit: Primers and benchmarks, 2 019, arXiv:1911.08967v1. [215] K. Saenko, B. Kulis, M. Fritz, and T . Darrell, Adapting visual category models to new domains, in Proc. 11th European Conference on Computer V ision , Heraklion, Sep. 2010, pp. 213226. [216] Z.C. Lipton, The mythos of model interpretability , ACM Que. , vol. 16, no. 3, May 2018, pp. 127.",
    "page_start": null,
    "page_end": null,
    "word_count": 31499,
    "created_at": "2025-08-18T06:54:58",
    "updated_at": "2025-08-18T06:54:58"
  },
  {
    "id": "7044830903984adbac62d4c84e170234",
    "doc_id": "c5708c3b876f4c3a8324df8bd483c473",
    "doc_name": "Transfer_learning_in_DL_1.pdf",
    "heading": "Document",
    "content": "See discussions, stats, and author profiles for this publication at: https:www.researchgate.netpublication367432163 Transfer Learning in Deep Learning Models for Building Load Forecasting: Case of Limited Data Preprint  January 2023 DOI: 10.48550arXiv.2301.10663 CITATIONS 0 READS 123 4 authors, including: Menna Nawar Alexandria Higher Institute of Engineering and Technology 2 PUBLICATIONS 11 CITATIONS SEE PROFILE Moustafa Shomer Valify Solutions 2 PUBLICATIONS 11 CITATIONS SEE PROFILE All content following this page was uploaded by Moustafa Shomer on 01 February 2023. The user has requested enhancement of the downloaded file. Authors manuscript version accepted for publication. The final published version is copyrighted by IEEE and will be available as: Menna Nawar, Moustafa Shomer, Samy Faddel, Huangjie Gong, Transfer Learning in Deep Learning Models for Building Load Forecasting: Case of Limited Data , in IEEE SoutheastCon, 2023. 2023 IEEE Copyright Notice. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprintingrepublishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Transfer Learning in Deep Learning Models for Building Load Forecasting: Case of Limited Data AbstractPrecise load forecasting in buildings could increase the bill savings potential and facilitate optimized strategies for power generation planning. With the rapid evolution of computer science, data-driven techniques, in particular the Deep Learning models, have become a promising solution for the load forecasting problem. These models have showed accurate forecasting results; however, they need abundance amount of historical data to maintain the performance. Considering the new buildings and buildings with low resolution measuring equipment, it is difficult to get enough historical data from them, leading to poor forecasting performance. In order to adapt Deep Learning models for buildings with limited and scarce data, this paper proposes a Building-to-Building Transfer Learning framework to overcome the problem and enhance the performance of Deep Learning models. The transfer learning approach was applied to a new technique known as Transformer model due to its efficacy in capturing data trends. The performa nce of the algorithm was tested on a large commercial building with limited data. The result showed that the proposed approach improved the forecasting accuracy by 56.8 compared to the case of conventional deep learning where training from scratch is used . The paper also compared the proposed Transformer model to other sequential deep learning models such as Long-short Term Memory (LSTM) and Recurrent Neural Network (RNN). The accuracy of the transformer model outperformed other models by reducing the root mean square error to 0.009, compared to LSTM with 0.011 and RNN with 0.051. Keywords Deep Learning , Transfer Learning , Load Forecasting, Transformer, Sequential Models I. INTRODUCTION As a result of population growth and ongoing technological and economic developments, the number of commercial buildings has increased by 6 from 2012 to 2018 [1]. Newer buildings seem to be bigger in size and have modern electronic devices and more sophisticated equipment, which led to more power consumpti on and energy bills. In 2018, the U.S. has consumed more energy than ever before recording 101.3 quadrillion Btu, up 4 from 2017 [2]. In the following year 2019, U.S. has represented 17 of the energy consumption out of the whole world consumption [3]. Therefore, it is essential for respective parties involved in energy management such as governments and companies to improve the power consumption efficiency. Load-forecasting of commercial buil dings plays a crucial role in increasing energy schedule efficiency, and has become a serious topic that promoted a substantial degree of research. The principal obstacles arise from the fact that there is not a specific factor that controls the energy consumption in commercial buildings, instead there are multiple factors affecting the change of the consumed power. Those factors could affect the forecasting either in a direct way or indirect way, including weather conditions, building size, building type, electronic equipment and human activities, etc. [4]. Since forecasting is a key in improving the energy efficiency issue in commercial buildings, it is necessary to understand different types of load-forecasting. According to the spectrum of time intervals, load forecasting problems are classified into three types: short -term load forecasting (STLF) [5], medium-term load forecasting (MTLF) and long-term load forecasting (LTLF) [6]. This paper focuses on MTLF given the adopted time interval of load forecasting . MTLF has been an active research area which is highly discussed in the literature since it delive rs valuable information for both planning and operation [7]-[10]. MTLF can be assumed from one week to several months and has a goal of making a systematically effective operational plan and scheduling energy for both power generation plants and distribution utilities. In the past decades, it has been observed that improving the accuracy of energy forecasting has become an active issue. It was discussed in many studies using conventional statistical techniques and Artificial Intelligence (AI) base d approaches. Traditional statistical techniques usually involve auto-regressive integrated moving average (ARIMA), linear regression, and exponential smoothing [11], [1 2]. Statistical methods are fast and easy to apply because they rely on linear function s to process the relationships between the historical and forecasted data. As the time-series load forecasting is a non-linear problem, these models are not always forecast ing the load satisfactorily. This issue motivated the development of AI -based models as they are non-linear models which apply non-linear functions to forecast the load demand. Driven by the suitability and efficiency of AI techniques, they have become common in the literature to solve arduous load forecasting problems [1 3], [1 4]. AI -models can be categorized as Machine Learning and Deep Learning models. In terms of Machine Learning models, since these models have advantages such as simplicity and high computation speed, they are highly discussed in the literature [15], [16]. However, Deep Learning models , have proved to be more accurate in energy forecasting tasks [17]. AI-models depend on a large amount of historical data to predict power consumption [18], [19]. In [20], the authors used Deep Learning to predict humidity, air temperatures and energy behavior of buildings. Menna Nawar1, Moustafa Shomer1, Samy Faddel2, and Huangjie Gong2 1Alexandria Higher Institute of Engineering and Technology 2US Research Center, ABB Inc. Corresponding author: samy.faddel-mohamedus.abb.com The algorithm was trained on a large amount of data. Relying on a large-scale data to predict energy consumption was also used in [21], where the authors used a hybrid Deep Learning model that consists of Lon g Short -Term Memory (LSTM) and Recurrent Neural Network (RNN) in LTLF and trained their model on six years of data points from 2004 to 2010. In [22], the authors adapted a developed deep neural network (DNN) for heating energy consumption forecasting. Alth ough, the model had an acceptable performance when it was compared to simulation model from EnergyPlus, the authors did not consider comparing their model to other popular Deep Learning models that are suitable for time-series forecasting. Transformer model is considered one of the newly suggested models in the literature that was mentioned to have a great potential. In [23], the authors trained the Transformer model on 87,648 sampling data point before applying it in STLF , which makes it impractical for new buildings or those that do not collect data at a frequent basis. In [24], the authors explored transfer of knowledge from another domain, then applied this knowledge in two machine learning models (Random Forrest and Feed Forward Networ k). In [25], Transformer model was trained on a very huge-scale of data (19 years of data points) for wind speed forecasting, which clearly proves that Transformer model is considered one of the most data -hungry models. One of the most recent studies [26], The authors modeled Multi - Layer Perceptron (MLP) and Long Short -Term Memory (LSTM) on a medium-sized office building with data scarcity to predict the building thermal dynamics. They applied both traditional Machine Learning and Transfer Learning. The results of deep analysis leveraged using Transfer Learning and lead to higher accuracy. It is noteworthy that none of the previous work considered the use of Transformer model with Transfer learning to tackle the case of limited date. This paper proposes a medium-term load forecasting methodology that can be used for buildings with scarce historical data . The methodology is based on taking the knowledge and information learnt from buildings data in the same domain and transferring it to the targeted building. The paper also applies the framework of knowledge transfer to the Transformer deep learning model that is known for its ability in capturing trends and relations. To the best of our knowledge, this is the first time to apply transfer learning to the Tran sformer model in energy forecasting domain. Finally, the paper compares the proposed methodology to common deep learning techniques such as RNN and LSTM. The paper is organized as follows: Section 2 introduces the transfer learning and background concepts concerning the transfer learning technique. Section 3 presents deep learning models and discusses the specific models used in this paper. Furthermore, Section 4 shows a case study used to evaluate the proposed method and its results . Finally, our conclusion and future work will be discussed in section 5. II. TRANSFER LEARNING TECHNIQUE A. Sequential Deep Learning Models Deep Learning (DL) has become progressively popular in the field of time-series forecasting. The building-unit of any DL model is neural network. The neural network receives inputs, which can be text, image, video, or time -series data like this paper. Then, the input is processed in one or more hidden layer by using weights. These weights can be adjusted during the training process. Finally, the model comes up with the prediction in the output layer. Sequential DL models are the models that use temporal data as inputs. Time-series data are form of sequence data. The main advantage of sequence models is assuming that all inputs and outputs are dependent of each other. Therefore, these models can work efficiently in energy forecasting since the previous inputs in time-series data are inextricably significant in predicting the energy-consumption output. Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) are popular sequential models used in energy forecasting using time -series data as inputs, while Transformer model is rarely used in this domain. Noteworthy, t he three models can capture the patterns and trends, especially the Transformer . The architecture of each model will be discussed in detail in section III, and based on the patterns and trends, the models can predict accurately the energy consumption value. B. Building-to-Building Transfer Learning Transfer Learning (TL) is a promising approach that can help in enhancing the model performance by applying the prior knowledge gained from a so urce task to a similar or different target task. This approach was inspired by the way humans learn new things. Humans have always benefited from their transfer of knowledge and its usage in other different areas. From computer science point of view, the a im of TL is to leverage a pre-trained models knowledge, then applying this knowledge in performing another task. Fig. 1 illustrates the difference between traditional learning in part (a) and Transfer Learning in part (b). It shows that Transfer Learning depends on transferring the knowledge to a pre-trained model. Hence the complexity of the training process will be much easier. Considering the load forecasting problem, it is customary using massive amount of historical data to train an AI -based model to predict the energy consumption. Thus, relying on a large data has become unavoidable, which is impracticable in the case of newly constructed buildings. Unlike TL case, it is available to use scarce historical data to train the DL model as long as it is possible to take advantage of previous knowledge gained from an older task or another source task. Given that the DL models are always data-hungry, the main advantage of TL in these models is that it makes the use of small amount of data for training not only possible, but also efficient. Nevertheless, other advantages of TL are faster training, better model-initializing and higher learning rate for training. From a theoretical perspective of TL, the source (first task) and target tasks (other tasks) could be from the same or different domains. When it comes to load forecasting field, it is more popular in the literature to apply TL between two different domains. This paper demonstrates for the first -time applying building -to-building Transfer Learning in MTLF. The pre-trained models used in this study are initially trained for a load forecasting task for an old building. Then, th e prior knowledge gained from training on previous data, is used to enhance the accuracy of MTLF for new or other commercial buildings with scarce data. Briefly, TL in this paper is mainly used to acquire the knowledge from some old buildings to improve modellingforecasting efficiency on new buildings, with limited data. III. DEEP LEARNING MODELS A. RNN and LSTM Models RNNs have the same general principle of ANNs. While traditional ANNs can only deal with the input data individually and process the relationships between inputs and outputs without linking each output to the preceding. The main difference that RNNs can add ress the sequential data by using internal memory and assume that every output is related to the previous state. Once the output of the RNN is generated, it is copied and returned to the system as an input, hence the name Recurrent of the RNN comes from. Fig. 2 illustrates the structure of RNN, where X and O indicate the input and the output through the time t. Hidden layers are represented by h. U and V denote the weight m atrices between inputs and hidden layers, hidden layers and outputs. W represents t he weight metric between hidden layers in several time stamps. Although RNN is capable of processing sequential data, it fails to extract long -term information after many repeated iterations, because RNN suffers from vanishing gradient problem (this means that the internal memory of RNN cannot keep tracking of the gradient after multiple loops). To overcome this problem, LSTM networks were proposed in [ 27]. LSTMs are considered modified RNNs, however LSTMs are more effective in time -series forecasting for t he long -term dependency than RNNs. The structure of LSTM is similar to RNN, with an addition hidden state, it is also called LSTM cell as shown in Fig. 3 (where c denotes the computations of weights). This cell works as a separate memory for the network, and it is responsible for remembering the long -term information in LSTMs. This cell contains three gates; forget, input and output gate. The forget gate decides wh ich state in the cell can be forgotten, as this information is no longer necessary for the next state prediction. The input gate is responsible for adding or updating the internal cell with the new information. The output gate selects which part should be addressed as an output among all of the information in LSTM cell. The gates in LSTM are represented by the following equations: 𝑖𝑡  𝜎(𝑤𝑖 [ℎ𝑡1, 𝑥𝑡]  𝑏𝑖) (1) 𝑓𝑡  𝜎(𝑤𝑓[ℎ𝑡1, 𝑥𝑡]  𝑏𝑓) (2) 𝑜𝑡  𝜎(𝑤𝑜[ℎ𝑡1, 𝑥𝑡]  𝑏𝑜) (3) (a) (b) (a) (b) Fig. 1 The difference between traditional learning (a) and transfer learning (b). Fig. 2 Fundamental topology of RNN where: 𝑖𝑡 is the input gate, 𝑓𝑡 is the forget gate and 𝑜𝑡 is the output gate of the network. Sigmoid function is represented by 𝜎, 𝑤 denotes the weight for the respective gates and ℎ𝑡1 is the previous output at timestep 𝑡  1 . The current input is represented by 𝑥𝑡 and 𝑏 indicates the biases for the respective gates. Although LSTM has overcome the obstacle of vanishing gradient in RNN, this model created other challenges. Since relying on the LSTM cell including the three gates, the information from previous steps has to go through a long sequence of computations. This problem leads to difficulty in training LSTM due to a very long gradient path. This recursion of the sequence will cause information loss eventually. As mentioned earlier, RNN and LSTM suffer from forgetting the information in the long -term, since both models process the inputs sequentially. Hence, the need for a new model addressing the inputs in parallel rather than sequentially. B. Attention mechanism and Transformer Model The solution to the long -term dependency problem was solved in 2014 and 2015 by [28], [29]. These pioneering papers proposed a technique called the Attention Mechanism. Attention Mechanism is the backbone of the Transformer model, illustrated in [ Fig. 4], Transformers are currently the state -of- the-art solution for many problems such as computer vision and natural language processing. It consists of two parts, encoder and decoder, where each block contains one or more attention layer followed by a multi -layer perceptron (MLP). Using this technique dramatically improved the quality of any sequence - related tasks and used later in the time -series forecasting domain. The attention allows the model to concentrate on only the important and relevant subsets in the long sequences of the input. Concretely, the model has to decide by learning on its own which information from the past steps are relevant for encoding the available input, and then takes the encoded information and decodes it to representative features that is used for forecasting. The core of attention mechanism is assuming that the data source contains elements that can be represented as Key (K) and Value (V). Another element; Query (Q) is associated with the target data, where: KT  R3x5 , V  R5x3 and Q  R4x3. There are three steps to a pply the attention mechanism [27]. First, calculating the similarity score between the key vector and the query vector according to ( 4). Secondly in ( 5), converting the similarity score into weights, then arranging these weights in a probability distribution. Finally, (6) represents calculating the attention value by weighted summation of all the resulted coefficients. The reader is referred to [27] for more information. 𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦(𝑄𝑉, 𝑘𝑖)  𝑄𝑉𝑘𝑖 𝑄𝑉𝑘𝑖 (4) 𝑊𝑖  𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦𝑖 ) 𝑓𝑜𝑟 𝑖  1, 2, 3,  , 𝑛 (5) 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 𝑆𝑐𝑜𝑟𝑒(𝑄, 𝐾, 𝑉)   𝑤𝑖 𝑛 𝑖1  𝑉 (6) In 2017, a new DL model, called Transformer, was proposed based on the attention mechanism [30]. Transformer was rapidly used in many different areas such as translation, image classification and time-series forecasting, and it overcame state- of-the-art models in these areas. In comparison, the main advantage to the Transformers architecture is that it allows the model to access the input data directly using parallel computation, not sequentially as the case of RNN and LSTM. Processing the inputs in parallel avoids the recursion and the iterations, which leads to reduction in the training time and the probability of losing information in the long dependencies. The Transformer does not depend on th e previous hidden states to capture the patterns in order to predict the output. Instead, it processes a batch of input data as a one unit learning positional embeddings to encode the relationships between each observation and search for dependencies and patterns in the time -series data. Positional embedding is a technique that was introduced to replace recurrence by using weights that can encode the information related to a specific position of a certain input , then the transformer decodes the information and transforms it into prediction for the next time - step. IV. CASE STUDY This paper used hourly collected data from two buildings over time interval of one year starting from 1 January 2016 to 1 January 2017. The data were adopted from American Society of Heating , Refrigerating and Air -Conditioning Engineers (ASHRAE) [31]. The proposed models , Transformer, LSTM and RNN, were trained on a subset of that data , representing only 20 of the total samples , which counted up to 336 data samples. The time window used in the experimentations is 6 hours. Fig. 3 Simple architecture of LSTM (a), the internal state or the cell of LSTM and its gates (b). Fig. 4 The Architecture of the Transformer Model Fig. 5 The load consumption over one week for one building including weekend and workdays Fig. 6 The comparison of the three models The dataset contained 14 features, including information about weather conditions like temperature, cloud coverage, precipitation depth and others. It also included buildings metadata like buildings sizes and primary usage. To improve the performance of the model , some engineered features were added like day-of-the-week, seasons, and day-of-the-month, to account for the vacations, holidays and seasonal trends. This paper focuses on educational buildings as they captured most of buildings types by 38. An example of the pattern of the load consumption over one week for one building is shown in Fig. 5 According to the floor count, the number of floors for the two buildings is equal to five floors, representing a large building type Finally, the data used in the study had many features. Some of these features were repre sentative and informative for the modeling process, while others where not relevant to the task or might harm the prediction. After analyzing the data, it was decided to go with the chosen featurism while ditching the rest. An example for the neglected data is the sea level pressure as it is almost constant across all data points. Also, the wind direction as it does not affect the temperature, but it is an indication for the presence of the wind. This would force the model to learn this correlation, instead of learning about the desired task. As well as, engineering the features to give more information to the proposed model, such as the day, month, and hour. It is crucial to determine If the day is a weekend or not, because the usage of electricity highly depends on this feature. The model can also learn to combine information from multiple features together. The results will first show how the transfer learning can improve the accuracy of the deep learning model , enabling forecasting in building with limited data. Then, the paper will compare the proposed transfer transformer model to other sequential models. A. Transfer-Learning Effect on the Transformer model This section aims to see the effect of applying building-to- building Transfer Learning on improving the performance of the Transformer model which were trained on limited data . In theory, transfer learning takes the weights of the pre -trained model and uses them as a starting point to train from, which increases the speed and probability of better convergence to reach high performance . Since it is the first time to apply this type of Transfer Learning to the Transformer model in the domain of energy forecasting and train it on limited data to forecast the load demand, it is essential to evaluate the impact of applying Transfer Learning . Considering the case of large commercial buildings, after adapting building -to-building Transfer Learning the performance of the Transformer model has been improved by 56.8  in terms o f mean squared error (MSE), and almost 34  in terms of root mean square error (RMSE) and mean absolute percentage error (MAPE) as shown in Table. I. B. Forecasting the Load of Large Buildings To evaluate the performance of the proposed model compared to the traditional sequential models such as the RNN and the LSTM, the three models were trained for 15 epochs to prevent models overfitting and make it easier to measure the performance of the models under the same training period. The same large building was used as the source of data for the three models. Also, transfer learning was applied to all of them to ensure apple to apple comparison. The results are presented in Fig. 6 and Table. II. The figure shows that the RNN model has the tendency to overestimate the load for both during heavy and light loading conditions compared to the other two models. The LSTM shows close performance to the transformer model though the transformer is better in following the trend, resulting in a better accuracy as given in Table. II. This result is because the Transformer is built for capturing the patterns, trends, and relations between data points. Regardless of the improvement in the accuracy, the th ree models do not seem to perform well under light loading conditions for the case of weekends as shown after hour 125 in Fig. 6. This will be investigated more in the future. However, typically load forecasting for weekends in commercial buildings is not of high importance since the load is very low and there are not any occupants in the building. TABLE I. IMPACT OF TRANSFER LEARNING IN TRANSFORMERS PERFORMANCE IN CASE OF LARGE BUILDINGS Type of Metric Transformer model Before Transfer Learning After Transfer Learning MSE 0.021 0.009 RMSE 0.146 0.096 MAPE 0.311 0.203 TABLE II. EVALUATION METRICES OF THE THREE MODELS IN LOAD - DEMAND FORECASTING OF LARGE BUILDINGS Metric Transformer LSTM RNN MSE 0.009 0.011 0.051 RMSE 0.096 0.106 0.227 MAPE 0.203 0.217 0.471 V. CONCLUSIONS This paper proposed building-to-building transfer learning for energy load forecasting, which aims for enhancing the performance of the Deep Learning models used currently in the field; by harnessing the knowledge lea rnt from buildings with enough data, and use it to boost the accuracy of predictions for buildings with scarce data. To validate the approach , a case study for a large building was presented, where the data used in our experiments was representing only 2.5 months generated hourly. The paper compared the performance of the mostly used Deep Learning models in time -series forecasting before and after building-to-building transfer learning. This work was the first in utilizing transfer learning and applying it to the Transformer model in the field of energy load forecasting, where the performance gain achieved by the Transformer according to MSE was 56.8 after applying our method. Future work will apply the same methodologies to different building domains and sizes to investigate the different outcomes. More research will focus into finding the limitations of this approach, mainly the least amount of data used with transfer learning to increase the performance. REFERENCES [1] Eia.gov, 2022. [Online]. Available: https:www.eia.govconsumptioncommercialdata2018pdfCBECS_2 018_Building_Characteristics_Flipbook.pdf. [Accessed: 15- Feb- 2022]. [2] Eia.gov, 2022. [Online]. Available: https:www.eia.govtodayinenergydetail.php?id39092. [Accessed: 15- Feb- 2022]. [3] Frequently Asked Questions (FAQs) - U.S. Energy Information Administration (EIA), Eia.gov, 2022. [Online]. Available: https:www.eia.govtoolsfaqsfaq.php?id87t1. [Accessed: 15- Feb- 2022]. [4] Pinzon, P. Vergara, L. da Silva and M. Rider, Optimal Management of Energy Consumption and Comfort for Smart Buildings Operating in a Microgrid, IEEE Transactions on Smart Grid, vol. 10, no. 3, pp. 3236 - 3247, 2019. Available: 10.1109tsg.2018.2822276. [5] T. Yalcinoz and U. Eminoglu, Short -term and medium -term power distribution load forecasting by neural networks, Energy Conversion and Management, vol. 46, no. 9 -10, pp. 1393 -1405, 2005. Available: 10.1016j.enconman.2004.07.005. [6] H. A l-Hamadi and S. Soliman, Long -termmid-term electric load forecasting based on short-term correlation and annual growth, Electric Power Systems Research, vol. 74, no. 3, pp. 353 -361, 2005. Available: 10.1016j.epsr.2004.10.015 [7] M. Ghiassi, D. K. Zimbra, a nd H. Saidane, Medium term system load forecasting with a dynamic artificial neural network model, Electric Power Systems Research, vol. 76, no. 5, pp. 302 316, Mar. 2006, doi: 10.1016j.epsr.2005.06.010. [8] N. Ayub et al., Big Data Analytics for Short and Medium-Term Electricity Load Forecasting Using an AI Techniques Ensembler, Energies, vol. 13, no. 19, Art. no. 19, Jan. 2020, doi: 10.3390en13195193. [9] L. Han, Y. Peng, Y. Li, B. Yong, Q. Zhou, and L. Shu, Enhanced Deep Networks for Short -Term and Mediu m-Term Load Forecasting, IEEE Access, vol. 7 , pp. 40454055, 2019 , doi: 10.1109ACCESS.2018.2888978. [10] L. Han, Y. Peng, Y. Li, B. Yong, Q. Zhou, and L. Shu, Enhanced Deep Networks for Short -Term and Medium -Term Load Forecasting, IEEE Access, vol. 7 , pp. 40454055, 2019 , doi: 10.1109ACCESS.2018.2888978. [11] P. S. Kalekar, Time series Forecasting using Holt -Winters Exponential Smoothing, Kanwal Rekhi School of Information Technology, p. 13, 2004. [12] I. Mpawenimana, A. Pegatoquet, V. Roy, L. Rodriguez, and C. Belleudy, A comparative study of LSTM and ARIMA for energy load prediction with enhanced data preprocessing, in 2020 IEEE Sensors Applications Symposium (SAS), Mar. 2020, pp. 1 6. doi: 10.1109SAS48726.2020.9220021. [13] E. Mocanu, P. H. Nguyen, M. Gibescu, and W. L. Kling, Deep learning for estimating building energy consumption, Sustainable Energy, Grids and Networks, vol. 6, pp. 91 99, Jun. 2016, doi: 10.1016j.segan.2016.02.005. [14] C. Fan, Y. Sun, Y. Zhao, M. Song, and J. Wang, Deep learn ing-based feature engineering methods for improved building energy prediction, Applied Energy, vol. 240, pp. 35 45, Apr. 2019, doi: 10.1016j.apenergy.2019.02.052. [15] M. Hambali, Akinyemi, M. Oladunjoye, and Y. N., Electric Power Load Forecast Using Decision Tree Algorithms, vol. 7, pp. 2942, Jan. 2017. [16] Y. Fu, Z. Li, H. Zhang, and P. Xu, Using Support Vector Machine to Predict Next Day Electricity Load of Public Buildings with Sub-metering Devices, Procedia Engineering, vol. 121, pp. 10161022, Jan. 2015, doi: 10.1016j.proeng.2015.09.097. [17] N. G. Paterakis, E. Mocanu, M. Gibescu, B. Stappers, and W. van Alst, Deep learning versus traditional machine learning methods for aggregated energy demand prediction, in 2017 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe), Sep. 2017, pp. 16. doi: 10.1109ISGTEurope.2017.8260289.. [18] C. Yang, Q. Cheng, P. Lai, J. Liu, and H. Guo, Data -Driven Modeling for Energy Consumption Estimation: Applications, in Green Energy and Technology, 2018, pp. 10571068. doi: 10.1007978-3-319-62575-1_72. [19] A. Gonzalez-Vidal, A. P. Ramallo -Gonzalez, F. Terroso -Saenz, and A. Skarmeta, Data driven modeling for energy consumption prediction in smart buildings, in 2017 IEEE International Conference on Big Data (Big Data), Boston, MA, Dec. 2017, pp. 4562 4569. doi: 10.1109BigData.2017.8258499. [20] V. J. Mawson and B. R. Hughes, Deep learning techniques for energy forecasting and condition monitoring in the manufacturing sector, Energy and Buildings, vol. 217, p. 10996 6, Jun. 2020, doi: 10.1016j.enbuild.2020.109966. [21] R. K. Agrawal, F. Muchahary, and M. M. Tripathi, Long term load forecasting with hourly predictions based on long -short-term-memory networks, in 2018 IEEE Texas Power and Energy Conference (TPEC), Feb. 2018, pp. 16. doi: 10.1109TPEC.2018.8312088. [22] .S. Lee et al., Deep Neural Network Approach for Prediction of Heating Energy Consumption in Old Houses, Energies, vol. 14, no. 1, Art. no. 1, Jan. 2021, doi: 10.3390en14010122. [23] Z. Zhao et al., Short-Term Load Forecasting Based on the Transformer Model, Information, vol. 12, no. 12, Art. no. 12, Dec. 2021, doi: 10.3390info12120516. [24] M. Jain, K. Gupta, A. Sathanur, V. Chandan, and M. M. Halappanavar, Transfer-Learnt Models for Predicting Electr icity Consumption in Buildings with Limited and Sparse Field Data, in 2021 American Control Conference (ACC), May 2021, pp. 2887 2894. doi: 10.23919ACC50511.2021.9483228. Model [25] Huijuan Wu, Keqilao Meng, Daoerji Fan, Zhanqiang Zhang, Qing Liu, Multistep Short -Term Wind Speed Forecasting Using Transformer, Energy, Vol. 276, Dec. 2022, doi: 10.1016j.energy.2022.125231. [26] G. Pinto, R. Messina, H. Li, T. Hong, M. S. Piscitelli, and A. Capozzoli, Sharing is caring: An extensive analysis of parameter -based transfer learning for the prediction of building thermal dynamics, Energy and Buildings, vol. 276, p. 112530, Dec. 2022, doi: 10.1016j.enbuild.2022.112530. [27] S. Hochreiter and J. Schmidhuber, Long Short -Term Memory, Neural Computation, vol. 9, no. 8, pp. 1735 1780, Nov. 1997, doi: 10.1162neco.1997.9.8.1735. [28] D. Bahdanau, K. Cho, and Y. Bengio, Neural Machine Translation by Jointly Learning to Align and Translate, undefined, 2015, Accessed: Mar. 03, 2022. [Online]. Available: https:www.semanticscholar.orgpaperNeural-Machine-Translation-by- Jointly-Learning-to-Bahdanau- Chofa72afa9b2cbc8f0d7b05d52548906610ffbb9c5 [29] T. Luong, H. Pham, and C. D. Manning, Effective Approaches to Attention-based Neural Machine Translation, in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, Sep. 2015, pp. 14121421. doi: 10.18653v1D15-1166. [30] A. Vaswani et al., Attention is All you Need, in Advances in Neural Information Processing Systems, 2017, vol. 30. Accessed: Mar. 01, 2022. [Online]. Available: https:proceedings.neurips.ccpaper2017hash3f5ee243547dee91fbd05 3c1c4a845aa-Abstract.html [31] Ashrae - Great Energy Predictor III, Kaggle. [Online]. Available: https:www.kaggle.comcashrae-energy-prediction. [Accessed: 17-Mar- 2022]. View publication stats",
    "page_start": null,
    "page_end": null,
    "word_count": 5323,
    "created_at": "2025-08-18T06:55:09",
    "updated_at": "2025-08-18T06:55:09"
  },
  {
    "id": "1890996199fd4cf3ac51b92b5e126866",
    "doc_id": "7e94d6ceb61245dba90b3f185f1d0e99",
    "doc_name": "A_comprehensive_survey_of_federated_transfer_learn_1.pdf",
    "heading": "Document",
    "content": "A comprehensive survey of federated transfer learning: challenges, methods and applications Wei GUO1, Fuzhen ZHUANG ()1,2, Xiao ZHANG ()3, Yiqi TONG1, Jin DONG4 1 Institute of Artificial Intelligence, Beihang University, Beijing 100191, China 2 SKLSDE, School of Computer Science, Beihang University, Beijing 100191, China 3 School of Computer Science and Technology, Shandong University, Shandong 266237, China 4 Beijing Academy of Blockchain and Edge Computing, Beijing 100080, China The Author(s) 2024. This article is published with open access at link.springer.com and journal.hep.com.cn Abstract Federated learning (FL) is a novel distributed machine learning paradigm that enables participants to collaboratively train a centralized model with privacy preservation by eliminating the requirement of data sharing. In practice, FL often involves multiple participants and requires the third party to aggregate global information to guide the update of the target participant. Therefore, many FL methods do not work well due to the training and test data of each participant may not be sampled from the same feature space and the same underlying distribution. Meanwhile, the differences in their local devices (system heterogeneity), the continuous influx of online data (incremental data), and labeled data scarcity may further influence the performance of these methods. To solve this problem, federated transfer learning (FTL), which integrates transfer learning (TL) into FL, has attracted the attention of numerous researchers. However, since FL enables a continuous share of knowledge among participants with each communication round while not allowing local data to be accessed by other participants, FTL faces many unique challenges that are not present in TL. In this survey, we focus on categorizing and reviewing the current progress on federated transfer learning, and outlining corresponding solutions and applications. Furthermore, the common setting of FTL scenarios, available datasets, and significant related research are summarized in this survey. Keywords federated transfer learning, federated learning, transfer learning, survey 1 Introduction In recent years, we have witnessed breakthroughs in machine learning, especially deep neural networks (DNNs), in various fields such as computer vision, smart cities, health care, and recommendation systems. Driven by high-quality training data, these methods have achieved impressive performance and even outperformed humans in certain tasks. With the rapid growth of the mobile Internet, a large amount of data is produced by billions of smart devices. However, these collected data cannot be directly uploaded to cloud servers or data centers for centralized processing due to limitations in data security, user privacy protection, and network bandwidth, which poses substantial challenges to the traditional machine learning approach. Such phenomena is commonly known as  isolated data islands. One emerging paradigm for enabling distributed machine learning to solve this problem is federated learning (FL), which was first proposed by [1]. The main idea of FL is to collaboratively train a centralized machine learning model with privacy preservation by transmitting and aggregating model parameters between the distributed participants, which eliminates the requirement of local data sharing and each participant can maintain ownership of their data. However, in certain FL scenarios, the data distribution varies widely between participants. For example, the training data from different participants share the same feature space but may not share the same sample ID space, or the training data from different participants may not even share the same feature space [2]. Therefore, when participants want to utilize global information to improve model utility through FL aggregation, the difference in data distributions, feature space, and label space among participants will influence the model convergence to the optimum [36]. Furthermore, due to inconsistent local storage, computational, and communication capabilities among different participant devices, FL may grapple with system heterogeneity challenges, leading to straggler situations or high error rates. In addition to the above-mentioned data heterogeneity and system heterogeneity problems, FL also suffers from model heterogeneity, incremental data, and labeled data scarcity challenges, which are also focal points of attention among many researchers. To address the aforementioned challenges, transfer learning (TL) is employed in FL as an effective method of facilitating knowledge transfer between source and target domains [7]. The main concept of TL is to minimize the divergence between the distributions of different domains. Similarly, in one communication round of FL, we could consider each Received January 12, 2024; accepted April 21, 2024 E-mail: zhuangfuzhenbuaa.edu.cn; xiaozhangsdu.edu.cn Front. Comput. Sci., 2024, 18(6): 186356 https:doi.org10.1007s11704-024-40065-x REVIEW ARTICLE participant as the target domain and the other participants as the source domains. Given that FL often involves multiple participants, i.e., multiple source domains, and requires the central server to aggregate information (e.g., model parameters) from multiple participants to guide the update of the target participant. In the process of continuous interaction among participants, knowledge is mutually transferred, which allows a local model obtained from a specific domain to be used by other participants through TL, thus alleviating limitations such as data heterogeneity, system heterogeneity, incremental data, and labeled data scarcity. We rethink FL in [ 8] from the perspective of TL, and refer to the combination of FL and TL as federated transfer learning (FTL) shown in Fig. 1. However, in classical TL strategies, the target domain can directly access the source domain data or model information, which contradicts the principle of FL. Hence, these TL strategies could not be directly applied in the FL. Moreover, the standard FL scheme contains a sending and receiving process through the communication between participant and server to ensure that the global model is updated and optimized across all local participants. So in a communication round, local participants can act as source and target domains at different stages. Concretely, during the sending stage, each participant acts as a source domain to transfer local knowledge to other participants. During the receiving stage, each participant serves as the target domain to receive knowledge from others. These conditions increase the difficulty of applying TL to FL. Overall, the above unique challenges of FTL have captured the attention of numerous researchers, and many significant contributions have been made. Existing surveys in the FL field mainly focus on traditional FL [3,911], including horizontal federated learning, vertical federated learning [6], incentive mechanism [12 ], privacy protection [13,14], or introducing FL applications such as healthcare [15,16], mobile edge networks [ 17], and internet of things (IoT) [18]. Despite some studies [19,20] focus on not identically and independently distributed (Non-IID) or other heterogeneous scenarios, such as model heterogeneity, device heterogeneity in FL, there is still a lack of systematic and comprehensive review on the definition, challenges, and corresponding solutions specific for the application of TL in FL, i.e., FTL. To fill this gap, this survey is dedicated to giving a comprehensive survey of FTL, including definitions, a categorization, and a discussion of existing challenges and corresponding solutions, common setting scenarios of distribution heterogeneity, available datasets, as well as an outline of current FTL applications and future prospects. In detail, Fig. 2 shows the categorizations of FTL and corresponding solutions. Section 2.1 demonstrates related definitions of FL and TL, we classify the common settings of FTL scenarios into six categories, including homogeneous FTL, heterogeneous FTL, dynamic heterogeneous FTL, model adaptive FTL, semi-supervised FTL, and unsupervised FTL. Sections 3.1 to 3.5 systematically summarize the corresponding solutions of existing FTL works in these scenarios, including motivation, core algorithm, model design, privacy-preserving mechanism, and communication architecture they adopt. Since some studies have involved multiple FTL scenarios, we only describe the major issues addressed by these studies. Finally, recognizing that systems and infrastructure are critical to the success of FTL, we outline current applications of FTL and propose future prospects. The key contributions of this work are summarized as follows. Fig. 1 The overview of FTL 2 Front. Comput. Sci., 2024, 18(6): 186356 1. This survey is the first to systematically and comprehen- sively rethink FL based on TL (FTL). We provide the definitions of FTL and its challenges including homogeneous FTL, heterogeneous FTL, dynamic heterogeneous FTL, model adaptive FTL, semi- supervised FTL, and unsupervised FTL, and further detail these challenges of FTL through examples. 2. Based on existing FTL solutions, which include both data-based and model- based strategies, we give the current research status for FTL challenges. 3. We summarize the scenario settings of the homogene- ous FTL shown in Section 2.3.1, which is the most common situation in FTL, including the setup methods and applied datasets. Meanwhile, to make checking convenient, we outline the existing research on FTL. 2 Overview In this section, the common notations used in this survey are listed in Table 1 for convenience. Besides, we further introduce the definitions, categorizations, and open challenges related to transfer learning, federated learning, and federated transfer learning. 2.1 Definition Following with previous works [7,8], we first give the definitions of domain, task, transfer learning, and  federated learning that are used in this survey, respectively. Fig. 2 Categorizations of FTL Table 1 The common notations Symbol Definition Symbol Definition n Number of instances m Number of domains k Number of participants k Actual number of participants z Number of classes l Number of model layers p Number of servers s Server u Participant g Global d Threshold f Decision function x Feature vector y Label e Communication round F Device A Active participant B;C Passive participant D Domain T Task X Feature space Y Label space X Instance space I Sample ID space Y Label set corresponding to X S Source domain T Target domain L Labeled instances U Unlabeled instances L Loss function R Relationship matrix M Model E Extractor  Mean  Variance  Importance variable  Tradeoff parameter  Interpolation coefficient Ω Structural risk  Model parameters Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 3 The involved common notations are summarized in Table 1. D X P(X) X X  fxjxi 2 X;i  1;:::; ng D D  fX; P(X)g X P(X) Definition 1 (Domain) A domain is constituted by two elements: a feature space and an probability distribution , where the symbol represents an instance set, i.e., . Thus, a domain can be denoted as . In general, if two domains are different, then they may have different feature spaces or different probability distributions [21]. T Y f T  fY; f g f y 2 Y f Definition 2 (Task) A task is constituted by a label space and a decision function , denoted as . Given the training data, the decision function is used to predict the corresponding label , where is not explicit but can be inferred from the sample data. mS 2 N f(DS i ;TS i )ji  1; :::; mS g mT 2 N f(DTj ;TT j )jj  1; :::; mT g f T j ( j  1; :::; mT ) Definition 3 (Transfer Learning) Given ansome observa- tion(s) corresponding to source domain(s) and task(s) (i.e., ), and ansome observation(s) about target domain(s) and task(s) (i.e., ), transfer learning aims to utilize the knowledge implied in the source domain(s) to improve the performance of the learned decision functions on the target domain(s) [7]. k u1; :::; uk X1; :::; Xk X  X1 [ [ Xk MS UM MF ED Xi Definition 4 (Federated Learning) Assume there are participants , each aiming to train a machine learning model with their own private datasets . A conventional approach is to upload all data together and use to train a global model . However, in many application scenarios, participants cannot directly upload their own data or access the data of other participants. Therefore, a typically federated learning system is a distributed learning process in which the participant collaboratively train a model without sharing local private data [8]. i j i  1; :::; k Xi , Xj Yi , Yj Pi(Xi;Yi) , Pj(Xi;Yj) X X Y Fi , F j Xi e1 , Xi e  i Xi  XL ! 0 k u1; :::;uk fspjp  1; :::;Ng E Definition 5 (Federated Transfer Learning) Given there are some challenges in FL for participants and ( ), including data heterogeneity ( or or , where instance space consists of feature space and label space ), system heterogeneity ( ), incremental data ( ), and scarcity of labeled data ( ), the FL combines the TL to solve these challenges, called FTL. The specific definition of FTL as follows. Given participants in FL, the central server set is designed to achieve model convergence over the communication rounds. During each communication round, the typical federated transfer learning process includes two distinct stages: ui 1  i  k DS i 1  i  mS DS i TS i DS i TS i sp X1; :::; Xk 1. Sending stage: participant(s) ( ) is(are) assumed as the role of the source domain(s) ( ), where they are responsible for contributing local ansome observation(s) ( , ) corresponding to and task(s) to the central server without sharing local raw data . The server then leverages the collected sending information to implement the aggregation process. uj 1  j  k f(DTj ;TT j )jj  1; :::;mT g 2. Receiving stage: once the aggregation is complete, participant(s) ( ) then are assumed as the role of the target domain(s) and utilize the received global aggregation information to perform local model updates. E p  0 The above sending and receiving stages are assumed to repeat for communication rounds, or until the model is observed to converge. Particularly, when , the above process is considered as a decentralized federated transfer learning process. 2.2 Category of federated learning According to the characteristics of data distribution among connected participants, FL can be categorized into horizontal FL (HFL) and vertical FL (VFL). Generally, HFL considers the distributed participants to have data with the same features but are different in sample space, while VFL considers the distributed participants to have the same samples but different features to jointly train a global model [6,19]. Federated transfer learning in [8] refers that these participants have differences in both feature space and label space. Due to the limited research on federated transfer learning in [8], this survey categorizes federated transfer learning and VFL as a type of VFL for description. On the other hand, depending on whether there is asome central server(s) responsible for coordinating participants, FL can also be divided into centralized FL (CFL) and decentralized FL (DFL), where CFL assumes that there is asome server(s) to gather local model-related information or other training information from the participants and then distributes the updated global model back to the participants, while DFL assumes participants directly aggregate information from neighboring participants [8]. In the following, we will provide a brief introduction to these FL frameworks and discuss the various settings of source domains, target domains, and tasks when employing transfer learning within these frameworks. 2.2.1 Horizontal federated learning X I i j Xi  Xj Ii  Ij S T HFL is commonly found in scenarios where participants share the same feature space but different sample space , which meets homogeneity FTL described in Section 2.3.1. For example, the medical record data of two regional hospitals and may be very similar due to they use the same information system, which both record the patients name, age, gender, and other user private data, so their feature spaces are the same ( ). However, the two hospitals have different user groups ( ) from their respective regions, and the user intersection of their local datasets is very limited. In FTL, any participant can serve as a source domain ( ) to provide knowledge or as a target domain ( ) to receive knowledge from other participants in the same feature space, therefore, we define HFL in homogeneous FTL as: XS i  XT j ;YS i  YT j ; IS i , IT j ;8XS i ; XT j orXT i  XS j ;YT i  YS j ; IT i , IS j ;8XT i ; XS j ;i , j: From an extended perspective, HFL meets heterogeneous FTL 4 Front. Comput. Sci., 2024, 18(6): 186356 when participants label space is inconsistent in the knowledge-transferring process, the HFL in heterogeneous FTL can be represented as: XS i  XT j ;YS i , YT j ; IS i , IT j ;8XS i ; XT j orXT i  XS j ;YT i , YS j ; IT i , IS j ;8XT i ; XS j ;i , j: 2.2.2 Vertical federated learning X I I i j i Xi Xj Xi , Xj Unlike HFL where all participants have their own local data labels, in the VFL scenario, participants feature spaces are inconsistent, and their sample spaces may also not be entirely the same. For example, suppose there is a high degree of overlap in the customer groups between a bank and a telecommunications company in the same region. The bank has information on users credit history ( ), such as loan repayment details and credit card usage, while the telecommunications company holds data on users call logs, data usage, and payment records ( ), where the feature space is different ( ). These two entities, which all act both as source domains or as target domains, can engage in VFL to mutually enhance their services in different feature spaces. We summarize VFL in heterogeneous FTL as: XS i , XT j ;YS i  (,)YT j ; IS i  (,)IT j ;8XS i ; XT j orXT i , XS j ;YT i  (,)YS j ; IT i  (,)IS j ;8XT i ; XS j ;i , j: 2.2.3 Centralized federated learning Standard CFL requires one or more central servers to build a global model by collecting local information from distributed participants [22], which involves three fundamental steps as described below: 1. Receiving stage: each participant receives the initial model sent by the server. ui Xi 2. Sending stage: participants use their own private data to train the local model (add local model notion), and then send the local model to the server. Mg 3. Receive stage: The central server updates the global model by collecting and aggregating all the local updates and then sends the updated global model back to the participants. In the FTL setting, during the sending and receiving stages, participants share knowledge through a central aggregation strategy, where each participant can act as a source domain providing knowledge or a target domain receiving knowledge. For instance, during the sending stage, participants act as source domains providing model parameters, while the server acts as the target domain, aggregating these parameters to form a global model. Conversely, in the receiving stage, the server serves as the source domain providing global model parameters to each participant. 2.2.4 Decentralized federated learning fu1; :::; ukg Compared with CFL, DFL is conducted over different participants without a central parameter server for global model aggregation. Each participant uses a private local dataset to optimize their local model after receiving model updates from other participants. This process involves two fundamental steps as described below: ui Mi Xi Mi fu1; :::; uk1g 1. Receiving stage: participant federally train its initial model locally with its own dataset , and then send the model to other participants without direct data exposure. ui Mg fM1; :::; Mkg 2. Sending stage: participant obtain the aggregated model by aggregating the received local model , and then update local model with aggregated model. Similar to CFL, each participant in DFL could still serve as either a source domain or a target domain without a central server during different stages of the FL process. 2.3 Federated transfer learning Transfer learning has achieved remarkable success by enabling the application of knowledge from one domain to improve performance in another, significantly reducing the need for extensive data collection and training time in new tasks [2326]. However, as shown in Fig. 3, constrained by the unique distributed learning paradigm of FL, current FTL studies face many additional challenging situations, including homogeneous FTL, heterogeneous FTL, dynamic heterogeneity FTL, model adaptive FTL, semi-supervised FTL and unsupervised FTL. More descriptions are presented below. 2.3.1 Homogeneous federated transfer learning i j Di Dj Di , Dj Xi , Xj Pi(X) Pj(Y) i j Ti , Tj Yi , Yj Pi(yjx) , Pj(yjx) Assume that the local data of participant and participant constitute a source domain and a target domain , respectively. represents a difference in either their feature spaces ( ) or marginal distributions ( or ). Similarly, if the task between participant and participant is not same, that is , then there is a difference in their label spaces ( ) or in their conditional distribution ( ). Pi(X) Pj(Y) Pi(yjx) , Pj(yjx) ni , nj HOFTL refers to differences in marginal distributions ( or ), conditional distributions ( ), or sample sizes between participant data, which is often caused by diversity in domain or task between participants. Based on this, HOFTL includes five scenarios: Pi(Y) , Pj(Y)  Prior shift: Pi(X) , Pj(X)  Covariate shift: Pi(xjy) , Pj(xjy)  Feature concept shift: Pi(yjx) , Pj(yjx)  Label concept shift: ni , nj  Quantity shift: Specifically, as described in Subsection 2.2.1, horizontal federated learning assumes that participants have the same feature space, so HOFTL is a form of transfer learning under this HFL assumption. Moreover, HOFTL can also be presented in vertical federated learning when there is partial overlap in the feature space between participants. Unless specifically stated, the HOFTL methods discussed in this survey are all related to HFL. Overall, compared with homogeneous transfer in traditional transfer learning that only considers marginal and conditional probability distributions, Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 5 HOFTL also considers changes in the total sample size, which corresponds to the Non-IID data setting in federated learning. The detailed challenges of HOFTL are described below, and considering that homogeneous FTL is one of the most frequently discussed, we have summarized the specific settings for each homogenous FTL scenario demonstrated in Table 2.  Prior shift P(Y) P(yjx) Prior shift, also known as class imbalance, implies that the prior probability distribution could be inconsistent between different participants when the conditional probability distribution is consistent [182]. In FL, the prior probability distribution inconsistency may occur when different participants have different class distributions in their local datasets. If these differences are not properly handled, they can lead to a federated model that performs unfair and suboptimal performance. For example, an FL system is designed to improve predictions for a specific disease (e.g., diabetes) across different hospitals that participate in model training without sharing private patient records. Hospital A is located in an urban area with a high prevalence of diabetes, possibly due to lifestyle factors prevalent in the population it serves. As a result, in hospital As patient data, 30 of patients might have diabetes. On the other hand, hospital B serves a rural area with a different demographic and lifestyle, resulting in only 10 of its patients having diabetes. This difference in the prevalence of diabetes is a classic example of prior probability shift. In some extreme cases, hospital B may even have no positive cases for this disease.  Covariate shift P(X) P(yjx) Covariate shift, or feature distribution imbalance, describes a situation where the input feature distribution is varied between participants while the conditional probability remains consistent. This presents a unique challenge in FL because the global model is trained on data from multiple participants, and each participants local data may represent a different underlying distribution of input features. For example, the patient population at hospital A had a higher average body mass index (BMI), which is a known risk factor for diabetes, while the patient population at hospital B had a lower average BMI. There is a significant difference in the input data (in this case, the BMI distribution) between the two hospitals, known as covariate shift.  Feature concept shift x y P(xjy) P(y) x y Concept drift, which includes feature concept shift and label concept shift, refers to the change in the relationship between variables and , where feature concept shift implies to discrepancy among participants with the same prior distribution [182]. This type of shift can be particularly challenging in federated learning because models need to generalize across all participants data. For example, consider two hospitals A and B jointly predicting the incidence of diabetes, where   represents the patients health characteristics and   represents the presence or absence of diabetes. Hospital As diabetic population mostly has a higher socioeconomic status, resulting in a different set of health characteristics, such as better control of blood sugar levels and fewer complications. In contrast, patients with diabetes at Fig. 3 The challenges of FTL 6 Front. Comput. Sci., 2024, 18(6): 186356 x y P(xjy) hospital B may have lower socioeconomic status and poorer health characteristics, such as uncontrolled blood sugar levels and higher rates of complications. Differences in the distribution of health characteristics ( ) for a given diabetic patient ( ) are an example of a shift.  Label concept shift P(yjx) P(x) P(yjx) A A ui A Similar to feature concept shift, the label concept drift refers to inconsistent among participants with the same covariate distribution [182]. Some external events or changes may lead to changes of in either the sourcetarget domain, which further renders the models from the sourcetarget domain no longer suitable for tasks in the targetsource domain. For example, in a federated recommendation system, geographical location is commonly used as the input feature to predict users favorite items. Thus, if the emergence of tendentious policy or new pillar industries supports the economic development of area , the consumption level of will be improved. In this situation, the expected user preference will change, causing the prediction results of participant from to become invalid and unsuitable for the improvement of model predictions from other regions participants.  Quantity shift Different from the prior shift, quantity shift refers to the situation where there is a significant imbalance in the number of training samples available among participants. In FL, some participants might have a large dataset, while others may have a relatively small one. This can lead to a situation where the global model is disproportionately influenced by participants with more data, potentially leading to biases or overfitting to the characteristics of those datasets. For example, a large-scale hospital may have thousands of patient records, while a small clinic may only have a few hundred. This difference in data volume is a classic example of quantity shift in FL. In summary, with uniform feature and label spaces, participants in homogeneous FTL still face data distribution shift problems, including prior shift, covariate shift, feature concept shift, label concept shift, and quantity shift. Most current FTL studies focus on prior and quantity shifts, with few studies tackling covariate shifts. Feature concept shift and label concept shift are even less explored. However, external elements change, like time or policy, may change the Table 2 Data heterogeneity settings of HOFTL and HEFTL Problem categorization Setting Reference Dataset HOFTL Prior shift Fixed ratio [2733] CIFAR-101), CIFAR-1001), MNIST2), Tiny-Imagenet3), ImageNet3), FEMNIST4), OFFICE [34], DIGIT [35], OpenImage [36], WESAD [37], KDD995), SVHN6), HAR7), OFFICE-Caltech 108), MIMIC-III9), Shakespeare [1], DomainNet10), NSL-KDD9911), CINIC10 [38], CelebA [39], StackOverflow [40] Natural partition [4157] 1 classparticipant [37,5860]  1 classesparticipant [ 1,48,59,61117] Dirichlet Distribution [118121,121133], [32,81,93,112,134146] JensenShannon divergence [147] Half-normal distribution [47,148] Log-normal distribution [79] Covariate shift 1 domainparticipant [41,65,125,149151], [1,70,76,129,152156], [89,91,96,112,157166] Mixed domainparticipant [167,168] Feature concept shift 1 degreeparticipant [169] Label concept shift [89,163] Quantity shift Natural [52,54,89,98,159,170,171], [109,114,161,172] By data source [1,61,125,157] By parameter [88] HEFTL Feature space hetergeneity Overlapped feature [ 37,87,133,173175] CIFAR-101), CIFAR-1001), MNIST2), MovieLens [176], ModelNet [177], FEMNIST4) NUS-WIDE [176] Non-overlapped feature [176,178181] Label space heterogeneity Feature and label space heterogeneity 1) See cs.toronto.edukrizcifar.html website. 2) See kaggle.comdatasetshojjatkmnist-dataset website. 3) See kaggle.comctiny-imagenet website. 4) See github.comwenzhu23333Federated-Learning website. 5) See kdd.ics.uci.edudatabaseskddcup99 website. 6) See ufldl.stanford.eduhousenumbers website. 7) See github.comxmouyangFL-Datasets-for-HAR website. 8) See v7labs.comopen-datasetsoffice-caltech-10 website. 9) See physionet.orgcontentmimiciii-demo1.4 website. 10) See ai.bu.eduM3SDA website. 11) See s.uci.edudataset227nomao website. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 7 relationship between features and labels for partial participants while leaving others unchanged. This intensifies the feature concept drift and label concept drift among participants, which is worth deeper study in the future. 2.3.2 Heterogeneous federated transfer learning i j Di Dj Di , Dj Ti , Tj Xi , Xj Yi , Yj HEFTL mainly refers to the problem of inconsistency in feature or label space between participants in FL. To the specific, similar to HOFTL, it is assumed that there are two participants and , whose private data constitute the source domain and the target domain , respectively. HEFTL demonstrates the differences in either their domain ( ) or task ( ), which caused by their various feature spaces ( ) orand label space ( ). Based on this, HEFTL has three scenarios: Xi , Xj  Feature space heterogeneity: Yi , Yj  Label space heterogeneity: Xi , Xj Yi , Yj  Feature and label space heterogeneity: and Unlike HOFTL, where all participants train models with the same data structure, HEFTL allows for collaboration between datasets that are not identically structured. Thus, vertical federated learning is a prime case for HEFTL, since the local private data among participants in VFL may contain different sets of attributes or dimensions. Next, we give a detailed description of the settings as mentioned above.  Feature space heterogeneity X Y A B Feature space heterogeneity refers to the situation that the feature space of different participants is inconsistent, while the label space is consistent, particularly when different datasets involved in the training process have different sets of features. For example, in FL, two retailers are trying to identify fake reviews by local model prediction. Retailer has a feature space that includes review length, the number of purchases, and purchase history, whereas retailer utilizes review timing, user location, and account age as feature space. They all annotated their reviews with binary labels as true (0) or fake (1). Although the reviews obtained by different retailers have inconsistent feature space, these retailers still aim to leverage FL to enhance the predictive performance of their respective local models within a consistent label space.  Label space heterogeneity X Y A C Label space heterogeneity refers to the situation where different participants have consistent feature space but inconsistent label space , which is the exact opposite of feature space heterogeneity. For example, in FL, two international e-commerce platforms are aiming to improve their recommendation systems. Each platform operates in a different region and thus has different product categories that are relevant to their local markets. Platform serves the Asian market and uses categories like Apparel, Gadgets, Furniture, and Anime Merchandise. Platform is based in North America, and uses labels such as Clothing, Tech, Home Improvement, and Sports Equipment. All two platforms collect user data including browsing time, click- through rates, purchase history, and search queries, which make up their consistent feature space. However, the way they categorize their products (labels) varies due to regional differences in terminology and market demand, leading to an inconsistent label space.  Feature and label space heterogeneity X Y A B Feature and label space heterogeneity, indicates the feature space and label space are both inconsistent among different participants. For example, there are two different specialty health clinics using federated learning to predict if patients will need to return for more treatment. Each clinic has its own set of measurements and outcomes. Clinic focuses on heart health, measuring things like heartbeat patterns and blood tests, and is concerned with whether patients might come back with heart issues. Clinic is a general clinic in a remote area, tracking health indicators like blood pressure and weight, and wants to predict if patients will return for any follow-up care or need a specialist. Each clinic collects different health information (different feature spaces) and has different categories for what counts as a patient needing to return (different label spaces). Considering the health indicators may be helpful in predicting patients heart issues in the future. Thus, they want to use FL to build better prediction models without sharing sensitive patient data. In summary, heterogeneous FTL may occur when there is inconsistency in the participants feature spaces or label spaces. The existing research primarily focuses on FTL with heterogeneous feature spaces where only the feature spaces are inconsistent. Other heterogeneous situations in FTL remain worthy of deeper investigation. 2.4 Dynamic heterogeneous FTL DHFTL refers to the condition where the participant set that contributes to the FTL aggregation or the local raw data of partial participants in this set is dynamically changing at each round. We further provide detailed descriptions of the causes of dynamic heterogeneity. 2.4.1 System heterogeneity F rg e1 u1; :::;un e m(0  m  k;n , m) e rg e e rg e1 Each participants local device in FL could have different storage, computation, and communication abilities. Due to the varying storage or computational capabilities, some devices may not be able to complete the local training in time before aggregation. Meanwhile, the communication ability among participants is also influenced by network connections, and some devices may lose connection during a communication round because of connectivity or power issues [70,183,184]. These aspects greatly amplify the straggler issue in the aggregation process [185], forming a dynamically changing set of participants during FL iterations as shown in Fig. 4. Assuming that there is a global optimal direction for the global model aggregated by participants in communication round , and participants could send their local model to server in time due to devices limitation in communication round , the global optimal direction aggregated by participants in round may have a significant difference with when there is data heterogeneity among participants, which is not conducive to 8 Front. Comput. Sci., 2024, 18(6): 186356 u1; :::; uk Fi , F j;i; j 2 (1;k) the global model convergence. Therefore, how to transfer the knowledge among participants within a dynamically changing participant set is a key challenge for DHFTL. Dynamic heterogeneous FTL for participants caused by system heterogeneity ( ) can be expressed as: DS 1 ; :::; DS n  Se1 , Se  DS 1 ;:::; DS m; n m e 1 e Se e where and represent the actual number of participants in communication round and of FTL, respectively. indicates the set of actual participants in the communication round . 2.4.2 Incremental data u1; :::; uk Xi e1 , Xi e Real-world FL applications are often dynamic, where local participants receive the new data, classes or tasks in an online manner [102,186,187], which proposes a key challenge is how to execute FTL from dynamically changing data distributions [ 188,189]. If only some participants are constantly adding data, or even if each participant synchronously adds new data, the newly added data could disrupt the original local data distribution, potentially exacerbating the differences between participant distributions as represented in Fig. 5. This requires the model to generalize well across both the old and new domains [190,191]. In addition, its also possible that the feature space of the newly added data is inconsistent with the original feature space. Dynamic heterogeneous FTL for participants caused by incremental data ( fX;Y gi e1 , fX;Ygi e;i 2 (1;k) e or ) in communication round . Dynamic heterogeneous FTL in a participant can be written as: P(XS i;e1) , P(XS i;e) or fX;Y gS i;e1 , fX;YgS i;e: Another situation where dynamic heterogeneous FTL of multiple participants can be denoted as: P(XS i;e) , P(XS j;e) or fX;Y gS i;e , fX;YgS j;e:(i; j 2 (1;k)): Nevertheless, we can only observe popularity in typical incremental learning approach [188,189,191,192], while these problems in incremental FTL receive relatively less attention. 2.4.3 Model adaptive FTL M In practical scenarios, due to differences in training objectives, participants may employ different model architectures for training [164,193]. Therefore, employing conventional aggregation methods for heterogeneous models output representations or parameters, such as averaging participants parameters in FedAvg [1], cannot effectively complete knowledge transfer between participants in FL. Additionally, even if the dimensions of the intermediate feature outputs are consistent across different models, the representational capacity of these features for local data could still vary, hindering performance improvement of the model in the target participant [66,164,193,194]. TL generally assumes that the model architectures in the source domain and the target domain are consistent, however, the inconsistency of models Fig. 4 The detailed description of system heterogeneity in FTL. In each round of global communication, due to the resource heterogeneity among the participants, partial participants could not participate in the global aggregation in time, which results in the actual optimization direction of the aggregated model dynamically changing and deviating from the global optimal optimization direction Fig. 5 The detailed description of incremental data in FTL. In each round of global communication, due to the increase in the local user data or class, the local data distribution of participants may change, causing the actual optimization direction of the aggregated model to constantly vary and deviate from the global optimal optimization direction Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 9 u1; :::; uk Mi , Mj;i; j 2 (1;k) in FL poses a new challenge for the performance of target participants tasks by aggregating process. This challenge, implementing effectively federated training in a model heterogeneous setting, is referred to as model adaptive FTL, which can be denoted as for participants : 2.4.4 Semi-supervised and unsupervised FTL u1; :::; uk  i Xi  XL ! 0;i 2 (1;k) Real-world FL applications especially need to use unlabeled data more than others [195,196]. On the one hand, in cross- device federated learning [182], individual devices create a lot of unlabeled data, like photos, texts, and health record data from wearables. Its unrealistic to label all this data due to its large volume. On the other hand, cross-silo FL [182] involves businesses, where data labeling often needs special knowledge. This is common in finance and healthcare sectors. Labeling this data would be time-consuming and expensive. Thus, SSFTL and USFTL have caught the interest of some researchers [122,123,195]. Overall, SSFTL has two common scenarios: ① only one participant has labeled data; ② several participants each have a small amount of labeled data locally, where case ③ is often seen in VFL, where it s usually assumed that only one active party has data label information. USFTL in FTL refers to the scenario where all participants lack labeled information. The labeled data scarcity in FTL for participants is denoted as: . 3 Methodology We elaborate on the current research strategies for each FTL challenge mentioned in Section 2.3. As shown in Fig. 6, it mainly includes two mainstreams: data-based and model- based strategies. Specifically, data-based strategies emphasize knowledge transfer by modulating and transforming participants data for space adaptation, distribution adaptation, and data attribute preservation or adjustment [7 ] without exposing any raw private data. The model-based strategies aim to improve the predictive accuracy of any given participant by the models from other participants in FTL. Tables 35 demonstrate related works on solving FTL challenges through these strategies. Note that currently there are very few FL works that specifically address the issues of label space heterogeneity or label  feature space heterogeneity. Therefore, we will not discuss them in a separate subsection. 3.1 Homogeneous federated transfer learning Homogeneous FTL and heterogeneous FTL are two of the most studied challenges in FTL. We will first illustrate the strategies for addressing homogeneous FTL challenges from both data-based and model-based perspectives as shown in Fig. 6. 3.1.1 Prior shift This subsection describes solutions to the prior shift challenge in HOFTL, which is one of the most common issue in FTL.  Instance augmentation Instance augmentation in FTL aims to enhance data homogeneity of various participants through techniques like oversampling [27,28,208 210] and undersampling [211213], which mainly occurs on the side of the participants. In detail, FedHome [28], Astraea [47], and FEDMIX [61] consider resolving prior probability bias at the local participant level. However, they ignore the effectiveness of global information in the local augmentation process. Therefore, some studies [ 118,119] suggest bridging the gap between participant and global distribution by creating a public dataset, but this also increases the risk of privacy leakage. To mitigate this issue, Faug [ 29], an FTL approach based on the generative adversarial network (GAN), is proposed to avoid privacy issues from multiple data transfers. Faug trains a GAN on minority class data at a central location, then sends it back to participants for data generation, helping build independent and identically distributed (IID) datasets. However, the construction of the generator increases extra computational and communication costs. the study [62] introduces a batch normalization (BN) based data augmentation approach, involving the following steps: t ith i 2 1; :::; l 1. BN layer parameterization: In the round of global iteration, the BN layer ( ) of the global Fig. 6 Data-based and model-based strategies of FTL 10 Front. Comput. Sci., 2024, 18(6): 186356 Mg (i 1)th i i y( j)(1  j  z) z x(z) M( x( j)) 1  j  z ai i; i model from the round can be parameterized as a distribution of means and variances . For a given target category , where represents the total number of possible categories, participants will sample from the Gaussian distribution to generate samples by forward propagate ( ), meanwhile the intermediate activation values produced in sample process are applied to obtain the BN statistics ). x( j)  ar gminx l i1 i i2 2  i i2 2 LH (M ( x( j)); y( j)) H 2. Augmented Data Update: The computation of the loss function follows the formula: , where represents the cross-entropy loss. During the M x( j) backpropagation process, the parameters of the model are kept fixed, and only is updated to obtain augmented data that is closer to the real distribution.  Instance selection Instance selection aims to select a subset of the available data that is most representative or informative for the training process. In distributed learning, numerous studies [214220] suggest that constructing local training samples that are closer to the target distribution through instance selection methods can effectively enhance model performance. However, these methods are designed for traditional distributed training where training data is public. They are not suitable for FL, which uses private datasets from different owners. To solve this Table 3 FTL frameworks Reference FTL challenges Architecture StrategyDHFTL MAFTL SSFTL USFTL HOFTL HEFTL HFL VFL CFL DFLSystem Incremental PS CS FCS LCS QS FSH LSH FLSH [27,28,62]    IA [47]    IA,MS,FC [120]     IA [61]     IA [29]    IA, KD [121]     IA, FC [30,58]    IS [197]     IS [198]     IS [123]     IS [149]    FA [176]      FA [172]     FA [125]      FA [126]    FM [199]     FM [200]    FM [173]     FC [167]    FC,CR [37,175]    FS [174,178]    FS [49]    FS [181]     FS [179]     FS [65]     FC,MC [48]     FS [180]     FS,MS [150]     FAI [103]     FAI [201]     FAI [66]      FAI,MC [41]     CR [151]    CR [102]     CR,MS [50]     CR,PD [72]    CR,PR [67]     DCR,KD [128]    DCR [153]    DCR 1 Abbreviation: PS: prior shift; CS: covariate shift; FCS: feature concept shift; LCS: label concept shift; QS: quantity shift; FSH: feature space heterogeneity; LSH: label space heterogeneity; FLSH: feature and label space heterogeneity; HFL: horizontal FL; VFL: vertical federated learning; CFL: centralized FL; DFL: decentralized FL; IA: instance augmentation; IS: instance selection; FA: feature augmentation; FM: feature mapping; FS: feature selection; FC: feature clustering; FAI: feature alignment; CR: consistency regularization; DCR: domain-dependent consistency regularization; PS: parameter sharing; PR: parameter restriction; PD: parameter decoupling; PP: parameter pruning; MW: model weighting; MS: model selection; MC: model clustering; MI: model interpolation; KD: knowledge distillation. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 11 problem, the study [58] suggests using a benchmark model that is trained on a targeted small dataset before FL starts to evaluate the relevance of each participants data, where only highly relevant data is used for local training. However, this method does not consider the influence of other participants on the benchmark model during FL training. Accordingly, the study [197] finds that calculating the sample loss during FL training can reflect the samples homogeneity with the global data distribution. Among these, local samples with stronger homogeneity are more conducive to improving the global models utility. As a result, this study proposes an FL framework named FedBalancer, which employs a selection strategy based on sample loss to filter local samples, aiming to build a local training set that is better aligned with the global sample distribution. However, FedBalancer increases computational costs because it requires calculating the loss value for every individual local sample. The study [30] introduces a less computationally expensive method for selecting samples. This method uses the gradient upper bound norms of samples to assess their importance to global model performance. It calculates gradients from the loss of the last layers pre-activation output, rather than calculating the gradient from the overall model parameters. This usually requires just one forward pass to accurately estimate a samples importance. Specifically, the proposed algorithm includes the following two steps: 1. Participant selection: using the private set intersection (PSI) protocol, each participant is informed about the target categories relevant to the target task. Participants Table 4 FTL frameworks (continued) Reference FTL challenges Architecture StrategyDHFTL MAFTL SSFTL USFTL HOFTL HEFTL HFL VFL CFL DFLSystem Incremental PS CS FCS LCS QS FSH LSH FLSH [69]    PS [154]    PS [68,117]    PS,PD [202]      PR [75,77,78,130,131]    PR [74,76]     PR [73]    PR,MI [71]     PR,MI [63,79,80,83]    PD [155]    PD [81]     PD [84]     PD [82]     PD [156]     PD [110]     PD,MI [106]     PD,MC [133]        PD,KD [132]    PD,PP,MS [129]     PD,PR,MI [87]       PP [85]     PP [86]     PP,MC [134]     PP,KD [1]      MW [157]      MW [88]     MW [158]    MW [203]    MW [204]    MW [90]     MW [135]    MW,CR [89]       MW,CR [152]     MW,PD [122]     MW,MC [91]     MS [60,92,93,136,148]    MS [51]     MS [52]     MS 1 Abbreviation: PS: prior shift; CS: covariate shift; FCS: feature concept shift; LCS: label concept shift; QS: quantity shift; FSH: feature space heterogeneity; LSH: label space heterogeneity; FLSH: feature and label space heterogeneity; HFL: horizontal FL; VFL: vertical federated learning; CFL: centralized FL; DFL: decentralized FL; IA: instance augmentation; IS: instance selection; FA: feature augmentation; FM: feature mapping; FS: feature selection; FC: feature clustering; FAI: feature alignment; CR: consistency regularization; DCR: domain-dependent consistency regularization; PS: parameter sharing; PR: parameter restriction; PD: parameter decoupling; PP: parameter pruning; MW: model weighting; MS: model selection; MC: model clustering; MI: model interpolation; KD: knowledge distillation. 12 Front. Comput. Sci., 2024, 18(6): 186356 d with low relevance to the target task are filtered out. Reversely, if their total number of samples, which match the target categories, reach a certain threshold , these qualified participants can participate in global aggregation. This prevents participants with large category distribution bias from interfering with the global models convergence. tth u u (xu;i;t) fxu;ign i1 (xu;i;t) 2. Sample selection: during round, each participant of the selected participants measures the importance of samples related to the target tasks categories, where is defined as: (xu;i;t)   j  u;l t;l u;i t;l u;i f (xu;i;t)j2; t;l u;i; t;l u;i lth xu;i tth  t;l l  diag(l( 1); :::; l( rl )) j( )j ,  where are the input and output of the last layer ( ) of sample in the iteration, respectively. . and f (x;) : k u1 nu n fu()  (xu;i;t) . is the output matrix, is a trade-off parameter. The importance of a sample is indicated by the value of : higher values mean higher importance. (xu;i;t) Additionally, this selection strategy assumes that there are mislabeled samples locally, and these are often significantly more important than correctly labeled samples [30]. Therefore, by filtering out outlier samples where is significantly higher than most other samples, the above algorithm can effectively measure the true distribution of local categories, selecting samples closer to the global distribution for local model training.  Feature clustering Feature clustering seeks to find a more abstract representation of original features to group similar data distributions together [7]. In the past, most clustering methods in FTL used model- Table 5 FTL frameworks (continued) Reference FTL challenges Architecture StrategyDHFTL MAFTL SSFTL USFTL HOFTL HEFTL HFL VFL CFL DFLSystem Incremental PS CS FCS LCS QS FSH LSH FLSH [53,55,95,97,137], [100,101,147]     MS [96]     MS [54,99]      MS [56]      MS [205]    MS [108,162,206]    MS [94]    MS,MW [98]      MS,MW [111]     MS,MC [42,45,46,59,64,127]    MC [43]     MC [169]    MC [160,168]    MC [170]    MC [145]     MC [159]     MC [107]    MC [171]    MC [207]    MC [109,161]     MC [44,104]    MC,MI [105]    MC,KD [112]     MI [31]     MI [116]     MI [163]      KD [32,119,138140], [33,57,141]    KD [113,115,144,166]     KD [164]     KD [124]      KD [114]       KD [118,142,143]    KD [165]    KD [146]     KD 1 Abbreviation: PS: prior shift; CS: covariate shift; FCS: feature concept shift; LCS: label concept shift; QS: quantity shift; FSH: feature space heterogeneity; LSH: label space heterogeneity; FLSH: feature and label space heterogeneity; HFL: horizontal FL; VFL: vertical federated learning; CFL: centralized FL; DFL: decentralized FL; IA: instance augmentation; IS: instance selection; FA: feature augmentation; FM: feature mapping; FS: feature selection; FC: feature clustering; FAI: feature alignment; CR: consistency regularization; DCR: domain-dependent consistency regularization; PS: parameter sharing; PR: parameter restriction; PD: parameter decoupling; PP: parameter pruning; MW: model weighting; MS: model selection; MC: model clustering; MI: model interpolation; KD: knowledge distillation. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 13 related information for clustering [4246,59,64,127,169,221]. This model-related information includes things like model parameters [46,64,127], gradient information [42,59], training loss [44,169,221], and other external info [43,45]. Except for these, clustering methods based on data-related information also can be applied to mitigate the prior shift issue. For example, Astraea [47] traverses all unassigned participant data distributions by a greedy strategy, looking for a group of participants that can make the overall data distribution of each cluster as close to a uniform distribution as possible.  Feature selection For example, Fed-FiS [175] generates local feature subsets on each participant by estimating the mutual information between features and between features and categories. Then, the server ranks each feature and uses classification tasks to obtain a global subset of features. Similarly, research [37] utilizes a federated feature selection algorithm based on MI in the operation of autonomous vehicles (AV). This algorithm completes global iterative feature selection by locally executing an aggregation function based on Bayes theory, which greatly reduces the computational cost. Moreover, Feature selection is a common idea to extract important features, which can obtain similar performance across different domains, and these important features can serve as a connection for knowledge transfer [7]. In HOFTL, the local dataset of different participants may have similarities in feature space, and high dimensional features can delay the training time, leading to more energy consumption [48]. In this case, removing irrelevant features and selecting useful overlapping features is crucial to address the distribution shift problem in FTL. Current FL researches [37,48,49,175] have proposed a variety of solutions to the above problems, which mainly include three steps: 1. Local filtering: filter the optimal subset of local features of each participant. 2. Global filtering: aggregate local optimal feature subsets to obtain global feature set. 3. Sharing: feed back the global feature set to the participants, allowing participants to focus on the features most relevant to the global representation. For example, FPSO-FS [49] is a federated feature selection algorithm based on particle swarm optimization (PSO), which proposes two global filtering strategies to determine the global optimal feature subset: i acci j; j  1;:::; k X  maxfXij1 k k j1 acci j(Xi;Datj);i  1;:::; kg Xi ith B Datj jth B acci j(Xi Datj) Xi Datj 1. Mean assembly strategy: for the private optimal feature subset of the participant, its average classification accuracy is obtained by the classification accuracy from all participants, i.e., . Then, an optimal subset with the highest average classification accuracy is selected as the overall optimal feature subset, as follows: , where is the private optimal feature subset from the participant, is the sample data held by the participant, and , is the classification accuracy of evaluated by . i acci j; j  1;:::; k X  maxfXijminj1;:::;k(acci j(X;Datj));i  1; :::; kg 2. Maximum and minimum assembly strategy: the first step is to identify the minimum classification accuracy for participant  s optimal feature subset, using the classification accuracy obtained by all participants. Following this, the subset with the highest minimum classification accuracy among all private optimal subsets is selected as the overall optimal fea- ture subset: .  Consistency regularization The FTL atrategies also can be explained from a model perspective. Figure 6 shows the corresponding strategies. Among them, consistency regularization [7] refers to the addition of regularization terms to the objective function of local (or global) model optimization, which aims to improve the model robustness of participants, facilitating the transfer of knowledge from the source model to the target model during the training process. In traditional transfer learning, domain adaptation machine [222,223] and consensus regularization framework [224,225] are widely used for knowledge transfer in multi-source domains [7], which are applicable to FL scenarios with two or more participants. The objective function is represented as: minf T LT;L( f T ) 1ΩD( f T ) 2Ω( f T ); f T where the first term, as a loss function, is used to minimize the classification error of labeled target domain instances, the second term represents different regularizers, and the third term is used to control the complexity of the final decision function . In addition, according to the research [7], domain-dependent consistency regularization can be expressed as: minf T nT;L j1 ( f T (xT;L j ) yT;L j ) 2 2Ω( f T ) 1 kS u1 u nT;U i1 ( f T (xT;U i )  f S u (xT;U i )) 2 ; u uth LT;L u ( f T ) where represents the weighting parameter that is determined by the relevance between the target domain and the source domain. For example, pFedMe [72] utilizes Moreau envelopes for regularizing participants loss functions. This approach effectively separates the optimization of individualized models from the learning process of the overarching global model within a structured bi-level framework tailored for FTL. MOON [128] proposes a contrastive learning-based federated optimization algorithm that uses the distribution difference in intermediate outputs between global and local models. Each participants local optimization goal, beyond the cross-entropy loss term , aims to minimize the distance between the local and global model representations (reducing weight divergence) and maximize the distance between the local model and its previous version (accelerating convergence). 14 Front. Comput. Sci., 2024, 18(6): 186356  Parameter sharing The parameters of a model essentially reflect the knowledge that the model has learned. Therefore, in FL, participants can also transfer knowledge at the parameter level [7] by parameter sharing, which avoids the privacy risks brought by direct transmission of local data [1]. For instance, the source and target models share parameters, and the target models use their local data to fine-tune the final layers of the source model, thereby creating a new model [68,69,154]. Since parameter sharing is a common approach in FL and often forms the basis for other methods, this section will focus on explaining the basic method of locally fine-tuning the global model. Specifically, study [154] finds that fine-tuning global model parameters with local training sets significantly improves prediction accuracy, particularly for local models that differ greatly from global predictions. Unlike FedPer [68], which fine-tunes the global models top parameters using local data, Per-FedAvg [69] averages all participant model parameters and fine-tunes all global model parameters using the MAML meta-learning method. However, the majority of existing FL frameworks based on parameter sharing mainly focus on improving the global models performance on each participant using the participants local data, overlooking the enhancement of a global models generalization performance from the servers perspective. [117] proposes an FL framework based on a fine-tuning and head model aggregation method, called FedFTHA, which includes FedFT and FedHA. From the participants perspective, FedFT focuses on improving the performance of the global model to the participants local dataset by retaining and fine-tuning the local head model. From the servers perspective, FedHA works to reconstruct a global model that exhibits generalized performance, leveraging the participants head model developed during FedFT. This approach enables both participants and the server engaged in FL to mutually benefit and realize a situation where all parties are advantaged.  Parameter restriction The knowledge learned from participants is kept as model parameters and is transferred by the server in CFL. Using the global model directly as the local model usually requires a strong correlation between the global and local data distributions. If there are large differences in data distribution among participants, using the global model directly and optimizing it with local data could lead to a significant decrease in the models generalization ability. To address this, some studies [7176,129,202] in FTL restrict the similarity between the source and target models by parameter restriction [7]. For example, FedProx [202] controls the differences between local and global model parameters by a proximal term, which aims to avoid the global model being significantly skewed by too many local updates and further affecting robust convergence. This approach keeps updates close to the initial model, helping to tackle the problem of prior distribution shift and covariate shift issues. However, this proximal term could not align local and global optimal points, and considering the potential loss of important parameter information when the global model is transferred locally. FedCL [130] introduces elastic weight consolidation (EWC) from continual learning [226]. By using a server-side proxy dataset to estimate the importance of global model weights, local updates can be adjusted to prevent significant changes in vital parameters during local adaptive training: minf T LT;L u ( f T )   i; j Ri jT;L u;i jS;L g;i j 2 ; R  Ri j cu (c cu) c(c  fcug(u  1)k) where represents the importance matrix of the global model, derived using the servers proxy dataset. FedCL prevents divergence between global and local model weights and ensures better generalization and accuracy. Additionally, FedNova [131] addresses distribution inconsistencies between participants by normalizing and scaling local updates, enhancing model convergence. SCAFFOLD [77] focuses on reducing gradient variance, which first introduces a control variable for the direction of the participant model gradient, and then corrects local model updates based on the difference between this control variable and a global variable , alleviating shift issue. Additionally, FedCSA [78] adjusts the weights of classifier parameters based on the distribution of each category on the participant. This adjustment enhances performance when dealing with class imbalance.  Parameter decoupling Research [227] indicates that the classifier of the model may exhibit significant accuracy decreasing when dealing with imbalanced prior probabilities. Therefore, many studies [227230] have suggested that sending partial local models for aggregation by decomposing models of participants into body (extractor) and head (classifier) parameters can improve accuracy in the target domain, which is called parameter decoupling. The body parameters can capture general data information and be maintained locally, enabling each participant to learn data characteristics for specific tasks, while head parameters learn specific features of the target domain for sharing with the server to improve the effectiveness of knowledge transfer. For instance, FedRep [79], FedBABU [80], FedAlt [155], FedPer [68] and SPATL [132] perform global aggregation by sharing a homogeneous feature extractor. LG-FedAvg [63], CHFL [133], and FedClassAvg [81] share a homogeneous classifier. Different from these, Fed-ROD [129] shows great effectiveness by splitting the model head into general and personalized layers. Two predictors are trained using a shared body model to handle the competing objectives of generic and personalized federated learning. One predictor employs empirical risk minimization (ERM) for personalization, while the other uses balanced risk minimization (BRM) for general learning. Moreover, FedU [82] proposes a local sharing protocol based on a Siamese network. By aggregating only the online models from the source Siamese network to update the target model, it effectively enables knowledge transfer between participants. However, the study [80] finds that existing parameter decoupling methods by updating the entire model during the training process, lead to a decrease in personalization Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 15 performance. Therefore, it proposed an FL framework, called FedBABU. It updates only the body part of the model parameters during the federated training process, and the head is fine-tuned for personalization during the evaluation process. To more accurately determine the degree of impact each layer of the model has on the target domain, the study [83] introduces a layered sharpness-aware Minimization (LWSAM) algorithm, which addresses the problem of poor participant performance due to the biased generic information shared by all participants. This method first calculates the distance between the global and local models at each layer, determining how much each layer is affected by the target domain. It accurately divides the model into head and body parts. Then it uses the sharpness-aware minimization (SAM) algorithm as a local optimizer. By adding more disturbances to the model body, the method adjusts the influence of the target distribution on the model from a global perspective.  Parameter pruning Due to varying data distributions among participants, directly applying an aggregated global model to a target domain often does not give optimal results. One popular solution [132] is parameter pruning, which selects a subset of model parameters from the source domain to apply to the target domain. For example, FedMask [85] uses a method called model binary masks to selectively activate certain model parameters for training. This can happen after just one step of communication, without needing to fine-tune the model on local data, which reduces redundancy in communication and computation. In study [132], each participant uses a pre- trained reinforcement learning agent to choose parameters for combining data in a federated manner. They then use a global encoder and a local predictor to transfer knowledge from the combined model to individual models. Another study [86] proposes a federated search method, which uses a lightweight search controller to find an accurate local sub-network for each participant. This method is good at extracting useful information and lowers the energy used for analysis and training. HeteroFL [87] proposes splitting the global model along its width while maintaining the full depth of the participants DNN models, which aims to find more suitable local models. However, this approach could construct very thin and deep subnetworks, leading to a significant loss of basic features. To overcome this issue, the study [134] introduces a federated learning framework, named ScaleFL, which uses early exits to adaptively reduce DNN models width and depth, finding models best suited for training with limited local resources.  Model weighting The knowledge transfer between participants can be accomplished by sharing local model-related information, such as model parameters, which involves aggregating them before local training, called model aggregation. However, different participants may have distinct optimal goals, simply averaging their model information with the same weight could result in the combined results not being the best solution [1]. The global model in the server may also be overly influenced by a single participants model, causing model drift [131,231]. Thus, model weighting is applied to aggregate models according to their contributions. This prevents model performance degradation caused by directly averaging information from different actors into a domain. For example, the study [88] finds local data with higher prediction errors has more contributions to improve the overall model performance, and then introduces an FL framework, called FedCav. FedCav measures the quality of local data using their prediction errors to decide the weights in the model aggregation process. Considering that the server doesnt know the local data distribution, FedFusion [203] uses a global representation of multiple virtual components with different parameters and weights to portray the data distribution of different participants. The server uses a variational autoencoder (VAE) to learn the best parameters and weights of the distribution components based on limited statistical information taken from the original model parameters. Additionally, the study [135] treats the blending of multiple models in FL as a graph-matching task, and then proposes an algorithm, called GAMF. It views channels and weights as nodes and edges of a graph, respectively. Then it uses a new hierarchical algorithm to increase the similarity of weights between channels, and proposes a cycle-consistent multi-graph matching method to merge various local source models in FL, enhancing the global models generalization. Experiments show that GAMF can be used as a plug-in to boost the performance of existing FL systems.  Model selection In reality, participants local data may significantly differ from the optimal global distribution, and each participants data characteristics can not be directly controlled, thus it is important to select participants related to the data or specific target labels for training, called model selection. For example, the study [91] proposes a DFL algorithm based on directed acyclic graphs (DAG). In the DAG, each participant selects the model updates of other participants based on their data similarity, which has been demonstrated effectively in both prior and covariate shift scenarios. Astraea [47], a scheduler-based multi-participant rescheduling framework, re-schedules multiple participants through a scheduler, which follows that the data distribution of multiple participants is most similar to a uniform distribution. Dubhe [148] is an FL algorithm based on repeated model selection, which allows the server to repeatedly select local models to participate in aggregation, and then send the encrypted distribution of the selected participants to the server to check the similarity between the global data distribution after aggregation and the consistency distribution. This process continuously adjusts the aggregation strategy and improves classification accuracy. Another study [60] proposes a Shapley value-based federated averaging algorithm. It calculates the Shapley value of each participant to assess its relevance to the servers learning objective, estimating the participants contribution in the next communication round of FL. This allows the server to select local models with higher contributions for training in each round of aggregation. In addition, some studies [51,92] require each participant to 16 Front. Comput. Sci., 2024, 18(6): 186356 collect models from all other participants and use an additional local validation set to evaluate the similarity between participants. In contrast, the study [93] utilizes mathematical analysis methods instead of using empirical search from validation data sets to characterize the similarity between participants. Apart from data distribution, another study [52] considers differences in local data volumes between participants. Note that uniform sampling could overlook participants with more data, reducing their contributions to the global model training and impacting the models performance. To address this, the researchers propose FedSampling [52], a framework that uses a data uniform sampling strategy. When the participants data distributions are highly imbalanced, participants randomly select others based on the ratio of the servers desired sample volume to the total available participant sample volume, further improving FL model performance.  Model clustering Some studies [44,103] suggest that grouping similar participants for FL can address the model drift issue caused by data distributions heterogeneity among participants, called model clustering. They determine participant similarity based on factors like model parameters [44,64,104,168,170,232], gradients [42,59], training loss [44,169], or other external information [43,45]. For example, FedCluster [42] uses cosine similarity of the model gradient to split participants into multiple clusters, maximizing similarity within clusters while minimizing it between clusters. To further enhance model adaptability in the target domain by utilizing the sub-model clustering method, the study [105] designs a scale-based aggregation strategy, which scales parameters according to the pruning rate of the sub-models and aggregates overlapping parameters. It further introduces a server-assisted model adjustment mechanism to promote beneficial collaboration between device source models and suppress detrimental collaboration. This mechanism dynamically adjusts the sub-model structure of server devices based on a global view of device data distribution similarity. In addition, studies [43,45] use exogenous information like the types of local devices participants use or patient drug features (drugs given within the initial 48 hours of ICU admission, comprising 1399 binary drug features) for clustering. However, these methods tend to overlook the cluster skew issue caused by grouping, leading to the global model overfitting to a specific clusters data distribution. Cluster skew refers not only to an imbalance in the category distribution among groups after clustering but also to an imbalance in the number of participants in each cluster. To address this issue, study [109] suggests a new FL aggregation method with deep reinforcement learning, called FedDRL, which can tap into the self-learning capability of the reinforcement learning agent, rather than setting explicit rules. Specifically, FedDRL utilizes a unique two-stage training process designed to augment the training data and reduce the training time of the deep reinforcement learning model. Moreover, the study [107] proposes a DFL framework based on hierarchical aggregation, named Spread. In this framework, the server acts as the FL coordinator, and edge devices are grouped into different clusters. Selected edge devices, as cluster leaders, responsible for model aggregation tasks. Spread monitors training quality and manages model aggregation congestion by adjusting both intra-cluster and inter-cluster aggregations.  Model interpolation      Different from model weighting methods that combine local models with varying weights, model interpolation aims to blend global and local model parameters proportionally to increase local prediction accuracy [104,110,129]. For example, research [73] prevents the local model and global model from diverging excessively by setting an interpolation coefficient artificially. When is set to 0, the local model only performs local model learning; as increases, the local model gradually becomes similar to the global model, realizing mixed model learning; when is very large, all local models are forced to be similar, maximizing the transfer of knowledge from the global to the local model. Research [112] interpolates a global model trained globally with a local k- nearest neighbors (KNN) model based on the shared representation, which is provided by the global model. The experiments show that it is also suitable for covariate issues except for prior shift issues. However, research [44] demonstrates that these methods, which rely on the separated training of the global model and local model to find the optimal interpolation coefficient , may not always be the best. Therefore, it proposes a combined optimization strategy that improves both local and global models at the same time. Similarly, research [31] proposes a model interpolation method based on elastic aggregation. They measure the sensitivity of each parameter by calculating the change in the overall prediction function output when each parameter changes, which reduces the update magnitude for more sensitive parameters, preventing the global model from excessively interfering with the local data distribution. 3.1.2 Covariate shift  Feature augmentation Some studies [125,149] have proposed solving covariate shift issues from the perspective of feature augmentation. For example, the study [149] proposes an FL paradigm based on a feature representation generator, called FRAug. It optimizes a common feature representation generator to help each participant generate synthetic feature representations locally, which are converted into participant-specific features by a locally optimized RTNet, which aims to make global and local feature distributions as similar as possible, increasing the training space for each participant. In addition, similar to the study [62] in data augmentation, the study [125] further proposes FedFA to augment features from a statistical perspective. The statistics of augmented data should match as closely as possible with the statistics of the original training data. FedFA follows two preconditions: 1. The data distribution of each participant can be characterized as the statistics of the latent feature distribution, i.e., mean and variance [125]. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 17 2. The statistics of latent features can capture basic domain perceptual characteristics [233235]. Therefore, when dealing with data shift in FL, discrepancies in feature statistics across local participants are inconsistent, and they display uncertain changes compared to the actual distributions statistics. Based on this, FedFA leverages a statistical probability augmentation algorithm based on a normal distribution to enhance the local feature statistics for each participant following as: x   xu u u  u; u u u  N(u; 2 u )   N(; 2 u ) xu xu  (xuu) u ( u; u) u u where the mean and variance are the original statistics of the latent features, and , . is normalized by , then expanded using the new statistical values . The variance dictates how much the latent features are augmented. The magnitude of this variance represents how much the latent feature distribution deviates statistically from the target distribution. By adjusting the variance appropriately, the skewness issue can be resolved either at the level of individual participants or across all participants. Moreover, it can be integrated as a plugin into any layer of any network to enhance features or solve any feature statistical bias, such as skewness in test time distribution. FedFA also exhibits excellent accuracy performance when addressing prior shift or quantity shift problems of homogeneous FTL.  Feature clustering PFA [65] first clusters participants with similar data distribution by computing Euclidean Distance of local representations, and then orchestrates an FL procedure on a group basis to effectively achieve adaptation based on their clustering results. Additionally, the study [167] introduces a new FL framework, called FPL, which is based on prototype clustering. FPL uses the mean features of local data as prototypes, and clusters these prototypes at the server using a comparison learning method. This process brings similar prototypes closer and pushes different ones further apart. Moreover, to increase the stability of model training, FPL uses consistency regularization to minimize the distance between the representative prototypes and their unbiased counterparts. Compared to transferring model parameters in the FPL, the size of the prototypes is much smaller than that of the model parameters, which significantly reduces communication costs.  Consistency regularization From the models perspective, directly adding model-level regularizers to the local objective function of the participants or server is a natural idea [7]. In this way, the knowledge maintained in the model(s) of the participants (source model) can be transferred to the model of another participant (target model) during the training process. For fully supervised learning, each participant first uses their local labeled data to obtain the classification loss term. For example, FedBN [151] keeps participant-specific batch normalization layers to normalize local data distribution. Its classification loss term can be written as: LT;L( f T )  k i1 xi j1 ( f T (xi j);yi j) 2 : Except for the classification loss term, a consistency loss term can be introduced based on a cluster-aware mechanism, which uses the differences in both intermediate outputs and predictions between the global and local models to guide local model optimization [153]. It further groups the participants into clusters based on the feature clustering method by harnessing the similarity among lower-level features of each participants model. Each cluster has its own global feature vector and average prediction value. By minimizing the L2 norm between each participant and the global feature and prediction value within its cluster, the method improves the robustness of local models under the covariate shift issue of homogeneous FTL. Additionally, PFL [167] introduces a consistency regularization term based on a global unbiased prototype. It suggests that the cluster prototype averaged by the server, as an unbiased prototype, can provide a relatively fair and stable optimization point. Calculating the square difference loss between the local feature vector and the unbiased prototype can address the issue of unstable prototype convergence. The regularizer in PFL can be expressed as: Lre gularizer  v j1 (xi; j U k j ) 2 ; i j u v U where and index samples in dataset of participant and the dimensions of feature output, respectively. is the number of dimensions. is the unbiased prototype.  Parameter decoupling Parameter decoupling is not only suitable for prior shift or quantity shift problems [87], but also for solving covariate shift problems in homogeneous FTL. For example, the study [156] proposes a more flexible way of decoupling, designing an FL algorithm based on structured pruning, called Hermes. In this method, participants determine the sub-networks to participate in server aggregation through model pruning. To prevent information loss that could result from directly averaging local models, Hermes only averages overlapping sub-network parameters on the server, keeping the parameters of the remaining non-overlapping parts unchanged. The aggregated parts of the sub-networks are then sent back to the local devices for network updates, thereby improving the models performance on local tasks.  Model weighting FedUReID [157] enhances the adaptability of the aggregated global model to each participants local model by applying an exponential moving average (EMA) to update the global model for each participant, where the weight of the EMA represents the similarity between the global and local models. Additionally, FedDG [158] takes advantage of the domain flatness constraint, which serves as a substitute for the complex domain divergence constraint, to approximate the optimal aggregate weights. Moreover, FedDG uses a momentum mechanism to dynamically assign a weight to each isolated domain by tracking the domain generalization gap, 18 Front. Comput. Sci., 2024, 18(6): 186356 improving its generalization capability. Past studies often simplify the blending of source models into a straightforward allocation problem, ignoring complex interactions between channels. Meanwhile, since model weights are shuffled during training, before merging, channels of each layer need to be aligned to maximize similarities in weights between multiple source models. This presents a quadratic assignment property problem, which is NP-hard problem. To tackle this problem, GAMF [135] propose to treat the channels and weights as nodes and edges of a graph to obtain the weights information. These weighting methods typically rely on model parameters or gradient differences to measure each participants contribution to the target prediction. However, the transmission of this information involves potential privacy leakage risks. Thus, the study [204] views the local model of each participant as black-box model, in which all data is stored locally and only the source models input and output interfaces are accessible. Each participants soft outputs are given a weight based on their inter-class variance. These weighted outputs are then used to create target pseudo-labels. Therefore, it proposes a federated adaptive learning framework called Co-MDA, called CO-MDA. CO-MDA changes the label noise learning section into a semi-supervised learning approach and proposes a Co2-Learning strategy. This strategy involves training two networks at the same time that filter each others errors through epoch-level co-teaching [236], and gradually co-guess the pseudo-labels with the outputs of both target networks to further reduce the impact of label noise.  Model clustering FedDL [170], FedAMP [104], and HYPCLUSTER [44] use model-related information (such as model parameters, convolution layer channels, LSTM hidden states, and neurons in fully connected layers) to construct a shared global model based on model clustering method. However, these methods require several communication rounds to separate all inconsistent participants, potentially affecting computational and communication efficiency. Therefore, the study [168] proposes a method FedMA to achieve hierarchical clustering of participants with a single round of communication. This method uses the difference between the initial global model parameters and local model parameters to generate multiple sub-clusters. Then, by calculating the pairwise distances between participants within all sub-clusters, similar sub- clusters are iteratively merged until only a single cluster remains, containing all samples. Furthermore, similar to study [127] in addressing prior shift issue, study [160] treats the multi-center participant clustering issue as an optimization problem, which can be effectively resolved using the expectation-maximization (EM) algorithm. In addition, different from traditional federated clustering methods, which associate each participants data distribution with only one cluster distribution (known as hard clustering), the study [159] introduces a soft-clustering-based FL paradigm, called FedSoft. FedSoft allows each local dataset to follow a mixture of multiple cluster distributions, improving the training of high-quality local and cluster models.  Model selection Model selection, a classic transfer learning method, has seen widespread use in FTL, either on its own or in combination with other methods, and it is equally effective in addressing covariate shift issues of FTL. For example, CMFL [96] compares the local update of each participant with the global update during each iteration of learning by calculating the proportion of parameters in the local update that have opposite signs to those in the global update, which aims to assess the degree of alignment between the two sets of gradients. A higher proportion indicates a greater deviation from the direction of joint convergence, rendering the local update less relevant. CMFL thus selectively excludes such divergent local updates from being uploaded, effectively minimizing communication costs in FL while ensuring convergence can still be significantly achieved. 3.1.3 Feature concept shift  label concept shift To mitigate feature concept shift challenges in FTL, study [169] utilizes an iterative federated hierarchical clustering algorithm, called IFCA. Different from traditional methods, IFCA does not require centralized clustering algorithms. The server only plays a role in average model parameters, which substantially decreases the servers computational load. However, IFCA needs to run a federated stochastic gradient descent (SGD) algorithm in each round until it converges. This process could increase computational and communication efficiency in large-scale FL systems. Regarding the label concept shift issue, current studies address it from the perspectives of feature alignment [150] or model clustering methods [89,207]. Feddg [150] uses the amplitude spectrum in the frequency domain as data distribution information and exchanges it among participants. The goal is that each participant can fully utilize multi-source data distribution information to learn parameters with higher generalization, which proves equally effective under covariate shift. In addition, considering that Bayesian optimization is a powerful surrogate-assisted algorithm for solving label concept shift issues in FL and black-box optimization problems where the local model-related information is not visible to other participants for privacy. Some researchers have turned their attention to federated Bayesian optimization [108,162,206,207]. However, these methods either have all participants work together on the same task, or make only one participant learn from others to tackle a specific task. However, in real life, the tasks of participants are often related to each other. the study [207] introduces an efficient federated multi-task Bayesian optimization framework, called FMTBO, which dynamically aggregates multi-task models based on a dissimilarity matrix derived from predictive rankings. Additionally, FMTBO designs a federated ensemble acquisition function that effectively searches for the best solution by using predictions from both global and local hyperparameters, enhancing the generalization of the global model. Besides, pFEDVEM [89] introduces an FL framework based on Bayesian models and latent variables, and combines the model weighting strategy to mitigate label concept shift issues. In this setup, a hidden shared model identifies common Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 19 patterns among local models, meanwhile, local models adapt to their specific environments using the information from the shared model, which determines the confidence levels of each participant. The confidence levels are then used to set the weights when combining local models. The extensive experiments have demonstrated that pFEDVEM robustly addresses three types of distribution shift issues including prior shift, covariate shift, and label concept shift, and obtains significant accuracy improvement compared to the baselines. 3.1.4 Quantity shift FEDMIX [61] has proven that the instance enhancement method is equally effective for quantity shift problems. Moreover, studies [161,171] view FL as a hedonic game, where each participant produces some cost (error) when joining in the FL process. Theres a Nash equilibrium between minimizing individual errors and overall errors. For example, a school may aim to minimize its local error, while a region or city may aim to minimize the overall error. Study [171] proposes to find a relatively stable participant partition by accurately estimating the expected error of each participant, which may overlook the need to minimize the overall error. Additionally, the numbers of samples for participants in [171] are only assumed to be small or large, Different from it, study [161] not only more focuses on overall social well-being, but also is suitable for any number of participants with any various numbers of samples. 3.2 Heterogeneous federated transfer learning This section will discuss existing works addressing the challenges of heterogeneous FTL, dynamic heterogeneous FTL, and model adaptive FTL from data-based and model- based perspectives as shown in Fig. 6. However, it is worth noting that there is very little research on scenarios with heterogeneous label space and heterogeneous feature and label space, so we will not elaborate on it here. 3.2.1 Feature space heterogeneity A B XB com (A;B) B;A XB com (A;B) XB com (B;A) (A;B) (B;A) XB pr i XB com (A;B) XA pr i XB com (B;A) A B Heterogeneous feature spaces often occur in VFL, thus, we mainly focus on the VFL scenario for feature space heterogeneous FTL. To address this issue, researchers can utilize methods such as feature alignment [172,237,238] or feature concatenation [176] to construct new feature datasets for model training. Among them, feature alignment in VFL can be completed by constructing a novel feature subspace [237], or filling in missing or incomplete features of each participants feature spaces [172]. These approaches enable knowledge transfer under a homogeneous feature space. Specifically, the study [172] assumes an inconsistency in the feature spaces between two participants, the active participant and the passive participant . Both of them map their features using their respective mapping functions and to the same feature space, resulting in new feature representations and . They optimize the mapping functions and by minimizing the similarity between the private features and , as well as and . Finally, participants and , through secure bilateral computation, obtain the complete XA b XB a features and respectively. Based on this, the federated aggregation can be implemented under the aligned feature space. However, these methods rely on the existing feature space of participants, ignoring the relationships among these features. By combining feature clustering methods, the active participant can create new, more valuable feature space for knowledge transfer. For example, study [173] proposes a VFL paradigm based on feature space decomposition clustering, called PrADA. The specific steps include: C p C q h h  p q 1. Feature grouping: participant applies domain expertise to divide raw features into groups, each containing tightly related features. Moreover, participant constructs interactive feature among pairs of feature groups, resulting in a total of feature groups (where ). B C fE  ffE;ig i  1 h 2. Pretraining stage: this stage involves collaborative efforts between source participant and participant , to train a set of feature extractors ( for to ) that are capable of learning features which are both invariant across domains and discriminative for labels. A C A C fE  ffE;ig i  1 h 3. Fine-tuning stage: this process is executed in collaboration between active participant and participant , with the goal of training participant s target label predictor by utilizing the pre-trained set of feature extractors ( , where to ). In addition, PrADA enhances privacy and security using a secure protocol based on partial homomorphic encryption. However, not all features of the participants are relevant to the task. Therefore, the novel feature space generated by aggregating the features of all parties needs to filter features that are not relevant to the task through feature selection. However, current feature selection techniques [232,239] in distribution learning, often need numerous training iterations, particularly when dealing with high-dimensional data. Directly applying them in FTL to solve feature space heterogeneous issues produces significant computational and communication overhead, as each training round involves multiple encryptions, decryption operations, and intermediate parameter transfers. For example, study [179] suggests using an embedded method to combine autoencoders with L2 constraints on feature weights for feature selection, and sets a threshold for post-training to determine the selected features for mitigating the problem of model parameter shrinkage [240]. Different from previous VFL research scenarios where there were mostly two participants and binary classification tasks, study [178] proposes an FL feature selection scheme suitable for multi-participant multi-classification. In addition, previous studies that mainly focus on the relationship between features and labels [180], ignoring the relationship between features, to solve this problem, similar to research [37,175] using MI theory into federated feature selection in HFL, study [174] proposes a feature selection VFL framework based on conditional mutual information, called FEAST. FEAST integrates feature information into a single statistical variable for FL transmission, which not only accomplishes key feature selection and further reduces communication costs while 20 Front. Comput. Sci., 2024, 18(6): 186356 ensuring privacy and security. In addition, study [181] first proposes a theoretically verifiable feature selection method, formalizing the feature selection problem in the VFL environment, and providing a theoretical framework to prove that unimportant features have been removed. 3.3 Dynamic heterogeneous FTL 3.3.1 System heterogeneity System heterogeneity among participants could lead to the emergence of stragglers in each iteration. To address this problem, instance selection can be leveraged by researchers to mitigate the computational burden of participants when they have heterogeneous local computational resources. However, it could lead to decreased model performance due to the reduced statistical utility of the training dataset. Study [198] obtains the optimal data selection scheme through an optimization function that includes lower and upper limits of resources, as well as arbitrary, non-decreasing cost functions per resource, meanwhile, it treats this problem as a scheduling problem of tasks assignment to resources, seeking to maximize the number of participants in each round of FL updates. In addition, FedBalancer [197] chooses samples for training by measuring their statistical utility, derived from the sample loss list based on the latest model. However, it is inefficient to wait for every participant to finish local training before proceeding with aggregation due to system heterogeneity. Thus, under a constant FL round deadline setting, instance selection could not immediately enhance the time-to-accuracy ratio. Furthermore, model-based strategies, such as consistency regularization [202], model selection [1], model clustering [55,97,98,145], parameter decoupling [106], parameter pruning [85] can also be applied as effective ways to address the straggler issue in FL. For example, Fedavg [1] directly drops models of these stragglers which can not accomplish local training in time when the other participants have completed the same amount of training. Based on this, FedProx [202] allows varying local training epochs across participants, tailored to each devices system capabilities. Subsequently, it aggregates the non-convergent updates submitted by stragglers, rather than dropping these less responsive participants from this iteration. However, these frameworks may come at the cost of sacrificing accuracy due to the omission of partial information and waiting for all participants to complete a uniform number of training epochs tends to extend the convergence time of FL. FedAT [106] blends synchronous and asynchronous updates by decoupling the model parameters at the layer level, which stratifies local models based on the time each participant needs to complete a round of training. During each training round, FedAT randomly selects some local models in each layer to calculate the loss gradient of local data, completing the synchronous update of models in that specific layer. Each layer, acting as a new training entity, then asynchronously updates the global model. The faster layers have shorter round-response delays, speeding up the convergence of the global model. The slower layers contribute to global training by asynchronously sending model updates to the server, which further improves the predictive performance of the model. Besides, [85,86,134] selectively use local models for transfer knowledge by parameter pruning methods under the limited computational resources. Study [145] introduces a FL framework, called FedHiSyn, which uses a resource-based hierarchical clustering approach. This framework first categorizes all available devices according to their computing capabilities. Given that a ring topology is more suitable for models with uniform resources, after local training, the models are sent to the server. Then, within their respective categories, they exchange local model weight updates based on the ring topology structure to mitigate the lag effect caused by system heterogeneity. Considering that the main challenge of dynamic heterogeneous FTL is the appropriate scheduling of participants, essentially a local model selection issue at each iteration. Some researchers [53,9496,136,137] assume the central party has 1-lookahead in source model selection strategies, which means that the dynamic input data beforehand is known. However, this can not be applied when dealing with unpredictable time series inputs. Thus, reinforcement learning has been increasingly used to design source model selection strategies [54,94,97,111,132]. Studies [97,98] propose FL paradigms based on the multi-armed bandit (MAB). In situations where the available computing resources of participants are unknown, these approaches calculate the difference between the data distribution of multiple combined participants and a class-balanced data distribution, and then pick local models for aggregation and assign weights based on these differences. In another study, Study [55] proposes a participant scheduling strategy by age of update (AoU) measurement. This strategy considers the age of the received parameters and the current channel quality at the same time, which improves efficiency and allows effective aggregation in federated joint learning. Except for the above-mentioned reinforcement learning methods in dynamic heterogeneous FTL, TiFL [99], a hierarchical federated learning framework, addresses system heterogeneity issues by grouping participants with similar training performance. Meanwhile, in each round of updates, TiFL adaptively selects local models for training within each group by simultaneously optimizing accuracy and training time. To accelerate model convergence under system heterogeneity, FedSAE [100] suggests choosing participants with larger local training losses to take part in aggregation during each training round. FedSAE also designs a mechanism to predict each participants maximum tolerable workload, aiming to dynamically adjust their local training rounds. Research [205] found that differences in bit-width among participant devices can impact the performance of the global model. Low-bit-width models are more compatible with hardware but could limit the models generalization, leading to poor performance of high-bit-width models. To tackle this, the study [205] introduces ProWD, a FL framework that considers bit-width heterogeneity. This framework selects sparse sub- weights compatible with full-precision model weights from the low-bit-width models received by the server. These selected sub-weights then participate in central aggregation along with the full-precision model weights. Another study Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 21 [101] proposes a FL framework, called Aergia, which freezes the most computation-heavy parts of the model and trains the unfrozen parts. Moreover, the server chooses a more reasonable offloading solution based on the training speed reported by each participant and the similarity between their datasets. In this way, the training of the frozen parts can be offloaded to participants with ample resources or faster training speeds. PyramidFL [147] is a fine-grained participant selection, which considers not only the distribution and system heterogeneity between the selected and non-selected participants but also within the selected participants themselves. Specifically, the server uses feedback from past training rounds to rank participants based on their importance, participants then use their rank to determine the number of iterations for data efficiency and the parameters to drop for system efficiency. Furthermore, the utility of each participant isnt static but varies across training rounds. If a participant is selected, its data utility will then decrease since these data have been seen by the model. Thus, reducing the likelihood of selection in subsequent training rounds allows participants who were not selected previously to have a higher probability of being chosen, which further improves the model performance under the dynamic heterogeneity and further enhances the fairness of participant selection. 3.3.2 Incremental heterogeneity T(t)i ui t T(t 1)j uj t 1 uj ui Existing FTL strategies mainly focus on model-based techniques, for example, GLFC [102] addresses the continuous emergence of new classes in federated online learning by consistency regularization method. It introduces a class-aware gradient compensation loss to ensure the consistency of the learning pace for new classes with the forgetting pace of old classes. It separately normalizes the gradients for new and old classes and reweights them for the local optimization goal, where the relationships between classes are obtained based on the best old classification model from the previous tasks. As the local data or tasks increase, where the tasks may be a new batch of data, the task learned by participant in round could be similar or related to the task learned by participant in round . In this situation, the transmission of aggregated global information among participants can facilitate knowledge transfer across participants. Nonetheless, when a new joined task in participant is irrelevant to the tasks of participant , it could affect the optimization direction of the local model, leading to a decrease in accuracy. To mitigate this issue, each participant selectively utilizes only the knowledge of the relevant tasks that have been trained on other participants during each iteration, while ignoring as much as possible the knowledge of irrelevant tasks that may interfere with local learning. Thus, parameter decomposition and model weighting methods have attracted the attention of researchers [110]. For example, FedWeIT [110] solves this problem by decomposing parameters into three different types for training: global parameters that capture the global and generic knowledge across all participants, local base parameters that capture generic knowledge for each participant, and task-adaptive parameters for each specific task per participant. Meanwhile, FedWeIT applies sparse masks to select parameters relevant to a given task, minimizing interference from irrelevant tasks of other participants and allocating attention to the servers aggregated parameters to selectively filter parameter information. However, all these methods lack a theoretical basis for ensuring convergence. FedL [56] uses dynamic adaptation to measure the extent to which online decision constraints are breached and calculates a maximum limit for this measure, which guarantees that the expected contribution of the chosen source model to the FL models performance matches its actual contribution. 3.4 Model adaptive FTL Q R R Q X R Model adaptive FTL is often caused by model heterogeneity, i.e., the local models of different participants may be inconsistent in architecture, which could cause incompatibility in the feature dimension and representational capacity among participants [115]. It means that the average aggregation approach based on consistent features can not be used directly in FL. To mitigate this issue, data-based strategies are proposed in FTL. For example, the study [200] proposes to apply a feature mapping method to obtain consistent representation space and complete FL. Considering that even with different model structures, they possess some common knowledge for the same input, i.e., the feature extraction layers generate similar feature maps, research [200] uses model drafts to align local data distributions of participants. These outputs from specific layers or models are interpreted as blurred images of data and defined as model drafts. By minimizing the similarity difference between the local and global drafts, the data distribution difference between participants can be reduced. Except for feature mapping [7,126] using explicit features, some implicit features can be utilized to align the source and target domains, facilitating knowledge transfer within this aligned space [7], called feature alignment. Implicit features include subspace attributes [201], spectral characteristics [150], prototype graphs [66]. For example, the study [66] uses prototypes to effectively transfer information under the model adaptive FTL by minimizing local and global prototype graphs within the same feature space, thereby capturing the semantic information of class structures. It avoids the possibility of data from different classes (across various participants) merging into a single class, or data from the same class being spread across multiple classes. Similarly, FedHeNN [103] is a FL framework based on instance-level representation. Each participant randomly selects part of local data and obtains instance-level representation to guide local training. By introducing a distance function based on centralized kernel alignment as a proximal term of the local loss function, it aligns local and global model representations, enabling federated learning across heterogeneous models. FedFoA [201] adds a linear calibration layer at the end of each local model to first calibrate the different feature dimensions among participants to the same dimension space. Participants use decomposition to obtain feature subspace and feature correlation matrix . The central server minimizes the reconstruction of local features and the product of global 22 Front. Comput. Sci., 2024, 18(6): 186356 Q Q and initial vector to get the optimal , guiding the update of the local sub-feature space. Inconsistent model architectures also make the simple average aggregation of model parameters ineffective. From the model-based perspective, studies [87,156] implement FedAvg on top of local sub-networks by parameter decoupling. [87] assumes that the model architectures of participants can dynamically change with each iteration, and further proposes to leverage parameter decoupling to obtain at least one fixed sub-network for each type of heterogeneous situation and aggregate them into a single global model. Thus, smaller local models can gain more from global aggregation by performing less global aggregation on a subset of the parameters from larger local models. Similarly, study [84] combines parameter decoupling with model clustering method to group local models based on the similarity of their personalized sub-networks, maximizing the level of knowledge sharing between participants. Besides, research [144] designed a Mapper at the local level to convert feature representations from different semantic spaces to the same feature space, and accomplish the knowledge transfer based on knowledge distillation (KD). They deploy a global generator at the server to extract global data distribution information and distill it into the local model of each participant. Then, local models are viewed as discriminators to reduce the difference between global and local data distributions in heterogeneous feature spaces. Since the feature representations synthesized by the global generator are usually more faithful and homogeneous to the global data distribution, they can achieve faster and better convergence. Additionally, local generators also can be used to enhance hard-to-judge sample data, improving model performance [144]. In real-world scenarios, cross-institutional FL is often more content with the VFL scenario. However, traditional VFL can only benefit from samples shared among multiple parties, which severely limits its application. Therefore, research [115] proposes a VFL framework based on representation distillation, called VFedTrans. This framework collaboratively models common features among multiple parties and extracts federated representations of shared samples, aiming to maximize data utility as much as possible through KD. Knowledge distillation is also often used for dealing with model adaptive FTL induced by model heterogeneity as shown in Table 5. FedMD [163] uses KD for federated learning in situations where different models are used. Instead of just combining model parameters, FedMD calculates class scores for each participant using a shared dataset. These scores are then sent to a server to calculate an average, which guides the training of local models. This method allows for knowledge sharing while keeping private data and model structures secure, and it works even when different local models are used. Contrary to the assumption that participants local models are entirely different, research [119] assumes that local models of participants are not fully heterogeneous, and there are cases where some models share the same structure. Therefore, they propose an FL framework based on ensemble distillation, called FedDF. It creates several prototype models, which represent participants with identical model structures. In each round, FedAvg is performed among participants with the same prototype model to initialize a global model (student model), followed by cross-architecture learning through knowledge distillation. In this process, the parameters of the local model (teacher model) are tested on an unlabeled public dataset to generate predictions for training each student model on the server. Research [113] adopts a federated communication strategy, denoted as FSFL, which is similar to FedMD, innovatively adding a latent embedding adaptive module to alleviate the impact of domain discrepancies between public and private datasets. However, these studies [32,33,57,114,119,139141,163,164,241] strongly rely on the construction of public datasets, which undoubtedly compromises data privacy and is operationally challenging in practice. The impact of the quality of these prerequisites on the performance of federated learning is also unknown [124]. Therefore, research [124] proposes an FL framework based on zero-shot knowledge distillation, called FedZKT. FedZKT requires no prerequisites for local data, and its distillation tasks are assigned to the server to reduce the local workload. In addition, some studies [29,115,142144,165] introduce generators to avoid the need for public datasets, enabling the aggregation of local information in a data-free manner. For example, FedFTG [143] uses the log-odds of each local model as a teacher to train a global generator and fine-tunes the global model using pseudo-data generated by the global generator. Due to the additional computational and communication costs imposed by the introduction of a generator, ScaleFL [134] proposes a self-distillation method based on exit predictions. This method treats self-distillation as an integral part of the local training process, requiring no extra overhead. ScaleFL enhances the knowledge flow among local sub-networks by minimizing the KullbackLeibler divergence (KL divergence) between early exits (students) and final predictions (teachers). Furthermore, the study [166] introduces a FL framework based on a data-free semantic collaborative distillation, called MCKD. This framework transfers soft predictions from local models to a server to learn representations that dont change across different domains. MCKD also introduces a knowledge filter to mitigate the potential amplification of irrelevant or malicious participants influences on the target domain by traditional averaging aggregation. This knowledge filter generates consensus knowledge for unlabeled data and sets a threshold to drop models where local model predicted classes are inconsistent with consensus classes, further adapting the central model to target data. However, most of these methods construct ensemble knowledge by merely averaging the soft predictions of multiple local models, overlooking that local models have a differential understanding of distillation samples. Research [138] suggests that a model is more likely to make the correct predictions when the samples are included in the domain used for the models training. Based on this, the study [138] proposes treating each participants local data as a specific domain and designs a domain-aware federated distillation method named DaFKD. DaFKD can recognize the importance of each model to the distillation samples. For a given distillation sample, if the local model has a significant Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 23 relevance factor with it, DaFKD assigns a higher weight to this local model. 3.5 Semi-supervised and unsupervised FTL The labeled data scarcity is a common scenario in both HFL and VFL scenarios. Data-based methods receive some attention in reality for SSFTL issues, some studies [120,121] utilize these methods to obtain valuable augmented data by strategically optimizing predictions or similarity calculations on these data. For example, SemiFed [120] uses both past local and global models to predict labels for local unlabeled data. When predictions have high confidence, the pseudo- labeled data is used to train as available data. Orchestra [121] uses a feature clustering method to obtain more available data for training. It refines local data clusters through interactions with the server, which helps find effective samples in unlabeled data, further increases sample size, and reduces distribution heterogeneity when labeled data is scarce. Different from them without considering the high-class imbalance of unlabeled data, based on the instance selection method, CBAFed [123] uses the empirical distribution of all training data from the last round of global communication to design category-balanced adaptive thresholds. That is, if the model uses more data for training in one class, the threshold for labeling unlabeled data in this class will increase, and vice versa. It aims to shrink the gap between local and global distributions by selecting a local training set, and further influence category distribution, preventing a decline in global model performance due to prior probability bias. Model-based strategies similarly show the effectiveness in addressing the SSFTL issue, study [50] introduces a framework based on the consistency regularization method, called FedMatch. This framework splits local model parameters into two parts, one for updating with labeled data, and another for unlabeled data. For the parameters associated with labeled data, the loss function solely includes cross- entropy loss, and the loss function related to unlabeled data follows as: minf T;U LT;U ( f T;U ) L2  L U 2 2 L1 U 1; L1 L2 U U  L where and regularization on aims to make sparse, while not drifting far from the current optimal parameter trained with labeled data. The first term differs from the studies [120,152], which not only uses the prediction results of unlabeled data and its augmented data to obtain cross-entropy loss but also considers pseudo-labels as real labels to obtain new category loss, making the model perform the same on original data and slightly perturbed data. Contrary to previous studies that assume the presence of both labeled and unlabeled data locally, research [50] describes a federated semi-supervised learning scenario where some participants have fully labeled data while others have only unlabeled data. To guide unlabeled participants learning by building the interaction between the learning at labeled and unlabeled participants [120,152], FedIRM [41] applies the intermediate output of the local model, which is trained by the labeled participants, to construct a category relationship matrix. By transmitting the relationship matrix of each labeled participant, it guides the unlabeled participants to learn their local relationship matrix. The formula is as follows: minf T;U (w)(LT;U ( f T;U ) LIRM); LIRM  1 z z j1 (LKL(RL j jjRU j ) LKL(RU j jjRL j )); Rj j where denotes the relation vector of class . Furthermore, researchers [67,128] also apply domain-dependent consistency regularization to solve this issue. For example, [67] uses the KL divergence between local and global model predictions to control the update of the local objective function. Additionally, other model-based strategies, such as parameter decoupling [50], model weighting [90,122,152], also attract some researchers attention. FedMatch [50] accomplishes the FTL under semi-supervised learning scenarios by training labeled and unlabeled data separately through model parameter decomposition. FedConsist [152] and RscFed [122] improve the performance of models in semi-supervised FTL by changing the weights of participants with labeled and unlabeled data. Since semi-supervised learning often deals with data that only has positive and unlabeled (PU) samples, one participants negative class could be made up of multiple positive classes from other participants. However, traditional PU learning mostly focuses on binary problems with only one kind of negative sample. Thus, study [90] assumes that the available data has multiple types of positive and negative classes (MPMM-PU), and further proposes to redefine the expected risk of MPMM-PU, which aims to decide each participants weight and examines the limits of the models generalization. In response to unsupervised FTL, a straightforward approach is to combine self-supervised methods with FL, such as [195,242]. However, this challenge is often accompanied by homogeneous or heterogeneous FTL issues. The feature- based strategies have obtained some attention to solve this problem, such as FedCA [199] based on feature selection, FSHFL [48] based on feature mapping, and FedFoA [201] based on feature augmentation. Moreover, studies also propose to alleviate the data drift issues between participants by model-based strategies, such as FedX [146] based on KD, and FedEMA [116] based on model interpolation. FedCA [199] shares features of local data and employs an auxiliary pubic dataset to minimize disparities in the representation space across participants. However, it ignores the inconsistency between the feature representation of local unlabeled data and global feature representation. To solve this problem, FSHFL [48], an FL framework based on an unsupervised federated feature selection approach, proposes the feature cleaning locally and global feature selection. The local feature cleaning utilizes an enhanced version of the one- class support vector machine (OCSVM) algorithm, called FAR-OCSVM, to identify features that lack sufficiently representative global features. The identification relies on local feature clustering, features within each cluster exhibit strong interrelationships, thus the clusters with more features contribute more significantly to the FTL process. Meanwhile, 24 Front. Comput. Sci., 2024, 18(6): 186356 the server selects global features from the collected local feature sets and then returns these global features to participants, directing them to select local features that are closest to the global representation. Considering that the public dataset has potential privacy leakage risk [199], FedX [146] incorporates local knowledge distillation and global knowledge distillation into the FedAvg [1] without any public data. Local knowledge distillation trains the network using the feature representations of local unlabeled datasets, and global knowledge distillation aims to mitigate data shift, which only relies on global model sharing. Besides, FedEMA [116] utilizes self-supervised learning methods with predictors, including MoCo and BYOL, to update local encoders through the EMA of the global encoder. 4 Application In this section, we explore and outline the prevalent applications where FTL makes a significant impact. 4.1 Federated cross-domain recommendation Cross-domain recommendation (CDR) aims to reduce data sparsity by transferring knowledge from a data-rich source domain to a target domain. However, this process presents significant challenges related to data privacy and knowledge transferability [243,244]. Therefore, federated learning is introduced to CDR to improve the performance of the target domain model while providing privacy protection. Currently, classic recommendation algorithms have been widely applied to federated learning, such as federated collaborative filtering [245,246], federated matrix factorization [247249], and federated graph neural networks [250]. However, these methods neglect the heterogeneity among them including data, resource, or model heterogeneity. Researchers have combined transfer learning techniques to accomplish tasks related to federated cross-domain recommendation. They utilize parameter sharing [251], parameter decoupling [252,253], and model clustering [254] to facilitate the process. 4.2 Federated medical image classification Medical data involving patient information is sensitive and its use is strictly regulated, limiting the application of current artificial intelligence technologies in the medical field. Federated learning, which trains models on local devices without sharing raw data, could protect patients data privacy security. However, data provided by different medical institutions, acting as source domains, often have heterogeneity in format, features, or distribution [255]. By integrating FL with transfer learning, we can leverage data from different healthcare institutions for model training, enhancing the models performance while preserving data privacy. This approach presents a promising direction for the application of artificial intelligence in the medical field. Common methods include federated knowledge distillation [139,256], federated weighting aggregation [204,257260], federated consistency regularization [261,262], federated model selection [263], federated model interpolation [264], federated model decoupling methods [265], federated model clustering [266], and feature clustering methods [267]. 4.3 Federated financial service Federated financial services include credit risk control [268272], stock prediction [273275], financial fraud transaction detection [276,277], etc. Among them, credit risk control is a standard procedure for financial institutions, which estimates whether an individual or entity is able to make future required payments [269]. Stock prediction allows economists, governors, and investors to model the market, manage the resources and enhance stock profits [273]. Fraudulent transaction detection is another difficult problem for individual banks to curb company financial losses and maintain positive customer relationships [276]. However, their predictive models all rely on large amounts of data to achieve training. Since the customers data or fraudulent transaction data can not be shared among different institutions, the prediction model of each financial institution often suffers from the limited sample issue, which significantly hinders the performance improvement of the model. The emergence of federated learning not only breaks this data silos problem but also can combine instance augmentation [271,275], instance selection [270], feature selection [268,269,277], feature alignment [237,238], parameter sharing [272], model weighting [273], model selection [274,276], knowledge distillation [272], and other technical approaches to achieve information exchange between institutions. 4.4 Federated traffic flow prediction Given that precise and up-to-date traffic flow data is of immense significance for traffic management, predicting traffic flow has emerged as a crucial element of intelligent transportation systems. However, current traffic flow prediction techniques, which rely on centralized machine learning, require the collection of raw data from mobile phones and cameras for model training, causing potential privacy issues. Researchers found that this issue also can be addressed using FTL, such as federated clustering aggregation [278283], federated weighting aggregation [284286], federated parameter sharing [285], and federated parameter control methods [283]. 5 Conclusion and future work This survey provides a systematic summary of federated transfer learning, which emerges from the combination of transfer learning with federated learning, offering the corresponding definitions, challenges, and strategies. For convenience for researchers, we compile settings for the most common data heterogeneity scenarios, including both homogeneous and heterogeneous federated transfers, and summarize some significant FTL studies for various challenges. In the future, the utility performance of FL models in more complex scenarios deserves further exploration, including label concept shift, feature concept shift, label space heterogeneity, and feature  label space heterogeneity. Due to differences or mutations in unknown or hidden relationships between input and output variables between participants, the concept shift becomes an important but less studied direction in FL. Meanwhile, label space heterogeneity and feature  Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 25 label space heterogeneity also deserve more in-depth study. Furthermore, although federated transfer learning uses model selection, weight aggregation and other transfer learning methods to solve the problems of data heterogeneity, dynamic heterogeneity and labeled data scarcity in FL, new privacy concerns may have also emerged about these strategy preferences. After participants crack the weighting strategy or selection mechanism used by the server to aggregate information from all parties, they may use unfair means to make the training of the global model develop in a direction more beneficial to themselves. Thus, possible leakage of strategy preferences should be properly considered when designing FTL strategies in the future. Finally, the communication costs, communication efficiency, and computational costs caused by these FTL strategies could be paid more attention by FL researchers. Compared with traditional FL methods, FTL strategies, such as instance augmentation, instance selection, feature selection, model selection, and model clustering, etc., increase additional computational costs and introduce more shared information among participants. Based on some traditional methods [287290] for enhancing computational or communication efficiency, FTL researchers can further optimize federated algorithms. In summary, FTL still has great potential research value in utility, privacy, and communication issues. Acknowledgements The research work was supported by the National Key RD Program of China (No. 2021ZD0113602), the National Natural Science Foundation of China (Grant Nos. 62176014 and 62202273), and the Fundamental Research Funds for the Central Universities. Competing interests The authors declare that they have no competing interests or financial conflicts to disclose. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the articles Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the articles Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:creativecommons.orglicenses by4.0. References McMahan B, Moore E, Ramage D, Hampson S, Arcas B A Y. Communication-efficient learning of deep networks from decentralized data. In: Proceedings of the 20th International Conference on Artificial Intelligence and Statistics. 2017, 12731282 1. Feng S, Li B, Yu H, Liu Y, Yang Q. Semi-supervised federated heterogeneous transfer learning. Knowledge-Based Systems, 2022, 252: 109384 2. Zhang C, Xie Y, Bai H, Yu B, Li W, Gao Y. A survey on federated learning. Knowledge-Based Systems, 2021, 216: 106775 3. Gao L, Fu H, Li L, Chen Y, Xu M, Xu C Z. FedDC: Federated learning with non-IID data via local drift decoupling and correction. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 1010210111 4. Shi Y, Zhang Y, Xiao Y, Niu L. Optimization strategies for client drift in federated learning: a review. Procedia Computer Science, 2022, 214: 11681173 5. Liu Y, Kang Y, Zou T, Pu Y, He Y, Ye X, Ouyang Y, Zhang Y Q, Yang Q. Vertical federated learning: concepts, advances and challenges. 2022, arXiv preprint arXiv: 2211.12814 6. Zhuang F, Qi Z, Duan K, Xi D, Zhu Y, Zhu H, Xiong H, He Q. A comprehensive survey on transfer learning. Proceedings of the IEEE, 2021, 109(1): 4376 7. Yang Q, Liu Y, Chen T, Tong Y. Federated machine learning: concept and applications. ACM Transactions on Intelligent Systems and Technology, 2019, 10(2): 12 8. Rahman K M J, Ahmed F, Akhter N, Hasan M, Amin R, Aziz K E, Islam A K M M, Mukta M S H, Islam A K M N. Challenges, applications and design aspects of federated learning: a survey. IEEE Access, 2021, 9: 124682124700 9. Liu J, Huang J, Zhou Y, Li X, Ji S, Xiong H, Dou D. From distributed machine learning to federated learning: a survey. Knowledge and Information Systems, 2022, 64(4): 885917 10. Li L, Fan Y, Lin K Y. A survey on federated learning. In: Proceedings of the16th IEEE International Conference on Control  Automation (ICCA). 2020, 791796 11. Zhan Y, Zhang J, Hong Z, Wu L, Li P, Guo S. A survey of incentive mechanism design for federated learning. IEEE Transactions on Emerging Topics in Computing, 2022, 10(2): 10351044 12. Yin X, Zhu Y, Hu J. A comprehensive survey of privacy-preserving federated learning: A taxonomy, review, and future directions. ACM Computing Surveys, 2022, 54(6): 131 13. Lyu L, Yu H, Yang Q. Threats to federated learning: a survey. 2020, arXiv preprint arXiv: 2003.02133 14. Pfitzner B, Steckhan N, Arnrich B. Federated learning in a medical context: a systematic literature review. ACM Transactions on Internet Technology, 2021, 21(2): 50 15. Nguyen D C, Pham Q V, Pathirana P N, Ding M, Seneviratne A, Lin Z, Dobre O, Hwang W J. Federated learning for smart healthcare: a survey. ACM Computing Surveys, 2023, 55(3): 60 16. Lim W Y B, Luong N C, Hoang D T, Jiao Y, Liang Y C, Yang Q, Niyato D, Miao C. Federated learning in mobile edge networks: a comprehensive survey. IEEE Communications Surveys  Tutorials, 2020, 22(3): 20312063 17. Nguyen D C, Ding M, Pathirana P N, Seneviratne A, Li J, Poor H V. . Federated learning for internet of things: a comprehensive survey. . IEEE Communications Surveys  Tutorials, 2021, 23(3): 16221658 18. Zhu H, Xu J, Liu S, Jin Y. Federated learning on non-IID data: a survey. Neurocomputing, 2021, 465: 371390 19. Tan A Z, Yu H, Cui L, Yang Q. Towards personalized federated learning. IEEE Transactions on Neural Networks and Learning Systems, 2023, 34(12): 95879603 20. Pan S J, Yang Q. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 2010, 22(10): 13451359 21. Konečný J, McMahan H B, Ramage D, Richtárik P. Federated optimization: distributed machine learning for on-device intelligence. 2016, arXiv preprint arXiv: 1610.02527 22. Long M, Wang J, Sun J, Yu P S. Domain invariant transfer kernel learning. IEEE Transactions on Knowledge and Data Engineering, 2015, 27(6): 15191532 23. Long M, Cao Y, Wang J, Jordan M I. Learning transferable features with deep adaptation networks. In: Proceedings of the 32nd International Conference on Machine Learning. 2015, 97105 24. Bengio Y. Deep learning of representations for unsupervised and transfer learning. In: Proceedings of 2011 International Conference on Unsupervised and Transfer Learning Workshop. 2011, 1737 25. Raina R, Battle A, Lee H, Packer B, Ng A Y. Self-taught learning: transfer learning from unlabeled data. In: Proceedings of the 24th 26. 26 Front. Comput. Sci., 2024, 18(6): 186356 International Conference on Machine Learning. 2007, 759766 Younis R, Fisichella M. FLY-SMOTE: re-balancing the non-IID IoTedge devices data in federated learning system. IEEE Access, 2022,10: 6509265102 27. Wu Q, Chen X, Zhou Z, Zhang J. FedHome: cloud-edge basedpersonalized federated learning for in-home health monitoring. IEEETransactions on Mobile Computing, 2022, 21(8): 28182832 28. Jeong E, Oh S, Kim H, Park J, Bennis M, Kim S L. Communication-efficient on-device machine learning: Federated distillation andaugmentation under non-IID private data. 2018, arXiv preprint arXiv: 1811.11479 29. Li A, Zhang L, Tan J, Qin Y, Wang J, Li X Y. Sample-level data selection for federated learning. In: Proceedings of the IEEE Conference on Computer Communications. 2021, 110 30. Chen D, Hu J, Tan V J, Wei X, Wu E. Elastic aggregation for federated optimization. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 1218712197 31. Chen H Y, Chao W L. Fedbe: Making Bayesian model ensemble applicable to federated learning. In: Proceedings of the 9th International Conference on Learning Representations. 2021 32. Seo H, Park J, Oh S, Bennis M, Kim S L. Federated knowledge distillation. 2020, arXiv preprint arXiv: 2011.02367 33. Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of 2012 IEEE Conference on Computer Vision and Pattern Recognition. 2012, 20662073 34. Peng X, Bai Q, Xia X, Huang Z, Saenko K, Wang B. Moment matching for multi-source domain adaptation. In: Proceedings of the IEEECVF International Conference on Computer Vision. 2019, 14061415 35. Kuznetsova A, Rom H, Alldrin N, Uijlings J, Krasin I, Pont-Tuset J, Kamali S, Popov S, Malloci M, Kolesnikov A, Duerig T, Ferrari V. The open images dataset V4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision, 2020, 128(7): 19561981 36. Cassara P, Gotta A, Valerio L. Federated feature selection for cyber- physical systems of systems. IEEE Transactions on Vehicular Technology, 2022, 71(9): 99379950 37. Darlow L N, Crowley E J, Antoniou A, Storkey A J. CINIC-10 is not ImageNet or CIFAR-10. 2018, arXiv preprint arXiv: 1810.03505 38. Liu Z, Luo P, Wang X, Tang X. Deep learning face attributes in the wild. In: Proceedings of the IEEE International Conference on Computer Vision. 2015, 37303738 39. Reddi S J, Charles Z, Zaheer M, Garrett Z, Rush K, Konečný J, Kumar S, McMahan H B. Adaptive federated optimization. In: Proceedings of the 9th International Conference on Learning Representations. 2021 40. Liu Q, Yang H, Dou Q, Heng P A. Federated semi-supervised medical image classification via inter-client relation matching. In: Proceedings of the 24th International Conference. 2021, 325335 41. Sattler F, Müller K R, Samek W. Clustered federated learning: model- agnostic distributed multitask optimization under privacy constraints. IEEE Transactions on Neural Networks and Learning Systems, 2021, 32(8): 37103722 42. Qayyum A, Ahmad K, Ahsan M A, Al-Fuqaha A, Qadir J. Collaborative federated learning for healthcare: multi-modal covid-19 diagnosis at the edge. IEEE Open Journal of the Computer Society, 2022, 3: 172184 43. Mansour Y, Mohri M, Ro J, Suresh A T. Three approaches for personalization with applications to federated learning. 2020, arXiv preprint arXiv: 2002.10619 44. Huang L, Shea A L, Qian H, Masurkar A, Deng H, Liu D. Patient clustering improves efficiency of federated machine learning to predict mortality and hospital stay time using distributed electronic medical records. Journal of Biomedical Informatics, 2019, 99: 103291 45. Ouyang X, Xie Z, Zhou J, Xing G, Huang J. ClusterFL: a clustering- based federated learning system for human activity recognition. ACM Transactions on Sensor Networks, 2023, 19(1): 17 46. Duan M, Liu D, Chen X, Liu R, Tan Y, Liang L. Self-balancing federated learning with global imbalanced data in mobile systems. IEEE Transactions on Parallel and Distributed Systems, 2021, 32(1): 5971 47. Zhang X, Mavromatics A, Vafeas A, Nejabati R, Simeonidou D. Federated feature selection for horizontal federated learning in IoT networks. IEEE Internet of Things Journal, 2023, 10(11): 1009510112 48. Hu Y, Zhang Y, Gao X, Gong D, Song X, Guo Y, Wang J. A federated feature selection algorithm based on particle swarm optimization under privacy protection. Knowledge-Based Systems, 2023, 260: 110122 49. Jeong W, Yoon J, Yang E, Hwang S J. Federated semi-supervised learning with inter-client consistency  disjoint learning. In: Proceedings of the 9th International Conference on Learning Representations. 2021 50. Li S, Zhou T, Tian X, Tao D. Learning to collaborate in decentralized learning of personalized models. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 97569765 51. Qi T, Wu F, Lyu L, Huang Y, Xie X. FedSampling: a better sampling strategy for federated learning. In: Proceedings of the 32nd International Joint Conference on Artificial Intelligence. 2023, 41544162 52. Chen M, Yang Z, Saad W, Yin C, Poor H V, Cui S. A joint learning and communications framework for federated learning over wireless networks. IEEE Transactions on Wireless Communications, 2021, 20(1): 269283 53. Deng Y, Lyu F, Ren J, Wu H, Zhou Y, Zhang Y, Shen X. AUCTION: automated and quality-aware client selection framework for efficient federated learning. IEEE Transactions on Parallel and Distributed Systems, 2022, 33(8): 19962009 54. Yang H H, Arafa A, Quek T Q S, Poor H V. Age-based scheduling policy for federated learning in mobile edge networks. In: Proceedings of 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2020, 87438747 55. Su L, Zhou R, Wang N, Fang G, Li Z. An online learning approach for client selection in federated edge learning under budget constraint. In: Proceedings of the 51st International Conference on Parallel Processing. 2022, 72 56. Wu C, Wu F, Lyu L, Huang Y, Xie X. Communication-efficient federated learning via knowledge distillation. Nature Communications, 2022, 13(1): 2032 57. Tuor T, Wang S, Ko B J, Liu C, Leung K K. Data selection for federated learning with relevant and irrelevant data at clients. 2020, arXiv preprint arXiv: 2001.08300 58. Duan M, Liu D, Ji X, Liu R, Liang L, Chen X, Tan Y. FedGroup: Efficient clustered federated learning via decomposed data-driven measure. 2020, arXiv preprint arXiv: 2010.06870 59. Nagalapatti L, Narayanam R. Game of gradients: mitigating irrelevant clients in federated learning. In: Proceedings of the 35th AAAI Conference on Artificial Intelligence. 2021, 90469054 60. Yoon T, Shin S, Hwang S J, Yang E. FedMix: approximation of Mixup under mean augmented federated learning. In: Proceedings of the 9th International Conference on Learning Representations. 2021 61. Hao W, El-Khamy M, Lee J, Zhang J, Liang K J, Chen C, Carin L. Towards fair federated learning with zero-shot data augmentation. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2021, 33053314 62. Liang P P, Liu T, Liu Z, Allen N B, Auerbach R P, Brent D,63. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 27 Salakhutdinov R, Morency L P. Think locally, act globally: federated learning with local and global representations. 2020, arXiv preprint arXiv: 2001.01523 Briggs C, Fan Z, Andras P. Federated learning with hierarchical clustering of local updates to improve training on non-IID data. In: Proceedings of 2020 International Joint Conference on Neural Networks (IJCNN). 2020, 19 64. Liu B, Guo Y, Chen X. PFA: privacy-preserving federated adaptation for effective model personalization. In: Proceedings of the Web Conference 2021. 2021, 923934 65. Tan Y, Long G, Liu L, Zhou T, Lu Q, Jiang J, Zhang C. FedProto: federated prototype learning across heterogeneous clients. In: Proceedings of the 36th AAAI Conference on Artificial Intelligence. 2022, 84328440 66. Shen T, Zhang J, Jia X, Zhang F, Huang G, Zhou P, Kuang K, Wu F, Wu C. Federated mutual learning. 2020, arXiv preprint arXiv: 2006.16765 67. Arivazhagan M G, Aggarwal V, Singh A K, Choudhary S. Federated learning with personalization layers. 2019, arXiv preprint arXiv: 1912.00818 68. Fallah A, Mokhtari A, Ozdaglar A. Personalized federated learning with theoretical guarantees: a model-agnostic meta-learning approach. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 300 69. Li T, Sahu A K, Talwalkar A, Smith V. Federated learning: challenges, methods, and future directions. IEEE Signal Processing Magazine, 2020, 37(3): 5060 70. Deng Y, Kamani M M, Mahdavi M. Adaptive personalized federated learning. 2020, arXiv preprint arXiv: 2003.13461 71. Dinh C T, Tran N H, Nguyen T D. Personalized federated learning with Moreau envelopes. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 1796 72. Hanzely F, Richtárik P. Federated learning of a mixture of global and local models. 2020, arXiv preprint arXiv: 2002.05516 73. Dinh C T, Tran N H, Nguyen T D, Bao W, Zomaya A Y, Zhou B B. Federated learning with proximal stochastic variance reduced gradient algorithms. In: Proceedings of the 49th International Conference on Parallel Processing. 2020, 48 74. Hanzely F, Hanzely S, Horváth S, Richtárik P. Lower bounds and optimal algorithms for personalized federated learning. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 194 75. Li T, Hu S, Beirami A, Smith V. Ditto: fair and robust federated learning through personalization. In: Proceedings of the 38th International Conference on Machine Learning. 2021, 63576368 76. Karimireddy S P, Kale S, Mohri M, Reddi S J, Stich S U, Suresh A T. SCAFFOLD: stochastic controlled averaging for federated learning. In: Proceedings of the 37th International Conference on Machine Learning. 2020, 476 77. Ma Z, Zhao M, Cai X, Jia Z. Fast-convergent federated learning with class-weighted aggregation. Journal of Systems Architecture, 2021, 117: 102125 78. Collins L, Hassani H, Mokhtari A, Shakkottai S. Exploiting shared representations for personalized federated learning. In: Proceedings of the 38th International Conference on Machine Learning. 2021, 20892099 79. Oh J, Kim S, Yun S Y. FedBABU: towards enhanced representation for federated image classification. 2021, arXiv preprint arXiv: 2106.06042 80. Jang J, Ha H, Jung D, Yoon S. FedClassAvg: local representation learning for personalized federated learning on heterogeneous neural networks. In: Proceedings of the 51st International Conference on 81. Parallel Processing. 2022, 76 Zhuang W, Gan X, Wen Y, Zhang S, Yi S. Collaborative unsupervised visual representation learning from decentralized data. In: Proceedings of the IEEECVF International Conference on Computer Vision. 2021, 48924901 82. Qu Z, Li X, Han X, Duan R, Shen C, Chen L. How to prevent the poor performance clients for personalized federated learning? In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 1216712176 83. Wang K, He Q, Chen F, Chen C, Huang F, Jin H, Yang Y. FlexiFed: personalized federated learning for edge clients with heterogeneous model architectures. In: Proceedings of the ACM Web Conference 2023. 2023, 29792990 84. Li A, Sun J, Zeng X, Zhang M, Li H, Chen Y. FedMask: joint computation and communication-efficient personalized federated learning via heterogeneous masking. In: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems. 2021, 4255 85. Yang Z, Sun Q. Personalized heterogeneity-aware federated search towards better accuracy and energy efficiency. In: Proceedings of the 41st IEEEACM International Conference on Computer Aided Design. 2022, 19 86. Diao E, Ding J, Tarokh V. HeteroFL: computation and communication efficient federated learning for heterogeneous clients. In: Proceedings of the 9th International Conference on Learning Representations. 2020 87. Zeng H, Zhou T, Guo Y, Cai Z, Liu F. FedCav: contribution-aware model aggregation on distributed heterogeneous data in federated learning. In: Proceedings of the 50th International Conference on Parallel Processing. 2021, 75 88. Zhu J, Ma X, Blaschko M B. Confidence-aware personalized federated learning via variational expectation maximization. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 2454224551 89. Lin X, Chen H, Xu Y, Xu C, Gui X, Deng Y, Wang Y. Federated learning with positive and unlabeled data. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1334413355 90. Beilharz J, Pfftzner B, Schmid R, Geppert P, Arnrich B, Polze A. Implicit model specialization through dag-based decentralized federated learning. In: Proceedings of the 22nd International Middleware Conference. 2021, 310322 91. Zhang M, Sapra K, Fidler S, Yeung S, Álvarez J M. Personalized federated learning with first order model optimization. In: Proceedings of the 9th International Conference on Learning Representations. 2021 92. Liu J, Wu J, Chen J, Hu M, Zhou Y, Wu D. FedDWA: personalized federated learning with dynamic weight adjustment. In: Proceedings of the 32nd International Joint Conference on Artificial Intelligence. 2023, 39934001 93. Wang H, Kaplan Z, Niu D, Li B. Optimizing federated learning on Non-IID data with reinforcement learning. In: Proceedings of the IEEE Conference on Computer Communications. 2020, 16981707 94. Nishio T, Yonetani R. Client selection for federated learning with heterogeneous resources in mobile edge. In: Proceedings of 2019 IEEE International Conference on Communications (ICC). 2019, 17 95. Wang L, Wang W, Li B. CMFL: mitigating communication overhead for federated learning. In: Proceedings of the 39th International Conference on Distributed Computing Systems (ICDCS). 2019, 954964 96. Xia W, Quek T Q S, Guo K, Wen W, Yang H H, Zhu H. Multi-armed bandit-based client scheduling for federated learning. IEEE Transactions on Wireless Communications, 2020, 19(11): 71087123 97. Yang M, Wang X, Zhu H, Wang H, Qian H. Federated learning with class imbalance reduction. In: Proceedings of the 29th European Signal Processing Conference (EUSIPCO). 2021, 21742178 98. 28 Front. Comput. Sci., 2024, 18(6): 186356 Chai Z, Ali A, Zawad S, Truex S, Anwar A, Baracaldo N, Zhou Y, Ludwig H, Yan F, Cheng Y. TiFL: a tier-based federated learning system. In: Proceedings of the 29th International Symposium on High- Performance Parallel and Distributed Computing. 2020, 125136 99. Li L, Duan M, Liu D, Zhang Y, Ren A, Chen X, Tan Y, Wang C. FedSAE: a novel self-adaptive federated learning framework in heterogeneous systems. In: Proceedings of the International Joint Conference on Neural Networks. 2021, 110 100. Cox B, Chen L Y, Decouchant J. Aergia: leveraging heterogeneity in federated learning systems. In: Proceedings of the 23rd ACMIFIP International Middleware Conference. 2022, 107120 101. Dong J, Wang L, Fang Z, Sun G, Xu S, Wang X, Zhu Q. Federated class-incremental learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 1015410163 102. Makhija D, Han X, Ho N, Ghosh J. Architecture agnostic federated learning for neural networks. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1486014870 103. Huang Y, Chu L, Zhou Z, Wang L, Liu J, Pei J, Zhang Y. Personalized cross-silo federated learning on non-IID data. In: Proceedings of the 35th AAAI Conference on Artificial Intelligence. 2021, 78657873 104. Zhu S, Qi Q, Zhuang Z, Wang J, Sun H, Liao J. FedNKD: a dependable federated learning using fine-tuned random noise and knowledge distillation. In: Proceedings of 2022 International Conference on Multimedia Retrieval. 2022, 185193 105. Chai Z, Chen Y, Anwar A, Zhao L, Cheng Y, Rangwala H. FedAT: a high-performance and communication-efficient federated learning system with asynchronous tiers. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021, 117 106. Hu C, Liang H H, Han X M, Liu B A, Cheng D Z, Wang D. Spread: decentralized model aggregation for scalable federated learning. In: Proceedings of the 51st International Conference on Parallel Processing. 2022, 75 107. Zhang X, Li Y, Li W, Guo K, Shao Y. Personalized federated learning via variational Bayesian inference. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 2629326310 108. Nguyen N H, Le Nguyen P, Nguyen T D, Nguyen T T, Nguyen D L, Nguyen T H, Pham H H, Truong T N. FedDRL: deep reinforcement learning-based adaptive aggregation for non-IID data in federated learning. In: Proceedings of the 51st International Conference on Parallel Processing. 2022, 73 109. Yoon J, Jeong W, Lee G, Yang E, Hwang S J. Federated continual learning with weighted inter-client transfer. In: Proceedings of the 38th International Conference on Machine Learning. 2021, 1207312086 110. Qu Z, Duan R, Chen L, Xu J, Lu Z, Liu Y. Context-Aware online client selection for hierarchical federated learning. IEEE Transactions on Parallel and Distributed Systems, 2022, 33(12): 43534367 111. Marfoq O, Neglia G, Vidal R, Kameni L. Personalized federated learning through local memorization. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1507015092 112. Huang W, Ye M, Du B, Gao X. Few-shot model agnostic federated learning. In: Proceedings of the 30th ACM International Conference on Multimedia. 2022, 73097316 113. Itahara S, Nishio T, Koda Y, Morikura M, Yamamoto K. Distillation- based semi-supervised federated learning for communication-efficient collaborative training with non-IID private data. IEEE Transactions on Mobile Computing, 2023, 22(1): 191205 114. Zhang J, Guo S, Guo J, Zeng D, Zhou J, Zomaya A Y. Towards data- independent knowledge transfer in model-heterogeneous federated learning. IEEE Transactions on Computers, 2023, 72(10): 28882901 115. Zhuang W, Wen Y, Zhang S. Divergence-aware federated self-116. supervised learning. In: Proceedings of the 10th ACM International Conference on Multimedia. 2022 Wang Y, Xu H, Ali W, Li M, Zhou X, Shao J. FedFTHA: a fine-tuning and head aggregation method in federated learning. IEEE Internet of Things Journal, 2023, 10(14): 1274912762 117. Gong X, Sharma A, Karanam S, Wu Z, Chen T, Doermann D, Innanje A. Preserving privacy in federated learning with ensemble cross- domain knowledge distillation. In: Proceedings of the 36th AAAI Conference on Artificial Intelligence. 2022, 1189111899 118. Lin T, Kong L, Stich S U, Jaggi M. Ensemble distillation for robust model fusion in federated learning. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 198 119. Lin H, Lou J, Xiong L, Shahabi C. SemiFed: semi-supervised federated learning with consistency and pseudo-labeling. 2021, arXiv preprint arXiv: 2108.09412 120. Lubana E S, Tang C I, Kawsar F, Dick R P, Mathur A. Orchestra: unsupervised federated learning via globally consistent clustering. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1446114484 121. Liang X, Lin Y, Fu H, Zhu L, Li X. RSCfed: random sampling consensus federated semi-supervised learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 1014410153 122. Li M, Li Q, Wang Y. Class balanced adaptive pseudo labeling for federated semi-supervised learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 1629216301 123. Zhang L, Wu D, Yuan X. FedZKT: zero-shot knowledge transfer towards resource-constrained federated learning with heterogeneous on-device models. In: Proceedings of the 42nd International Conference on Distributed Computing Systems (ICDCS). 2022, 928938 124. Zhou T, Konukoglu E. FedFA: federated feature augmentation. In: Proceedings of the 11th International Conference on Learning Representations. 2023 125. Yue K, Jin R, Pilgrim R, Wong C W, Baron D, Dai H. Neural tangent kernel empowered federated learning. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 2578325803 126. Long G, Xie M, Shen T, Zhou T, Wang X, Jiang J. Multi-center federated learning: clients clustering for better personalization. World Wide Web, 2023, 26(1): 481500 127. Li Q, He B, Song D. Model-contrastive federated learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2021, 1070810717 128. Chen H Y, Chao W L. On bridging generic and personalized federated learning for image classification. In: Proceedings of the 10th International Conference on Learning Representations. 2022 129. Yao X, Sun L. Continual local training for better initialization of federated models. In: Proceedings of 2020 IEEE International Conference on Image Processing (ICIP). 2020, 17361740 130. Wang J, Liu Q, Liang H, Joshi G, Poor H V. Tackling the objective inconsistency problem in heterogeneous federated optimization. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 638 131. Yu S, Nguyen P, Abebe W, Qian W, Anwar A, Jannesari A. SPATL: salient parameter aggregation and transfer learning for heterogeneous federated learning. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2022, 114 132. Liu C, Yang Y, Cai X, Ding Y, Lu H. Completely heterogeneous federated learning. 2022, arXiv preprint arXiv: 2210.15865 133. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 29 Ilhan F, Su G, Liu L. ScaleFL: Resource-adaptive federated learning with heterogeneous clients. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 2453224541 134. Liu C, Lou C, Wang R, Xi A Y, Shen L, Yan J. Deep neural network fusion via graph matching with applications to model ensemble and federated learning. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1385713869 135. Cho Y J, Wang J, Joshi G. Client selection in federated learning: convergence analysis and power-of-choice selection strategies. 2020, arXiv preprint arXiv: 2010.01243 136. Huang T, Lin W, Wu W, He L, Li K, Zomaya A Y. An efficiency- boosting client selection scheme for federated learning with fairness guarantee. IEEE Transactions on Parallel and Distributed Systems, 2021, 32(7): 15521564 137. Wang H, Li Y, Xu W, Li R, Zhan Y, Zeng Z. DaFKD: domain-aware federated knowledge distillation. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 2041220421 138. Gong X, Song L, Vedula R, Sharma A, Zheng M, Planche B, Innanje A, Chen T, Yuan J, Doermann D, Wu Z Y. Federated learning with privacy-preserving ensemble attention distillation. IEEE Transactions on Medical Imaging, 2023, 42(7): 20572067 139. Li Q, He B, Song D. Practical one-shot federated learning for cross- silo setting. In: Proceedings of the 30th International Joint Conference on Artificial Intelligence. 2021, 14841490 140. Sattler F, Marban A, Rischke R, Samek W. Communication-efficient federated distillation. 2020, arXiv preprint arXiv: 2012.00632 141. Zhu Z, Hong J, Zhou J. Data-free knowledge distillation for heterogeneous federated learning. In: Proceedings of the 38th International Conference on Machine Learning. 2021, 1287812889 142. Zhang L, Shen L, Ding L, Tao D, Duan L Y. Fine-tuning global model via data-free knowledge distillation for non-IID federated learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 1016410173 143. Yang Y, Yang R, Peng H, Li Y, Li T, Liao Y, Zhou P. FedACK: federated adversarial contrastive knowledge distillation for cross- lingual and cross-model social bot detection. In: Proceedings of the ACM Web Conference 2023. 2023, 13141323 144. Li G, Hu Y, Zhang M, Liu J, Yin Q, Peng Y, Dou D. FedHiSyn: a hierarchical synchronous federated learning framework for resource and data heterogeneity. In: Proceedings of the 51st International Conference on Parallel Processing. 2022, 8 145. Han S, Park S, Wu F, Kim S, Wu C, Xie X, Cha M. FedX: unsupervised federated learning with cross knowledge distillation. In: Proceedings of the 17th European Conference on Computer Vision. 2022, 691707 146. Li C, Zeng X, Zhang M, Cao Z. PyramidFL: a fine-grained client selection framework for efficient federated learning. In: Proceedings of the 28th Annual International Conference on Mobile Computing and Networking. 2022, 158171 147. Zhang S, Li Z, Chen Q, Zheng W, Leng J, Guo M. Dubhe: towards data unbiasedness with homomorphic encryption in federated learning client selection. In: Proceedings of the 50th International Conference on Parallel Processing. 2021, 83 148. Chen H, Frikha A, Krompass D, Gu J, Tresp V. FRAug: tackling federated learning with non-IID features via representation augmentation. In: Proceedings of the IEEECVF International Conference on Computer Vision. 2023, 48264836 149. Liu Q, Chen C, Qin J, Dou Q, Heng P A. FedDG: federated domain generalization on medical image segmentation via episodic learning in continuous frequency space. In: Proceedings of the IEEECVF 150. Conference on Computer Vision and Pattern Recognition. 2021, 10131023 Li X, Jiang M, Zhang X, Kamp M, Dou Q. FedBN: federated learning on non-IID features via local batch normalization. In: Proceedings of the 9th International Conference on Learning Representations. 2021 151. Yang D, Xu Z, Li W, Myronenko A, Roth H R, Harmon S, Xu S, Turkbey B, Turkbey E, Wang X, Zhu W, Carrafiello G, Patella F, Cariati M, Obinata H, Mori H, Tamura K, An P, Wood B J, Xu D. Federated semi-supervised learning for COVID region segmentation in chest CT using multi-national data from China, Italy, Japan. Medical Image Analysis, 2021, 70: 101992 152. Wang H, Zhao H, Wang Y, Yu T, Gu J, Gao J. FedKC: federated knowledge composition for multilingual natural language understanding. In: Proceedings of the ACM Web Conference 2022. 2022, 18391850 153. Wang K, Mathews R, Kiddon C, Eichner H, Beaufays F, Ramage D. Federated evaluation of on-device personalization. 2019, arXiv preprint arXiv: 1910.10252 154. Pillutla K, Malik K, Mohamed A, Rabbat M G, Sanjabi M, Xiao L. Federated learning with partial model personalization. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1771617758 155. Li A, Sun J, Li P, Pu Y, Li H, Chen Y. Hermes: an efficient federated learning framework for heterogeneous mobile clients. In: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking. 2021, 420437 156. Zhuang W, Wen Y, Zhang S. Joint optimization in edge-cloud continuum for federated unsupervised person re-identification. In: Proceedings of the 29th ACM International Conference on Multimedia. 2021, 433441 157. Zhang R, Xu Q, Yao J, Zhang Y, Tian Q, Wang Y. Federated domain generalization with generalization adjustment. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 39543963 158. Ruan Y, Joe-Wong C. FedSoft: soft clustered federated learning with proximal local updating. In: Proceedings of the 36th AAAI Conference on Artificial Intelligence. 2022, 81248131 159. Xie H, Xiong L, Yang C. Federated node classification over graphs with latent link-type heterogeneity. In: Proceedings of the ACM Web Conference 2023. 2023, 556566 160. Donahue K, Kleinberg J M. Optimality and stability in federated learning: a game-theoretic approach. In: Proceedings of the International Conference on Neural Information Processing Systems. 2021, 12871298 161. Dai Z, Low B K H, Jaillet P. Federated Bayesian optimization via Thompson sampling. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 812 162. Li D, Wang J. FedMD: Heterogenous federated learning via model distillation. 2019, arXiv preprint arXiv: 1910.03581 163. Li Y, Zhou W, Wang H, Mi H, Hospedales T M. FedH2L: federated learning with model and statistical heterogeneity. 2021, arXiv preprint arXiv: 2101.11296 164. Wu Y, Kang Y, Luo J, He Y, Fan L, Pan R, Yang Q. FedCG: leverage conditional GAN for protecting privacy and maintaining competitive performance in federated learning. In: Proceedings of the 31st International Joint Conference on Artificial Intelligence. 2022, 23342340 165. Niu Z, Wang H, Sun H, Ouyang S, Chen Y W, Lin L. MCKD: Mutually collaborative knowledge distillation for federated domain adaptation and generalization. In: Proceedings of 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2023, 15 166. 30 Front. Comput. Sci., 2024, 18(6): 186356 Huang W, Ye M, Shi Z, Li H, Du B. Rethinking federated learning with domain shift: a prototype view. In: Proceedings of 2023 IEEECVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023, 1631216322 167. Wang H, Yurochkin M, Sun Y, Papailiopoulos D S, Khazaeni Y. Federated learning with matched averaging. In: Proceedings of the 8th International Conference on Learning Representations. 2020 168. Ghosh A, Chung J, Yin D, Ramchandran K. An efficient framework for clustered federated learning. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 1643 169. Tu L, Ouyang X, Zhou J, He Y, Xing G. FedDL: federated learning via dynamic layer sharing for human activity recognition. In: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems. 2021, 1528 170. Donahue K, Kleinberg J. Model-sharing games: Analyzing federated learning under voluntary participation. In: Proceedings of the 35th AAAI Conference on Artificial Intelligence. 2021, 53035311 171. Gao D, Liu Y, Huang A, Ju C, Yu H, Yang Q. Privacy-preserving heterogeneous federated transfer learning. In: Proceedings of 2019 IEEE International Conference on Big Data (Big Data). 2019, 25522559 172. Kang Y, He Y, Luo J, Fan T, Liu Y, Yang Q. Privacy-preserving federated adversarial domain adaptation over feature groups for interpretability. IEEE Transactions on Big Data, 2022, doi: 10.1109TBDATA.2022.3188292 173. Fu R, Wu Y, Xu Q, Zhang M. FEAST: a communication-efficient federated feature selection framework for relational data. Proceedings of the ACM on Management of Data, 2023, 1(1): 107 174. Banerjee S, Elmroth E, Bhuyan M. Fed-FiS: a novel information- theoretic federated feature selection for learning stability. In: Proceedings of the 28th International Conference on Neural Information Processing. 2021, 480487 175. Wu Z, Li Q, He B. Practical vertical federated learning with unsupervised representation learning. IEEE Transactions on Big Data, 2022 176. He Y, Kang Y, Zhao X, Luo J, Fan L, Han Y, Yang Q. A hybrid self- supervised learning framework for vertical federated learning. 2022, arXiv preprint arXiv: 2208.08934 177. Feng S, Yu H. Multi-participant multi-class vertical federated learning. 2020, arXiv preprint arXiv: 2001.11154 178. Feng S. Vertical federated learning-based feature selection with non- overlapping sample utilization. Expert Systems with Applications, 2022, 208: 118097 179. Jiang J, Burkhalter L, Fu F, Ding B, Du B, Hithnawi A, Li B, Zhang C. VF-PS: how to select important participants in vertical federated learning, efficiently and securely? In: Proceedings of the 36th International Conference on Neural Information Processing Systems. 2022, 152 180. Castiglia T, Zhou Y, Wang S, Kadhe S, Baracaldo N, Patterson S. LESS-VFL: Communication-efficient feature selection for vertical federated learning. In: Proceedings of the 40th International Conference on Machine Learning. 2023, 37573781 181. Kairouz P, McMahan H B, Avent B, Bellet A, Bennis M, et al. Advances and open problems in federated learning. Foundations and Trends in Machine Learning, 2021, 14(12): 1210 182. Chai Z, Fayyaz H, Fayyaz Z, Anwar A, Zhou Y, Baracaldo N, Ludwig H, Cheng Y. Towards taming the resource and data heterogeneity in federated learning. In: Proceedings of 2019 USENIX Conference on Operational Machine Learning. 2019, 1921 183. Ye M, Fang X, Du B, Yuen P C, Tao D. Heterogeneous federated learning: state-of-the-art and research challenges. ACM Computing 184. Surveys, 2024, 56(3): 79 Schlegel R, Kumar S, Rosnes E, Amat A G I. CodedPaddedFL and CodedSecAgg: Straggler mitigation and secure aggregation in federated learning. IEEE Transactions on Communications, 2023, 71(4): 20132027 185. You C, Xiang J, Su K, Zhang X, Dong S, Onofrey J, Staib L, Duncan J S. Incremental learning meets transfer learning: application to multi- site prostate MRI segmentation. In: Proceedings of the 3rd MICCAI Workshop on Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health. 2022, 316 186. Chen Y, Qin X, Wang J, Yu C, Gao W. FedHealth: a federated transfer learning framework for wearable healthcare. IEEE Intelligent Systems, 2020, 35(4): 8393 187. Ditzler G, Polikar R. Incremental learning of concept drift from streaming imbalanced data. IEEE Transactions on Knowledge and Data Engineering, 2013, 25(10): 22832301 188. Elwell R, Polikar R. Incremental learning of concept drift in nonstationary environments. IEEE Transactions on Neural Networks, 2011, 22(10): 15171531 189. Tan D S, Lin Y X, Hua K L. Incremental learning of multi-domain image-to-image translations. IEEE Transactions on Circuits and Systems for Video Technology, 2021, 31(4): 15261539 190. Huang Y, Bert C, Gomaa A, Fietkau R, Maier A, Putz F. A survey of incremental transfer learning: combining peer-to-peer federated learning and domain incremental learning for multicenter collaboration. 2023, arXiv preprint arXiv: 2309.17192 191. Tang J, Lin K Y, Li L. Using domain adaptation for incremental SVM classification of drift data. Mathematics, 2022, 10(19): 3579 192. Alam S, Liu L, Yan M, Zhang M. FedRolex: model-heterogeneous federated learning with rolling sub-model extraction. In: Proceedings of the 36th International Conference on Neural Information Processing Systems. 2022, 2152 193. Yu F, Zhang W, Qin Z, Xu Z, Wang D, Liu C, Tian Z, Chen X. Heterogeneous federated learning. 2020, arXiv preprint arXiv: 2008.06767 194. Jin Y, Wei X, Liu Y, Yang Q. Towards utilizing unlabeled data in federated learning: a survey and prospective. 2020, arXiv preprint arXiv: 2002.11545 195. Diao E, Ding J, Tarokh V. SemiFL: semi-supervised federated learning for unlabeled clients with alternate training. In: Proceedings of the 36th International Conference on Neural Information Processing Systems. 2022, 1299 196. Shin J, Li Y, Liu Y, Lee S J. FedBalancer: data and pace control for efficient federated learning on heterogeneous clients. In: Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services. 2022, 436449 197. Pilla L L. Optimal task assignment for heterogeneous federated learning devices. In: Proceedings of 2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS). 2021, 661670 198. Zhang F, Kuang K, You Z, Shen T, Xiao J, Zhang Y, Wu C, Zhuang Y, Li X. Federated unsupervised representation learning. 2020, arXiv preprint arXiv: 2010.08982 199. Liao Y, Ma L, Zhou B, Zhao X, Xie F. DraftFed: a draft-based personalized federated learning approach for heterogeneous convolutional neural networks. IEEE Transactions on Mobile Computing, 2024, 23(5): 39383949 200. Liu Y, Guo S, Zhang J, Zhou Q, Wang Y, Zhao X. Feature correlation- guided knowledge transfer for federated self-supervised learning. 2022, arXiv preprint arXiv: 2211.07364 201. Li T, Sahu A K, Zaheer M, Sanjabi M, Talwalkar A, Smith V. Federated optimization in heterogeneous networks. In: Proceedings of 202. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 31 the Machine Learning and Systems. 2020, 429450 Duan J H, Li W, Zou D, Li R, Lu S. Federated learning with data- agnostic distribution fusion. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 80748083 203. Liu X, Xi W, Li W, Xu D, Bai G, Zhao J. Co-MDA: federated multisource domain adaptation on black-box models. IEEE Transactions on Circuits and Systems for Video Technology, 2023, 33(12): 76587670 204. Yoon J, Park G, Jeong W, Hwang S J. Bitwidth heterogeneous federated learning with progressive weight dequantization. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 2555225565 205. Dai Z, Low B K H, Jaillet P. Differentially private federated Bayesian optimization with distributed exploration. In: Proceedings of the International Conference on Neural Information Processing Systems. 2021, 91259139 206. Zhu H, Wang X, Jin Y. Federated many-task Bayesian optimization. IEEE Transactions on Evolutionary Computation, 2023 207. Chawla N V, Bowyer K W, Hall L O, Kegelmeyer W P. SMOTE: synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 2002, 16: 321357 208. He H, Bai Y, Garcia E A, Li S. ADASYN: Adaptive synthetic sampling approach for imbalanced learning. In: Proceedings of 2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence). 2008, 13221328 209. Han H, Wang W Y, Mao B H. Borderline-smote: a new over-sampling method in imbalanced data sets learning. In: Proceedings of the International Conference on Intelligent Computing. 2005, 878887 210. Kubat M, Matwin S. Addressing the curse of imbalanced training sets: one-sided selection. In: Proceedings of the 14th International Conference on Machine Learning. 1997, 179186 211. Yen S J, Lee Y S. Cluster-based under-sampling approaches for imbalanced data distributions. Expert Systems with Applications, 2009, 36(3): 57185727 212. Tsai C F, Lin W C, Hu Y H, Yao G T. Under-sampling class imbalanced datasets by combining clustering analysis and instance selection. Information Sciences, 2019, 477: 4754 213. Zhang L, Li Y, Xiao X, Li X Y, Wang J, Zhou A, Li Q. CrowdBuy: privacy-friendly image dataset purchasing via crowdsourcing. In: Proceedings of the IEEE Conference on Computer Communications. 2018, 27352743 214. Li A, Zhang L, Qian J, Xiao X, Li X Y, Xie Y. TODQA: efficient task- oriented data quality assessment. In: Proceedings of the 15th International Conference on Mobile Ad-Hoc and Sensor Networks (MSN). 2019, 8188 215. Katharopoulos A, Fleuret F. Not all samples are created equal: deep learning with importance sampling. In: Proceedings of the 35th International Conference on Machine Learning. 2018, 25302539 216. Alain G, Lamb A, Sankar C, Courville A, Bengio Y. Variance reduction in SGD by distributed importance sampling. 2015, arXiv preprint arXiv: 1511.06481 217. Loshchilov I, Hutter F. Online batch selection for faster training of neural networks. 2015, arXiv preprint arXiv: 1511.06343 218. Schaul T, Quan J, Antonoglou I, Silver D. Prioritized experience replay. In: Proceedings of the 4th International Conference on Learning Representations. 2016 219. Wu C Y, Manmatha R, Smola A J, Krahenbuhl P. Sampling matters in deep embedding learning. In: Proceedings of the IEEE International Conference on Computer Vision. 2017, 28592867 220. Li K, Xiao C. CBFL: a communication-efficient federated learning framework from data redundancy perspective. IEEE Systems Journal, 221. 2022, 16(4): 55725583 Duan L, Tsang I W, Xu D, Chua T S. Domain adaptation from multiple sources via auxiliary classifiers. In: Proceedings of the 26th Annual International Conference on Machine Learning. 2009, 289296 222. Duan L, Xu D, Tsang I W H. Domain adaptation from multiple sources: a domain-dependent regularization approach. IEEE Transactions on Neural Networks and Learning Systems, 2012, 23(3): 504518 223. Zhuang F, Luo P, Xiong H, Xiong Y, He Q, Shi Z. Cross-domain learning from multiple sources: a consensus regularization perspective. IEEE Transactions on Knowledge and Data Engineering, 2010, 22(12): 16641678 224. Luo P, Zhuang F, Xiong H, Xiong Y, He Q. Transfer learning from multiple source domains via consensus regularization. In: Proceedings of the 17th ACM Conference on Information and Knowledge Management. 2008, 103112 225. Kirkpatrick J, Pascanu R, Rabinowitz N, Veness J, Desjardins G, Rusu A A, Milan K, Quan J, Ramalho T, Grabska-Barwinska A, Hassabis D, Clopath C, Kumaran D, Hadsell R. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences of the United States of America, 2017, 114(13): 35213526 226. Yu H, Zhang N, Deng S, Yuan Z, Jia Y, Chen H. The devil is the classifier: investigating long tail relation classification with decoupling analysis. 2020, arXiv preprint arXiv: 2009.07022 227. Kang B, Xie S, Rohrbach M, Yan Z, Gordo A, Feng J, Kalantidis Y. Decoupling representation and classifier for long-tailed recognition. In: Proceedings of the 8th International Conference on Learning Representations. 2020 228. Yosinski J, Clune J, Bengio Y, Lipson H. How transferable are features in deep neural networks? In: Proceedings of the 27th International Conference on Neural Information Processing Systems. 2014, 33203328 229. Devlin J, Chang M W, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language understanding. In: Proceedings of 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2019, 41714186 230. Li X, Huang K, Yang W, Wang S, Zhang Z. On the convergence of FedAvg on non-IID data. In: Proceedings of the 8th International Conference on Learning Representations. 2020 231. Yamada Y, Lindenbaum O, Negahban S, Kluger Y. Feature selection using stochastic gates. In: Proceedings of the 37th International Conference on Machine Learning. 2020, 987 232. Zhou K, Yang Y, Qiao Y, Xiang T. Domain generalization with mixstyle. In: Proceedings of the 9th International Conference on Learning Representations. 2021 233. Li Q, Huang J, Hu J, Gong S. Feature-distribution perturbation and calibration for generalized Reid. In: Proceedings of 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2024, 28802884 234. Huang X, Belongie S. Arbitrary style transfer in real-time with adaptive instance normalization. In: Proceedings of the IEEE International Conference on Computer Vision. 2017, 15101519 235. Han B, Yao Q, Yu X, Niu G, Xu M, Hu W, Tsang I W, Sugiyama M. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In: Proceedings of the 32nd International Conference on Neural Information Processing Systems. 2018, 85368546 236. Liu Y, Kang Y, Xing C, Chen T, Yang Q. A secure federated transfer learning framework. IEEE Intelligent Systems, 2020, 35(4): 7082 237. Yang H, He H, Zhang W, Cao X. FedSteg: a federated transfer learning framework for secure image steganalysis. IEEE Transactions 238. 32 Front. Comput. Sci., 2024, 18(6): 186356 on Network Science and Engineering, 2021, 8(2): 10841094 Li Y, Chen C Y, Wasserman W W. Deep feature selection: theory and application to identify enhancers and promoters. Journal of Computational Biology, 2016, 23(5): 322336 239. Louizos C, Welling M, Kingma D P. Learning sparse neural networks through L0 regularization. In: Proceedings of the 6th International Conference on Learning Representations. 2018 240. Chang H, Shejwalkar V, Shokri R, Houmansadr A. Cronus: robust and heterogeneous collaborative learning with black-box knowledge transfer. 2019, arXiv preprint arXiv: 1912.11279 241. Van Berlo B, Saeed A, Ozcelebi T. Towards federated unsupervised representation learning. In: Proceedings of the 3rd ACM International Workshop on Edge Systems, Analytics and Networking. 2020, 3136 242. Liu J, Zhao P, Zhuang F, Liu Y, Sheng V S, Xu J, Zhou X, Xiong H. Exploiting aesthetic preference in deep cross networks for cross- domain recommendation. In: Proceedings of the Web Conference 2020. 2020, 27682774 243. Li P, Tuzhilin A. DDTCDR: deep dual transfer cross domain recommendation. In: Proceedings of the 13th International Conference on Web Search and Data Mining. 2020, 331339 244. Ammad-Ud-Din M, Ivannikova E, Khan S A, Oyomno W, Fu Q, Tan K E, Flanagan A. Federated collaborative filtering for privacy- preserving personalized recommendation system. 2019, arXiv preprint arXiv: 1901.09888 245. Minto L, Haller M, Livshits B, Haddadi H. Stronger privacy for federated collaborative filtering with implicit feedback. In: Proceedings of the 15th ACM Conference on Recommender Systems. 2021, 342350 246. Chai D, Wang L, Chen K, Yang Q. Secure federated matrix factorization. IEEE Intelligent Systems, 2021, 36(5): 1120 247. Du Y, Zhou D, Xie Y, Shi J, Gong M. Federated matrix factorization for privacy-preserving recommender systems. Applied Soft Computing, 2021, 111: 107700 248. Li Z, Ding B, Zhang C, Li N, Zhou J. Federated matrix factorization with privacy guarantee. Proceedings of the VLDB Endowment, 2021, 15(4): 900913 249. Wu C, Wu F, Cao Y, Huang Y, Xie X. FedGNN: federated graph neural network for privacy-preserving recommendation. 2021, arXiv preprint arXiv: 2102.04925 250. Zhang C, Long G, Zhou T, Yan P, Zhang Z, Zhang C, Yang B. Dual personalization on federated recommendation. In: Proceedings of the 32nd International Joint Conference on Artificial Intelligence. 2023, 45584566 251. Wu J, Liu Q, Huang Z, Ning Y, Wang H, Chen E, Yi J, Zhou B. Hierarchical personalized federated learning for user modeling. In: Proceedings of the Web Conference 2021. 2021, 957968 252. Wu M, Li L, Chang T, Rigall E, Wang X, Xu C Z. FedCDR: federated cross-domain recommendation for privacy-preserving rating prediction. In: Proceedings of the 31st ACM International Conference on Information  Knowledge Management. 2022, 21792188 253. Luo S, Xiao Y, Song L. Personalized federated recommendation via joint representation learning, user clustering, and model adaptation. In: Proceedings of the 31st ACM International Conference on Information  Knowledge Management. 2022, 42894293 254. Kaissis G A, Makowski M R, Rückert D, Braren R F. Secure, privacy- preserving and federated machine learning in medical imaging. Nature Machine Intelligence, 2020, 2(6): 305311 255. Sui D, Chen Y, Zhao J, Jia Y, Xie Y, Sun W. FedED: federated learning via ensemble distillation for medical relation extraction. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020, 21182128 256. Silva S, Gutman B A, Romero E, Thompson P M, Altmann A, Lorenzi257. M. Federated learning in distributed medical databases: meta-analysis of large-scale subcortical brain data. In: Proceedings of the 16th IEEE international symposium on biomedical imaging (ISBI 2019). 2019, 270274 Jin H, Dai X, Xiao J, Li B, Li H, Zhang Y. Cross-cluster federated learning and blockchain for internet of medical things. IEEE Internet of Things Journal, 2021, 8(21): 1577615784 258. Xia Y, Yang D, Li W, Myronenko A, Xu D, Obinata H, Mori H, An P, Harmon S, Turkbey E, Turkbey B, Wood B, Patella F, Stellato E, Carrafiello G, Ierardi A, Yuille A, Roth H. Auto-FedAvg: learnable federated averaging for multi-institutional medical image segmentation. 2021, arXiv preprint arXiv: 2104.10195 259. Jiang M, Roth H R, Li W, Yang D, Zhao C, Nath V, Xu D, Dou Q, Xu Z. Fair federated medical image segmentation via client contribution estimation. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 1630216311 260. Acar D A E, Zhao Y, Navarro R M, Mattina M, Whatmough P N, Saligrama V. Federated learning based on dynamic regularization. In: Proceedings of the 9th International Conference on Learning Representations. 2021 261. Chen Z, Yang C, Zhu M, Peng Z, Yuan Y. Personalized retrogress- resilient federated learning toward imbalanced medical data. IEEE Transactions on Medical Imaging, 2022, 41(12): 36633674 262. Xu A, Li W, Guo P, Yang D, Roth H, Hatamizadeh A, Zhao C, Xu D, Huang H, Xu Z. Closing the generalization gap of cross-silo federated medical image segmentation. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 2083420843 263. Jiang M, Yang H, Cheng C, Dou Q. IOP-FL: Inside-outside personalization for federated medical image segmentation. IEEE Transactions on Medical Imaging, 2023, 42(7): 21062117 264. Wang J, Jin Y, Stoyanov D, Wang L. FedDP: dual personalization in federated medical image segmentation. IEEE Transactions on Medical Imaging, 2024, 43(1): 297308 265. Adnan M, Kalra S, Cresswell J C, Taylor G W, Tizhoosh H R. Federated learning and differential privacy for medical image analysis. Scientific Reports, 2022, 12(1): 1953 266. Wu Y, Zeng D, Wang Z, Shi Y, Hu J. Federated contrastive learning for volumetric medical image segmentation. In: Proceedings of the 24th International Conference on Medical Image Computing and Computer Assisted Intervention. 2021, 367377 267. Li Y, Wen G. Research and practice of financial credit risk management based on federated learning. Engineering Letters, 2023, 31(1): 268. Xu Z, Cheng J, Cheng L, Xu X, Bilal M. Mses credit risk assessment model based on federated learning and feature selection. Computers, Materials  Continua, 2023, 75(3): 55735595 269. Lee C M, Delgado Fernández J, Potenciano Menci S, Rieger A, Fridgen G. Federated learning for credit risk assessment. In: Proceedings of the 56th Hawaii International Conference on System Sciences. 2023, 386395 270. Yang W, Zhang Y, Ye K, Li L, Xu C Z. FFD: a federated learning based method for credit card fraud detection. In: Proceedings of the 8th International Congress on Big Data. 2019, 1832 271. Wang Z, Xiao J, Wang L, Yao J. A novel federated learning approach with knowledge transfer for credit scoring. Decision Support Systems, 2024, 177: 114084 272. Pourroostaei Ardakani S, Du N, Lin C, Yang J C, Bi Z, Chen L. A federated learning-enabled predictive analysis to forecast stock market trends. Journal of Ambient Intelligence and Humanized Computing, 2023, 14(4): 45294535 273. Yan Y, Yang G, Gao Y, Zang C, Chen J, Wang Q. Multi-participant274. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 33 vertical federated learning based time series prediction. In: Proceedings of the 8th International Conference on Computing and Artificial Intelligence. 2022, 165171 Shaheen M, Farooq M S, Umer T. Reduction in data imbalance for client-side training in federated learning for the prediction of stock market prices. Journal of Sensor and Actuator Networks, 2024, 13(1): 1 275. Myalil D, Rajan M A, Apte M, Lodha S. Robust collaborative fraudulent transaction detection using federated learning. In: Proceedings of the 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA) . 2021, 373378 276. Abadi A, Doyle B, Gini F, Guinamard K, Murakonda S K, Liddell J, Mellor P, Murdoch S J, Naseri M, Page H, Theodorakopoulos G, Weller S. Starlit: Privacy-preserving federated learning to enhance financial fraud detection . 2024, arXiv preprint arXiv: 2401.10765 277. Liu Y, Yu J J Q, Kang J, Niyato D, Zhang S. Privacy-preserving traffic flow prediction: a federated learning approach. IEEE Internet of Things Journal, 2020, 7(8): 77517763 278. Zhang C, Dang S, Shihada B, Alouini M S. Dual attention-based federated learning for wireless traffic prediction. In: Proceedings of the IEEE Conference on Computer Communications. 2021, 110 279. Zeng T, Guo J, Kim K J, Parsons K, Orlik P, Di Cairano S, Saad W . Multi-task federated learning for traffic prediction and its application to route planning. In: Proceedings of 2021 IEEE Intelligent Vehicles Symposium (IV) . 2021, 451457 280. Zhang C, Cui L, Yu S, Yu J J Q. A communication-efficient federated learning scheme for IoT-based traffic forecasting . IEEE Internet of Things Journal, 2022, 9(14): 1191811931 281. Qi T, Chen L, Li G, Li Y, Wang C. FedAGCN: a traffic flow prediction framework based on federated learning and asynchronous graph convolutional network. Applied Soft Computing , 2023, 138: 110175 282. Phyu H P, Stanica R, Naboulsi D . Multi-slice privacy-aware traffic forecasting at ran level: a scalable federated-learning approach. IEEE Transactions on Network and Service Management, 2023, 20(4): 50385052 283. Xia M, Jin D, Chen J . Short-term traffic flow prediction based on graph convolutional networks and federated learning. IEEE Transactions on Intelligent Transportation Systems, 2023, 24(1): 11911203 284. Zhang L, Zhang C, Shihada B . Efficient wireless traffic prediction at the edge: a federated meta-learning approach . IEEE Communications Letters, 2022, 26(7): 15731577 285. Hu S, Ye Y, Hu Q, Liu X, Cao S, Yang H H, Shen Y, Angeloudis P, Parada L, Wu C. A federated learning-based framework for ride- sourcing traffic demand prediction. IEEE Transactions on Vehicular Technology , 2023, 72(11): 1400214015 286. Huo J T, XU Y W, Huo Z S, Xiao L M, He Z X. Research on key technologies of edge cache in virtual data space across WAN. Frontiers of Computer Science , 2023, 17(1): 171102. 287. Li H Z, Jin H, Zheng L, Huang Y, Liao X F. ReCSA: a dedicated sort accelerator using ReRAM-based content addressable memory. Frontiers of Computer Science, 2023, 17(2): 172103. 288. Jia J, Liu Y, Zhang G Z, Gao Y L, Qian D P. Software approaches for resilience of high performance computing systems: a survey. Frontiers of Computer Science , 2023, 17(4): 174105. 289. Guo J Y, Zhang L, ROMERO HUNG J, Li C, Zhao J R, Guo M Y . FPGA sharing in the cloud: a comprehensive analysis. Frontiers of Computer Science , 2023, 17(5): 175106. 290. Wei Guo is a PhD student at the Institute of Artificial Intelligence, Beihang University, China. She received his MSc degree from the School of Electronics and Computer Science at Southampton University, UK. Her research interests primarily lie in federated learning and transfer learning. Fuzhen Zhuang received the BE degree from the College of Computer Science, Chongqing University, China in 2006, and the PhD degree in computer science from the Institute of Computing Technology, Chinese Academy of Sciences, China in 2011. He is currently a full professor with the Institute of Artificial Intelligence, Beihang University, China. He has published more than 140 papers in some prestigious refereed journals and conference proceedings. His research interests include transfer learning, machine learning, data mining, multitask learning, knowledge graph, and recommendation systems. He is a senior member of the CCF. He was a recipient of the Distinguished Dissertation Award of CAAI in 2013. Xiao Zhang is an associate professor at the School of Computer Science and Technology, Shandong University, China. His research interests include distributed learning, federated learning, edge intelligence, and data mining. He has published more than 20 papers in prestigious refereed journals and conference proceedings, such as IEEE TKDE, TMC, UBICOMP, SIGKDD, SIGIR, IJCAI, ACM CIKM, and IEEE ICDM. Yiqi Tong is a PhD student at the School of Computer Science and Engineering, Beihang University, China. He received his MSc degree from the School of Informatics at Xiamen University, China. His research interests primarily lie in natural language processing and recommendation systems. Jin Dong is the General Director of Beijing Academy of Blockchain and Edge Computing (BABEC), Director of National Blockchain Technology Innovation Center, Director of Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing. He has been dedicated to technical research in the fields of blockchain, privacy computing, chip design, etc. The team led by him developed the first of kind high performance hardware -software integrated blockchain system - ChainMaker around the globe, aiming to break through the performance and security bottlenecks of large- scale blockchain applications. This has been widely adopted by a variety of key economic and industrial applications in China. Jin Dong received his PhD degree from Tsinghua University, China and has filed more than forty US patents. 34 Front. Comput. Sci., 2024, 18(6): 186356",
    "page_start": null,
    "page_end": null,
    "word_count": 29122,
    "created_at": "2025-08-18T06:55:15",
    "updated_at": "2025-08-18T06:55:15"
  },
  {
    "id": "e0e4ca17bfaa49539edf66e7cbe6d597",
    "doc_id": "e26f3a6733224b559b969854425f3057",
    "doc_name": "Transfer_learning_handbook.pdf",
    "heading": "Document",
    "content": "T ransfer Learning Lisa T orrey and Jude Shavlik University of Wisconsin, Madison WI, USA Abstract. T ransfer learning is the improvement of learning in a new task through the transfer of knowledge from a related task that has al- ready been learned. While most machine learning algorithms are designe d to address single tasks, the development of algorithms that facilitate transfer learning is a topic of ongoing interest in the machine-learni ng community . This chapter provides an introduction to the goals, formu - lations, and challenges of transfer learning. It surveys current rese arch in this area, giving an overview of the state of the art and outlining the open problems. The survey covers transfer in both inductive learn ing and reinforcement learning, and discusses the issues of negative transf er and task mapping in depth. INTRODUCTION Human learners appear to have inherent ways to transfer knowledge between tasks. That is, we recognize and apply relevant knowledge from previous learning experiences when we encounter new tasks. The more related a new task is to our previous experience, the more easily we can master it. Common machine learning algorithms, in contrast, traditionally address iso- lated tasks. Transfer learning attempts to change this by developing methods to transfer knowledge learned in one or more source tasks and use it to improve learning in a related target task (see Figure 1). T echniques that enable knowl- edge transfer represent progress towards making machine learning as eﬃcient as human learning. This chapter provides an introduction to the goals, formulations, and chal- lenges of transfer learning. It surveys current research in this area, giving an overview of the state of the art and outlining the open problems. T ransfer methods tend to be highly dependent on the machine learning al- gorithms being used to learn the tasks, and can often simply be considered extensions of those algorithms. Some work in transfer learning is in the cont ext of inductive learning, and involves extending well-known classiﬁcation and infer- ence algorithms such as neural networks, Bayesian networks, and Markov Logi c Networks. Another major area is in the context of reinforcement learning, and involves extending algorithms such as Q-learning and policy search. This chapter surveys these areas separately . Appears in the Handbook of Research on Machine Learning Applications , published by IGI Global, edited by E. Soria, J. Martin, R. Magdalena, M. Martinez and A. Serrano, 2009. 1 Given Data Source-Task Knowledge Learn Target Task Fig. 1. T ransfer learning is machine learning with an additional source of infor mation apart from the standard training data: knowledge from one or more related task s. The goal of transfer learning is to improve learning in the target task by leveraging knowledge from the source task. There are three common measures by which transfer might improve learning. First is the initial performance a chievable in the target task using only the transferred knowledge, before any further learn- ing is done, compared to the initial performance of an ignorant agent. Second is the amount of time it takes to fully learn the target task given the transf erred knowledge compared to the amount of time to learn it from scratch. Third is t he ﬁnal performance level achievable in the target task compared to the ﬁnal level without transfer. Figure 2 illustrates these three measures. If a transfer method actually decreases performance, then negative transfer has occurred. One of the major challenges in developing transfer methods is to produce positive transfer between appropriately related tasks while avoidi ng negative transfer between tasks that are less related. A section of this chapter discusses approaches for avoiding negative transfer. When an agent applies knowledge from one task in another, it is often nec- essary to map the characteristics of one task onto those of the other to speci fy correspondences. In much of the work on transfer learning, a human provides this mapping, but some methods provide ways to perform the mapping auto- matically . Another section of the chapter discusses work in this area. performance training with transfer without transfer higher start higher slope higher asymptote Fig. 2. Three ways in which transfer might improve learning. 2 Source Task Target Task Task 1 Task 2 Task 3 Task 4 Transfer Learning Multi-task Learning Fig. 3.As we deﬁne transfer learning, the information ﬂows in one direction on ly , from the source task to the target task. In multi-task learning, information c an ﬂow freely among all tasks. W e will make a distinction between transfer learning and multi-task learn- ing [5], in which several tasks are learned simultaneously (see Figure 3). Mult i- task learning is clearly closely related to transfer, but it does not involve des- ignated source and target tasks; instead the learning agent receives informatio n about several tasks at once. In contrast, by our deﬁnition of transfer learning, the agent knows nothing about a target task (or even that there will be a target task) when it learns a source task. It may be possible to approach a multi-t ask learning problem with a transfer-learning method, but the reverse is not possi- ble. It is useful to make this distinction because a learning agent in a real-world setting is more likely to encounter transfer scenarios than multi-task scenarios . TRANSFER IN INDUCTIVE LEARNING In an inductive learning task, the objective is to induce a predictive model from a set of training examples [28]. Often the goal is classiﬁcation, i.e. assigning class la- bels to examples. Examples of classiﬁcation systems are artiﬁcial neural netwo rks and symbolic rule-learners. Another type of inductive learning involves model- ing probability distributions over interrelated variables, usually with gra phical models. Examples of these systems are Bayesian networks and Markov Logic Networks [34]. The predictive model learned by an inductive learning algorithm should make accurate predictions not just on the training examples, but also on future exam- ples that come from the same distribution. In order to produce a model with this generalization capability , a learning algorithm must have an inductive bias [28]  a set of assumptions about the true distribution of the training data. The bias of an algorithm is often based on the hypothesis space of possible models that it considers. F or example, the hypothesis space of the Naive Bayes model is limited by the assumption that example characteristics are condition- ally independent given the class of an example. The bias of an algorithm can also be determined by its search process through the hypothesis space, which deter- mines the order in which hypotheses are considered. F or example, rule-learning algorithms typically construct rules one predicate at a time, which reﬂects the 3 assumption that predicates contribute signiﬁcantly to example coverage by them- selves rather than in pairs or more. T ransfer in inductive learning works by allowing source-task knowledge to aﬀect the target tasks inductive bias. It is usually concerned with improving the speed with which a model is learned, or with improving its generalization capability . The next subsection discusses inductive transfer, and the following ones elaborate on three speciﬁc settings for inductive transfer. There is some related work that is not discussed here because it speciﬁcally addresses multi-task learning. F or example, Niculescu-Mizil and Caruana [29] learn Bayesian networks simultaneously for multiple related tasks by bias ing learning toward similar structures for each task. While this is clearly rel ated to transfer learning, it is not directly applicable to the scenario in which a target task is encountered after one or more source tasks have already been learned. Inductive T ransfer In inductive transfer methods, the target-task inductive bias is chosen or adjusted based on the source-task knowledge (see Figure 4). The way this is done varies depending on which inductive learning algorithm is used to learn the source and target tasks. Some transfer methods narrow the hypothesis space, limiting the possible models, or remove search steps from consideration. Other methods broaden the space, allowing the search to discover more complex models, or add new search steps. Baxter [2] frames the transfer problem as that of choosing one hypothesis space from a family of spaces. By solving a set of related source tasks in eac h hypothesis space of the family and determining which one produces the best overall generalization error, he selects the most promising space in the famil y for a target task. Baxters work, unlike most in transfer learning, includes theor eti- cal as well as experimental results. He derives bounds on the number of source tasks and examples needed to learn an inductive bias, and on the generaliza- tion capability of a target-task solution given the number of source tasks and examples in each task. All Hypotheses Allowed Hypotheses Inductive Learning Search Inductive Transfer All Hypotheses Allowed Hypotheses Search Fig. 4. Inductive learning can be viewed as a directed search through a spec iﬁed hy- pothesis space [28]. Inductive transfer uses source-task knowledge to adjust the induc- tive bias, which could involve changing the hypothesis space or the se arch steps. 4 Thrun and Mitchell [55] look at solving Boolean classiﬁcation tasks in a lifelong-learning framework, where an agent encounters a collection of related problems over its lifetime. They learn each new task with a neural network, but they enhance the standard gradient-descent algorithm with slope information acquired from previous tasks. This speeds up the search for network parameters in a target task and biases it towards the parameters for previous tasks. Mihalkova and Mooney [27] perform transfer between Markov Logic Net- works. Given a learned MLN for a source task, they learn an MLN for a related target task by starting with the source-task one and diagnosing each formula , adjusting ones that are too general or too speciﬁc in the target domain. The hypothesis space for the target task is therefore deﬁned in relation to the source- task MLN by the operators that generalize or specify formulas. Hlynsson [17] phrases transfer learning in classiﬁcation as a minimum descrip- tion length problem given source-task hypotheses and target-task data. That is, the chosen hypothesis for a new task can use hypotheses for old tasks but stip- ulate exceptions for some data points in the new task. This method aims for a tradeoﬀ between accuracy and compactness in the new hypothesis. Ben-David and Schuller [3] propose a transformation framework to determine how related two Boolean classiﬁcation tasks are. They deﬁne two tasks as relat ed with respect to a class of transformations if they are equivalent under that clas s; that is, if a series of transformations can make one task look exactly li ke the other. They provide conditions under which learning related tasks concurrently requires fewer examples than single-task learning. Bayesian T ransfer One area of inductive transfer applies speciﬁcally to Bayesian learning meth- ods. Bayesian learning involves modeling probability distributions and taking advantage of conditional independence among variables to simplify the model. An additional aspect that Bayesian models often have is a prior distribution , which describes the assumptions one can make about a domain before seeing any training data. Given the data, a Bayesian model makes predictions by com- bining it with the prior distribution to produce a posterior distribution . A strong prior can signiﬁcantly aﬀect these results (see Figure 5). This serves as a natura l way for Bayesian learning methods to incorporate prior knowledge  in the case of transfer learning, source-task knowledge. Marx et al. [24] use a Bayesian transfer method for tasks solved by a log istic regression classiﬁer. The usual prior for this classiﬁer is a Gaussian distri bution with a mean and variance set through cross-validation. T o perform transfer, they instead estimate the mean and variance by averaging over several source tasks . Raina et al. [33] use a similar approach for multi-class classiﬁcation by learning a multivariate Gaussian prior from several source tasks. Dai et al. [7] apply a Bayesian transfer method to a Naive Bayes classiﬁer. They set the initial probability parameters based on a single source task, and revise them using target-task data. They also provide some theoretical bounds on the prediction error and convergence rate of their algorithm. 5 Bayesian Learning Bayesian Transfer Prior distribution  Data  Posterior Distribution Fig. 5. Bayesian learning uses a prior distribution to smooth the estimate s from train- ing data. Bayesian transfer may provide a more informative prior from s ource-task knowledge. Hierarchical T ransfer Another setting for transfer in inductive learning is hierarchical transfer. In this setting, solutions to simple tasks are combined or provided as tools to produce a solution to a more complex task (see Figure 6). This can involve many ta sks of varying complexity , rather than just a single source and target. The target ta sk might use entire source-task solutions as parts of its own, or it might use them in a more subtle way to improve learning. Sutton and McCallum [43] begin with a sequential approach where the pre- diction for each task is used as a feature when learning the next task. They then proceed to turn the problem into a multi-task learning problem by combining all the models and applying them jointly , which brings their method outside our deﬁnition of transfer learning, but the initial sequential approach is an example of hierarchical transfer. Line Curve Surface Circle Pipe Fig. 6. An example of a concept hierarchy that could be used for hierarchical tr ansfer, in which solutions from simple tasks are used to help learn a solution t o a more complex task. Here the simple tasks involve recognizing lines and curves in images, and the more complex tasks involve recognizing surfaces, circles, and ﬁnally pip e shapes. 6 Stracuzzi [42] looks at the problem of choosing relevant source-task Boolean concepts from a knowledge base to use while learning more complex concepts. He learns rules to express concepts from a stream of examples, allowing existing concepts to be used if they help to classify the examples, and adds and removes dependencies between concepts in the knowledge base. T aylor et al. [49] propose a transfer hierarchy that orders tasks by diﬃcul ty , so that an agent can learn them in sequence via inductive transfer. By putting tasks in order of increasing diﬃculty , they aim to make transfer more eﬀective. This approach may be more applicable to the multi-task learning scenario, since by our deﬁnition of transfer learning the agent may not be able to choose the order in which it learns tasks, but it could be applied to help choose from an existing set of source tasks. T ransfer with Missing Data or Class Labels Inductive transfer can be viewed not only as a way to improve learning in a standard supervised-learning task, but also as a way to oﬀset the diﬃculties posed by tasks that involve unsupervised learning, semi-supervised learning, o r small datasets. That is, if there are small amounts of data or class lab els for a task, treating it as a target task and performing inductive transfer from a related source task can lead to more accurate models. These approaches therefore use source-task data to enhance target-task data, despite the fact that the two datasets are assumed to come from diﬀerent probability distributions. The Bayesian transfer methods of Dai et al. [7] and Raina et al. [33] are intended to compensate for small amounts of target-task data. One of the beneﬁts of Bayesian learning is the stability that a prior distribution can provide i n the absence of large datasets. By estimating a prior from related source tasks, these approaches prevent the overﬁtting that would tend to occur with limited data. Dai et al. [8] address transfer learning in a boosting algorithm using large amounts of data from a previous task to supplement small amounts of new data. Boosting is a technique for learning several weak classiﬁers and combining them to form a stronger classiﬁer [16]. After each classiﬁer is learned, the examples are reweighted so that later classiﬁers focus more on examples the previous ones misclassiﬁed. Dai et al. extend this principle by also weighting source-task examples according to their similarity to target-task examples. This allows the algorithm to leverage source-task data that is applicable to the target task w hile paying less attention to data that appears less useful. Shi et al. [39] look at transfer learning in unsupervised and semi-supervised settings. They assume that a reasonably sized dataset exists in the target task , but it is largely unlabeled due to the expense of having an expert assign labels. T o address this problem they propose an active learning approach, in which the target-task learner requests labels for examples only when necessary . They construct a classiﬁer with labeled examples, including mostly source-task ones, and estimate the conﬁdence with which this classifer can label the unknown examples. When the conﬁdence is too low, they request an expert label. 7 TRANSFER IN REINFORCEMENT LEARNING A reinforcement learning (RL) agent operates in a sequential-control environ- ment called a Markov decision process (MDP) [45]. It senses the state of the en- vironment and performs actions that change the state and also trigger rewards. Its objective is to learn a policy for acting in order to maximize its cumulative reward. This involves solving a temporal credit-assignment problem, since an entire sequence of actions may be responsible for a single immediate reward. A typical RL agent behaves according to the diagram in Figure 7. At time step t, it observes the current state st and consults its current policy π to choose an action, π(st)  at. After taking the action, it receives a reward rt and observes the new state st1, and it uses that information to update its policy before repeating the cycle. Often RL consists of a sequence of episodes, which end whenever the agent reaches one of a set of ending states. During learning, the agent must balance between exploiting the current policy (acting in areas that it knows to have high rewards) and exploring new areas to ﬁnd potentially higher rewards. A common solution is the ǫ-greedy method, in which the agent takes random exploratory actions a small fraction of the ti me (ǫ  1), but usually takes the action recommended by the current policy . There are several categories of RL algorithms. Some types of methods are only applicable when the agent knows its environment model (the reward function and the state transition function). In this case dynamic programming can sol ve directly for the optimal policy without requiring any interaction with the en vi- ronment. In most RL problems, however, the model is unknown. Model-learning approaches use interaction with the environment to build an approximation of the true model. Model-free approaches learn to act without ever explicitly mod- eling the environment. Temporal-diﬀerence methods [44] operate by maintaining and iteratively up- dating value functions to predict the rewards earned by actions. They begin with an inaccurate function and update it based on interaction with the en- vironment, propagating reward information back along action sequences. One popular method is Q-learning [62], which involves learning a function Q(s, a) that estimates the cumulative reward starting in state s and taking action a and following the current policy thereafter. Given the optimal Q-function, the Environment Agent s0 a0 r0 s1 a1 r1  time Fig. 7. A reinforcement learning agent interacts with its environment: it receives in- formation about its state (s), chooses an action to take (a), receives a rew ard (r), and then repeats. 8 optimal policy is to take the action corresponding to argmaxaQ(st, a). When there are small ﬁnite numbers of states and actions, the Q-function can be repre- sented explicitly as a table. In domains that have large or inﬁnite state spaces, a function approximator such as a neural network or support-vector machine can be used to represent the Q-function. Policy-search methods, instead of maintaining a function upon which a policy is based, maintain and update a policy directly . They begin with an inaccurate policy and update it based on interaction with the environment. Heuristic sea rch and optimization through gradient descent are among the approaches that can be used in policy search. T ransfer in RL is concerned with speeding up the learning process, since RL agents can spend many episodes doing random exploration before acquiring a reasonable Q-function. W e divide RL transfer into ﬁve categories that represent progressively larger changes to existing RL algorithms. The subsections below describe those categories and present examples from published research. Starting-Point Methods Since all RL methods begin with an initial solution and then update it through experience, one straightforward type of transfer in RL is to set the initia l solution in a target task based on knowledge from a source task (see Figure 8). Compa red to the random or zero setting that RL algorithms usually use at ﬁrst, these starting-point methods can begin the RL process at a point much closer to a good target-task solution. There are variations on how to use the source-tas k knowledge to set the initial solution, but in general the RL algorithm in the target task is unchanged. T aylor et al. [53] use a starting-point method for transfer in temporal -diﬀerence RL. T o perform transfer, they copy the ﬁnal value function of the source task and use it as the initial one for the target task. As many transfer approaches do, this requires a mapping of features and actions between the tasks, and they provide a mapping based on their domain knowledge. 0000 0000 0000 target-task training 4195 2719 8452 Initial Q-table transfer no transfer source task Fig. 8. Starting-point methods for RL transfer set the initial solution based on the source task in the hope of starting at a higher performance level than the typical initial solution would. In this example, a Q-function table is initi alized to a source-task table, and the target-task performance begins at a level that is only reach ed after some training when beginning with a typical all-zero table. 9 T anaka and Y amamura [47] use a similar approach in temporal-diﬀerence learning without function approximation, where value functions are simply rep- resented by tables. This greater simplicity allows them to combine knowledge from several source tasks: they initialize the value table of the target tas k to the average of tables from several prior tasks. F urthermore, they use the standard deviations from prior tasks to determine priorities between temporal-diﬀerence backups. Approaching temporal-diﬀerence RL as a batch problem instead of an in- cremental one allows for diﬀerent kinds of starting-point transfer methods. I n batch RL, the agent interacts with the environment for more than one step or episode at a time before updating its solution. Lazaric et al. [21] perform trans- fer in this setting by ﬁnding source-task samples that are similar to the target task and adding them to the normal target-task samples in each batch, thus increasing the available data early on. The early solutions are almost entir ely based on source-task knowledge, but the impact decreases in later batches as more target-task data becomes available. Moving away from temporal-diﬀerence RL, starting-point methods can take even more forms. In a model-learning Bayesian RL algorithm, Wilson et al. [63] perform transfer by treating the distribution of previous MDPs as a prior fo r the current MDP . In a policy-search genetic algorithm, T aylor et al. [54] tra nsfer a population of policies from a source task to serve as the initial populati on for a target task. Imitation Methods Another class of RL transfer methods involves applying the source-task policy to choose some actions while learning the target task (see Figure 9). While they make no direct changes to the target-task solution the way that starting-poin t methods do, these imitation methods aﬀect the developing solution by producing diﬀerent function or policy updates. Compared to the random exploration that RL algorithms typically do, decisions based on a source-task policy can lead the agent more quickly to promising areas of the environment. One method is to follow a source-task policy only during exploration steps of the target task, when the agent would otherwise be taking a random action. Madden and Howley [23] use this approach in tabular Q-learning. They represent a source-task policy as a set of rules in propositional logic and choose acti ons based on those rules during exploration steps. F ernandez and V eloso [15] instead give the agent a three-way choice between exploiting the current target-task policy , exploiting a past policy , and explor ing randomly . They introduce a second parameter, in addition to the ǫ of ǫ-greedy exploration, to determine the probability of making each choice. Another imitation method called demonstration involves following a source- task policy for a ﬁxed number of episodes at the beginning of the target task and then reverting to normal RL. In the early steps of the target task, the cur- rent policy can be so ill-formed that exploiting it is no diﬀerent than exploring randomly . This approach aims to avoid that initial uncertainty and to generat e 10 (a) (b) source target training time training time policy used source target policy used Fig. 9. Imitation methods for RL transfer follow the source-task policy duri ng some steps of the target task. The imitation steps may all occur at the beginn ing of the target task, as in (a) above, or they may be interspersed with steps th at follow the developing target-task policy , as in (b) above. enough data to create a reasonable target-task policy by the time the demon- stration period ends. T orrey et al. [58] and T orrey et al. [56] perform tr ansfer via demonstration, representing the source-task policy as a relational ﬁnite-stat e machine and a Markov Logic Network respectively . Hierarchical Methods A third class of RL transfer includes hierarchical methods. These view the source as a subtask of the target, and use the solution to the source as a building block for learning the target. Methods in this class have strong connections to the area of hierarchical RL, in which a complex task is learned in pieces through division into a hierarchy of subtasks (see Figure 10). An early approach of this type is to compose several source-task solutions to form a target-task solution, as is done by Singh [40]. He addresses a scenari o in which complex tasks are temporal concatenations of simple ones, so that a target task can be solved by a composition of several smaller solutions . Mehta et al. [25] have a transfer method that works directly within the hi- erarchical RL framework. They learn a task hierarchy by observing successful behavior in a source task, and then use it to apply the MaxQ hierarchical RL algorithm [10] in the target task. This removes the burden of designing a task hierarchy through transfer. Other approaches operate within the framework of options, which is a term for temporally-extended actions in RL [31]. An option typically consists of a starting condition, an ending condition, and an internal policy for choosing lower - level actions. An RL agent treats each option as an additional action along w ith the original lower-level ones (see Figure 10). In some scenarios it may be useful to have the entire source-task policy as an option in the target task, as Croonenborghs et al. [6] do. They learn a relat ional decision tree to represent the source-task policy and allow the target-task learner to execute it as an option. Another possibility is to learn smaller options, ei ther during or after the process of learning the source task, and oﬀer them to the target. Asadi and Huber [1] do this by ﬁnding frequently-visited states in the source task to serve as ending conditions for options. 11 Run Kick Pass Shoot Soccer Run Kick Run Kick Pass Kick Shoot (a) (b) Fig. 10. (a) An example of a task hierarchy that could be used to train agents to play soccer via hierarchical RL. Lower-level abilities like ki cking a ball and running are needed for higher-level abilities like passing and shooting, whi ch could then be combined to learn to play soccer. (b) The mid-level abilities re presented as options alongside the low-level actions. Alteration Methods The next class of RL transfer methods involves altering the state space, action space, or reward function of the target task based on source-task knowledge. These alteration methods have some overlap with option-based transfer, which also changes the action space in the target task, but they include a wide range of other approaches as well. One way to alter the target-task state space is to simplify it through stat e abstraction. W alsh et al. [60] do this by aggregating over comparable source-t ask states. They then use the aggregate states to learn the target task, which reduces the complexity signiﬁcantly . There are also approaches that expand the target-task state space instead of reducing it. T aylor and Stone [51] do this by adding a new state variable in the target task. They learn a decision list that represents the source-task policy and use its output as the new state variable. While option-based transfer methods add to the target-task action space, there is also some work in decreasing the action space. Sherstov and Stone [38] do this by evaluating in the source task which of a large set of actions are mo st useful. They then consider only a smaller action set in the target task, which decreases the complexity of the value function signiﬁcantly and also decreases the amount of exploration needed. Reward shaping is a design technique in RL that aims to speed up learning by providing immediate rewards that are more indicative of cumulative rewards . Usually it requires human eﬀort, as many aspects of RL task design do. Konidar is and Barto [19] do reward shaping automatically through transfer. They learn t o predict rewards in the source task and use this information to create a shaped reward function in the target task. 12 New RL Algorithms A ﬁnal class of RL transfer methods consists of entirely new RL algorithms. Rather than making small additions to an existing algorithm or making chang es to the target task, these approaches address transfer as an inherent part of RL. They incorporate prior knowledge as an intrinsic part of the algorithm. Price and Boutilier [32] propose a temporal-diﬀerence algorithm in which value functions are inﬂuenced by observations of expert agents. They use a vari- ant of the usual value-function update calculation that includes an experts ex- perience, weighted by the agents conﬁdence in itself and in the expert. They also perform extra backups at states the expert visits to focus attention o n those areas of the state space. There are several algorithms for case-based RL that accomodate transfer. Sharma et al. [37] propose one in which Q-functions are estimated using a Gaus- sian kernel over stored cases in a library . Cases are added to the library from both the source and target tasks when their distance to their nearest neighbor is above a threshold. T aylor et al. [48] use source-task examples more selectiv ely in their case-based RL algorithm. They use target-task cases to make decisions when there are enough, and only use source-task examples when insuﬃcient target examples exist. T orrey et al. [57, 59] use an RL algorithm called Knowledge-Based Kernel Regression (KBKR) that allows transfer via advice-taking. Advice in this algo- rithm is a rule telling the agent which action to prefer in a set of states describ ed by a conjunct of predicates. KBKR approximates the Q-function with a support- vector machine and includes advice as a soft constraint. The Q-function, which is relearned in batches using temporal-diﬀerence updates, trades oﬀ between matching the agents experience and matching the advice. T orrey et al. gener- ate advice through automated analysis of the source-task solution; in [59] t hey construct advice directly from the source-task Q-function, and in [57] they learn rules in ﬁrst-order logic by observing the source-task agent as it follows its policy . A VOIDING NEGA TIVE TRANSFER Given a target task, the eﬀectiveness of any transfer method depends on the source task and how it is related to the target. If the relationship is strong and the transfer method can take advantage of it, the performance in the target t ask can signiﬁcantly improve through transfer. However, if the source task is not suﬃciently related or if the relationship is not well leveraged by the transfer method, the performance with many approaches may not only fail to improve  it may actually decrease. This section examines work on preventing transfer from negatively aﬀecting performance. Ideally , a transfer method would produce positive transfer between appro- priately related tasks while avoiding negative transfer when the tasks are no t a good match. In practice, these goals are diﬃcult to achieve simultaneously . Ap- proaches that have safeguards to avoid negative transfer often produce a smaller 13 transfer performance task relatedness aggressive safe Fig. 11. A representation of how the degree of relatedness between the source and target tasks translates to target-task performance when conducting tran sfer from the source task. With aggressive approaches, there can be higher beneﬁts at h igh degrees of relatedness, but there can also be negative transfer at low levels. Safer approaches may limit negative transfer at the lower end, but may also have fewe r beneﬁts at the higher end. eﬀect from positive transfer due to their caution. Conversely , approaches that transfer aggressively and produce large positive-transfer eﬀects often have no protection against negative transfer (see Figure 11). F or example, consider the imitation methods for RL transfer. On one end of the range an agent imitates a source-task policy only during infrequent ex- ploration steps, and on the other end it demonstrates the source-task policy for a ﬁxed number of initial episodes. The exploration method is very cautious and therefore unlikely to produce negative transfer, but it is also unlikely to produce large initial performance increases. The demonstration method is very aggressive; if the source-task policy is a poor one for the target task, fo llowing it blindly will produce negative transfer. However, when the source-task solution is a decent one for the target task, it can produce some of the largest initial performance improvements of any method. Rejecting Bad Information One way of approaching negative transfer is to attempt to recognize and reject harmful source-task knowledge while learning the target task. The goal in this approach is to minimize the impact of bad information, so that the transfer per - formance is at least no worse than learning the target task without transfer. At the extreme end, an agent might disregard the transferred knowledge completely , but some methods also allow it to selectively reject parts and keep other parts. Option-based transfer in reinforcement learning (e.g. Croonenborghs et al. [6]) is an example of an approach that naturally incorporates the ability to reject bad information. Since options are treated as additional actions, the agent can choose to use them or not to use them; in Q-learning, for example, agents learn Q-values for options just as for native actions. If an option regularly produces poor performance, its Q-values will degrade and the agent will choose it less fre- quently . However, if an option regularly leads to good results, its Q-values will 14 grow and the agent will choose it more often. Option-based transfer can there- fore provide a good balance between achieving positive transfer and avoiding negative transfer. A speciﬁc approach that incorporates the ability to reject bad information is the KBKR advice-taking algorithm for transfer in reinforcement learning [57, 59] . Recall that KBKR approximates the Q-function with a support-vector machine and includes advice from the source task as a soft constraint. Since the Q-function trades oﬀ between matching the agents experience and matching the advice, the agent can learn to disregard advice that disagrees with its experience. Rosenstein et al. [35] present an approach for detecting negative transfer in naive Bayes classiﬁcation tasks. They learn a hyperprior for both the sour ce and target tasks, and the variance of this hyperprior is proportional to the di ssimi- larity between the tasks. It may be possible to use a method like this to deci de whether to transfer at all, by setting an acceptable threshold of similarity . Choosing a Source T ask There are more possibilities for avoiding negative transfer if there exist s not just one source task, but a set of candidate source tasks. In this case the problem becomes choosing the best source task (see Figure 12). T ransfer methods without much protection against negative transfer may still be eﬀective in this scenar io, as long as the best source task is at least a decent match. An example of this approach is the previously-mentioned transfer hierarchy of T aylor et al. [49], who order tasks by diﬃculty . Appropriate source task s are usually less diﬃcult than the target task, but not so much simpler that they contain little information. Given a task ordering, it may be possible to locate the position of the target task in the hierarchy and select a source task that is only moderately less diﬃcult. T alvitie and Singh [46] use a straightforward method of selecting a previo us Markov decision process to transfer. They run each candidate MDP in the target task for a ﬁxed length of time and order them by their performance. Then they select the best one and continue with it, only proceeding down the list if the current MDP begins performing signiﬁcantly worse than it originally appear ed. This trial-and-error approach, though it may be costly in the aggregate number of training episodes needed, is simple and widely applicable. Kuhlmann and Stone [20] look at ﬁnding similar tasks when each task is speciﬁed in a formal language. They construct a graph to represent the elements and rules of a task. This allows them to ﬁnd identical tasks by checking for graph isomorphism, and by creating minor variants of a target-task graph, they can also search for similar tasks. If they ﬁnd an isomorphic match, they conduct value-function transfer. Eaton and DesJardins [12] propose choosing from among candidate solutions to a source task rather than from among candidate source tasks. Their setting is multi-resolution learning, where a classiﬁcation task is solved by an ensem ble of models that vary in complexity . Low-resolution models are simple and coar se, while higher-resolution models are more complex and detailed. They reason that 15 Task 1 Target Task Task 2 Task N  Select Task 3 Task 1 Task 2 Task 4 Task 3 Target Task (a) (b) dist 1 dist 2 dist 3 dist 4 Fig. 12. (a) One way to avoid negative transfer is to choose a good source task from which to transfer. In this example, T ask 2 is selected as being the most related. (b) Another way to avoid negative transfer is to model the way source task s are related to the target task and combine knowledge from them with those relationship s in mind. high-resolution models are less transferrable between tasks, and select a resolu- tion below which to share models with a target task. Modeling T ask Similarity Given multiple candidate source tasks, it may be beneﬁcial to use several or all o f them rather than to choose just one (see Figure 12). Some approaches discussed in this chapter do this naively , without evaluating how the source tasks are related to the target. However, there are some approaches that explicitly model relationships between tasks and include this information in the transfer method. This can lead to better use of source-task knowledge and decrease the risk of negative transfer. Carroll and Seppi [4] develop several similarity measures for reinforcement learning tasks, comparing policies, value functions, and rewards. These are only measurable while the target task is being learned, so their practical use in transf er scenarios is limited. However, they make the relevant point that task simil arity is intimately linked with a particular transfer method, and cannot be evaluated independently . Eaton et al. [13] construct a graph in which nodes represent source tasks and distances represent a transferability metric. Given a new inductive learning task, they estimate parameters by ﬁtting the task into the graph and learning a func- tion that translates graph locations to task parameters. This method not onl y models the relationships between tasks explicitly , but also gives an algorithm for the informed use of several source tasks in transfer learning. Ruckert and Kramer [36] look at inductive transfer via kernel methods. They learn a meta-kernel that serves as a similarity function between tasks. Given t his and a set of kernels that perform well in source tasks, they perform numerical optimization to construct a kernel for a target task. This approach determines the inductive bias in the target task (the kernel) by combining information from several source tasks whose relationships to the target are known. 16 AUTOMA TICALL Y MAPPING T ASKS An inherent aspect of transfer learning is recognizing the correspondences be- tween tasks. Knowledge from one task can only be applied to another if it is expressed in a way that the target-task agent understands. In some cases, the representations of the tasks are assumed to be identical, or at least one is a subset of the other. Otherwise, a mapping is needed to translate between task representations (see Figure 13). Many transfer approaches do not address the mapping problem directly and require that a human provide this information. However, there are some trans fer approaches that do address the mapping problem. This section discusses some of this work. Equalizing T ask Representations F or some transfer scenarios, it may be possible to avoid the mapping problem altogether by ensuring that the source and target tasks have the same represen- tation. If the language of the source-task knowledge is identical to or a subset of the language of the target task, it can be applied directly with no transla- tion. Sometimes a domain can be constructed so that this occurs naturally , or a common representation that equalizes the tasks can be found. Relational learning is useful for creating domains that naturally produce com- mon task representations. First-order logic represents objects in a domain w ith symbolic variables, which can allow abstraction that the more typical pr oposi- tional feature vector cannot. Driessens et al. [11] show how relational reinforce- ment learning can simplify transfer in RL. Another framework for constructing a domain relationally is that of Konidaris and Barto [19], who express knowledge in two diﬀerent spaces. In agent space the representation is constant across tasks, while in problem space it is task- dependent. They transfer agent-space knowledge only because its common rep- resentation makes it straightforward to transfer. Property 1  Property 2 Property N Source Task Target Task Property 1  Property 2 Property 3 Property M Fig. 13. A mapping generally translates source-task properties into target-tas k prop- erties. The numbers of properties may not be equal in the two tasks, and the mapping may not be one-to-one. Properties may include entries in a feature v ector, objects in a relational world, RL actions, etc. 17 Pan et al. [30] take a mathematical approach to ﬁnding a common repre- sentation for two separate classiﬁcation tasks. They use kernel methods to ﬁnd a low-dimensional feature space where the distributions of source and target data are similar, and transfer a source-task model for this smaller space. This approach stretches our strict deﬁnition of transfer learning, which assumes the target task is unknown when the source task is learned, but in some scenarios it may be practical to adjust the source-task solution to a diﬀerent feature space after gaining some knowledge about the target task. T rying Multiple Mappings One straightforward way of solving the mapping problem is to generate several possible mappings and allow the target-task agent to try them all. The candidat e mappings can be an exhaustive set, or they can be limited by constraints on what elements are permissible matches for other elements. Exhaustive sets may be computationally infeasible for large domains. T aylor et al. [50] perform an exhaustive search of possible mappings in R L transfer. They evaluate each candidate using a small number of episodes in the target task, and select the best one to continue learning. Mihalkova et al. [26 ] limit their search for mappings in MLN transfer, requiring that mapped pred- icates have matching arity and argument types. Under those constraints, they conduct an exhaustive search to ﬁnd the best mapping between networks. Soni and Singh [41] not only limit the candidate mappings by considering object types, but also avoid a separate evaluation of each mapping by using options in RL transfer. They generate a set of possible mappings by connecting target-task objects to all source-task objects of the matching type. With eac h mapping, they create an option from a source MDP . The options framework gives an inherent way to compare multiple mappings while learning a target MDP without requiring extra trial periods. Mapping by Analogy If the task representations must diﬀer, and the scenario calls for choosing one mapping rather than trying multiple candidates, then there are some methods that construct a mapping by analogy . These methods examine the characteristics of the source and target tasks and ﬁnd elements that correspond. F or example, in reinforcement learning, actions that correspond produce similar rewards and state changes, and objects that correspond are aﬀected similarly by actions. Analogical structure mapping [14] is a generic procedure based on cognitive theories of analogy that ﬁnds corresponding elements. It assigns scores to local matches and searches for a global match that maximizes the scores; permissi- ble matches and scoring functions are domain-dependent. Several transfer ap- proaches use this framework solve the mapping problem. Klenk and F orbus [18] apply it to solve physics problems that are written in a predicate-calculus lan- guage by retrieving and forming analogies from worked solutions written i n the 18 same language. Liu and Stone [22] apply it in reinforcement learning to ﬁnd matching features and actions between tasks. There are also some approaches that rely more on statistical analysis tha n on logical reasoning to ﬁnd matching elements. T aylor and Stone [52] learn map- pings for RL tasks by running a small number of target-task episodes and then training classiﬁers to characterize actions and objects. If a classiﬁer trained f or one action predicts the results of another action well, then those actions are mapped; likewise, if a classiﬁer trained for one object predicts the behavior of another object well, those objects are mapped. W ang and Mahadevan [61] trans- late datasets to low-dimensional feature spaces using dimensionality reduction, and then perform a statistical shaping technique called Procrustes analysis to align the feature spaces. THE FUTURE OF TRANSFER LEARNING The challenges discussed in this chapter will remain relevant in future work on transfer learning, particularly the avoidance of negative transfer and the autom a- tion of task mapping. Humans appear to have mechanisms for deciding when to transfer information, selecting appropriate sources of knowledge, and determin- ing the appropriate level of abstraction. It is not always clear how to make thes e decisions for a single machine learning algorithm, much less in general. Another challenge for future work is to enable transfer between more di- verse tasks. Davis and Domingos [9] provide a potential direction for t his in their work on MLN transfer. They perform pattern discovery in the source task to ﬁnd second-order formulas, which represent universal abstract concepts like symmetry and transitivity . When learning an MLN for the target task, they allow the search to use the discovered formulas in addition to the original pr ed- icates in the domain. This approach is recognizeable as inductive transfer, but the source-task knowledge is highly abstract, which allows the source and target tasks to diﬀer signiﬁcantly . Y et another challenge is to perform transfer in more complex testbeds. Par- ticularly in RL transfer, it can become much more diﬃcult to achieve transfer a s the source and target tasks become more complex. Since practical applications of reinforcement learning are likely to be highly complex, it is important not to limit research on RL tranfer to simple domains. T ransfer learning has become a sizeable subﬁeld in machine learning. It has ideological beneﬁts, because it is seen as an important aspect of human learning, and also practical beneﬁts, because it can make machine learning more eﬃcient. As computing power increases and researchers apply machine learning to more complex problems, knowledge transfer can only become more desirable. ACKNOWLEDGEMENTS This chapter was written while the authors were partially supported by DARP A grants HR0011-07-C-0060 and F A8650-06-C-7606. 19 References 1. M. Asadi and M. Huber. Eﬀective control knowledge transfer through l earning skill and representation hierarchies. In International Joint Conference on Artiﬁcial Intelligence, 2007. 2. J. Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research, 12:149198, 2000. 3. S. Ben-David and R. Schuller. Exploiting task relatedness for mu ltiple task learn- ing. In Conference on Learning Theory , 2003. 4. C. Carroll and K. Seppi. T ask similarity measures for transfer in rei nforcement learning task libraries. In IEEE International Joint Conference on Neural Net- works, 2005. 5. R. Caruana. Multitask learning. Machine Learning , 28:4175, 1997. 6. T. Croonenborghs, K. Driessens, and M. Bruynooghe. Learning relati onal skills for inductive transfer in relational reinforcement learning. In International Conference on Inductive Logic Programming , 2007. 7. W. Dai, G. Xue, Q. Y ang, and Y. Y u. T ransferring Naive Bayes classiﬁe rs for text classiﬁcation. In AAAI Conference on Artiﬁcial Intelligence , 2007. 8. W. Dai, Q. Y ang, G. Xue, and Y. Y u. Boosting for transfer learning. In Interna- tional Conference on Machine Learning , 2007. 9. J. Davis and P . Domingos. Deep transfer via second-order Markov logic. In AAAI Workshop on Transfer Learning for Complex Tasks , 2008. 10. T. Dietterich. Hierarchical reinforcement learning with the M AXQ value function decomposition. Journal of Artiﬁcial Intelligence Research , 13:227303, 2000. 11. K. Driessens, J. Ramon, and T. Croonenborghs. T ransfer learning for r einforce- ment learning through goal and policy parametrization. In ICML Workshop on Structural Knowledge Transfer for Machine Learning , 2006. 12. E. Eaton and M. DesJardins. Knowledge transfer with a multiresolut ion ensemble of classiﬁers. In ICML Workshop on Structural Knowledge Transfer for Machine Learning, 2006. 13. E. Eaton, M. DesJardins, and T. Lane. Modeling transfer relationshi ps between learning tasks for improved inductive transfer. In European Conference on Machine Learning, 2008. 14. B. F alkenhainer, K. F orbus, and D. Gentner. The structure-mappi ng engine: Al- gorithm and examples. Artiﬁcial Intelligence , 41:163, 1989. 15. F. F ernandez and M. V eloso. Probabilistic policy reuse in a reinforc ement learning agent. In Conference on Autonomous Agents and Multi-Agent Systems , 2006. 16. Y. F reund and R. Schapire. A decision-theoretic generalization of on- line learn- ing and an application to boosting. Journal of Computer and System Sciences , 55(1):119139, 1997. 17. H. Hlynsson. T ransfer learning using the minimum description l ength principle with a decision tree application. Masters thesis, University of A msterdam, 2007. 18. M. Klenk and K. F orbus. Measuring the level of transfer learning by an AP physics problem-solver. In AAAI Conference on Artiﬁcial Intelligence , 2007. 19. G. Konidaris and A. Barto. Autonomous shaping: Knowledge transfer in rei nforce- ment learning. In International Conference on Machine Learning , 2006. 20. G. Kuhlmann and P . Stone. Graph-based domain mapping for transfer learn ing in general games. In European Conference on Machine Learning , 2007. 21. A. Lazaric, M. Restelli, and A. Bonarini. T ransfer of samples in batch reinforcement learning. In International Conference on Machine Learning , 2008. 20 22. Y. Liu and P . Stone. V alue-function-based transfer for reinforceme nt learning using structure mapping. In AAAI Conference on Artiﬁcial Intelligence , 2006. 23. M. Madden and T. Howley . T ransfer of experience between reinforc ement learning environments with progressive diﬃculty . Artiﬁcial Intelligence Review , 21:375398, 2004. 24. Z. Marx, M. Rosenstein, L. Kaelbling, and T. Dietterich. T ransfer learning with an ensemble of background tasks. In NIPS Workshop on Transfer Learning , 2005. 25. N. Mehta, S. Ray , P . T adepalli, and T. Dietterich. Automatic disco very and transfer of MAXQ hierarchies. In International Conference on Machine Learning , 2008. 26. L. Mihalkova, T. Huynh, and R. Mooney . Mapping and revising Markov L ogic Networks for transfer learning. In AAAI Conference on Artiﬁcial Intelligence , 2007. 27. L. Mihalkova and R. Mooney . T ransfer learning with Markov Logic Netw orks. In ICML Workshop on Structural Knowledge Transfer for Machine Lear ning, 2006. 28. T. Mitchell. Machine Learning . McGraw-Hill, 1997. 29. A. Niculescu-Mizil and R. Caruana. Inductive transfer for Bayesi an network struc- ture learning. In Conference on AI and Statistics , 2007. 30. S. Pan, J. Kwok, and Q. Y ang. T ransfer learning via dimensionality red uction. In AAAI Conference on Artiﬁcial Intelligence , 2008. 31. T. Perkins and D. Precup. Using options for knowledge transfer in r einforce- ment learning. T echnical Report UM-CS-1999-034, University of Massachuse tts, Amherst, 1999. 32. B. Price and C. Boutilier. Implicit imitation in multiagent reinf orcement learning. In International Conference on Machine Learning , 1999. 33. R. Raina, A. Ng, and D. Koller. Constructing informative priors using transfer learning. In International Conference on Machine Learning , 2006. 34. M. Richardson and P . Domingos. Markov logic networks. Machine Learning , 62(1- 2):107136, 2006. 35. M. Rosenstein, Z. Marx, L. Kaelbling, and T. Dietterich. T o transf er or not to transfer. In NIPS Workshop on Inductive Transfer , 2005. 36. U. Ruckert and S. Kramer. Kernel-based inductive transfer. In European Confer- ence on Machine Learning , 2008. 37. M. Sharma, M. Holmes, J. Santamaria, A. Irani, C. Isbell, and A. Ram. T ran sfer learning in real-time strategy games using hybrid CBRRL. In International Joint Conference on Artiﬁcial Intelligence , 2007. 38. A. Sherstov and P . Stone. Action-space knowledge transfer in MDPs : F ormalism, suboptimality bounds, and algorithms. In Conference on Learning Theory , 2005. 39. X. Shi, W. F an, and J. Ren. Actively transfer domain knowledge. In European Conference on Machine Learning , 2008. 40. S. Singh. T ransfer of learning by composing solutions of elemental seq uential tasks. Machine Learning , 8(3-4):323339, 1992. 41. V. Soni and S. Singh. Using homomorphisms to transfer options across cont inuous reinforcement learning domains. In AAAI Conference on Artiﬁcial Intelligence , 2006. 42. D. Stracuzzi. Memory organization and knowledge transfer. In ICML Workshop on Structural Knowledge Transfer for Machine Learning , 2006. 43. C. Sutton and A. McCallum. Composition of conditional random ﬁelds for tran sfer learning. In Conference on Empirical methods in Natural Language Processing , 2005. 44. R. Sutton. Learning to predict by the methods of temporal diﬀeren ces. Machine Learning, 3:944, 1988. 21 45. R. Sutton and A. Barto. Reinforcement Learning: An Introduction . MIT Press, 1998. 46. E. T alvitie and S. Singh. An experts algorithm for transfer learning. I n Interna- tional Joint Conference on Artiﬁcial Intelligence , 2007. 47. F. T anaka and M. Y amamura. Multitask reinforcement learning on the dis tri- bution of MDPs. Transactions of the Institute of Electrical Engineers of Japan , 123(5):10041011, 2003. 48. M. T aylor, N. Jong, and P . Stone. T ransferring instances for model-b ased rein- forcement learning. In European Conference on Machine Learning , 2008. 49. M. T aylor, G. Kuhlmann, and P . Stone. Accelerating search with tran sferred heuristics. In ICAPS Workshop on AI Planning and Learning , 2007. 50. M. T aylor, G. Kuhlmann, and P . Stone. Autonomous transfer for reinforc ement learning. In Conference on Autonomous Agents and Multi-Agent Systems , 2008. 51. M. T aylor and P . Stone. Cross-domain transfer for reinforcement learn ing. In International Conference on Machine Learning , 2007. 52. M. T aylor and P . Stone. T ransfer via inter-task mappings in policy se arch reinforce- ment learning. In Conference on Autonomous Agents and Multi-Agent Systems , 2007. 53. M. T aylor, P . Stone, and Y. Liu. V alue functions for RL-based behavi or transfer: A comparative study . In AAAI Conference on Artiﬁcial Intelligence , 2005. 54. M. T aylor, S. Whiteson, and P . Stone. T ransfer learning for policy se arch methods. In ICML Workshop on Structural Knowledge Transfer for Machine Lear ning, 2006. 55. S. Thrun and T. Mitchell. Learning one more thing. In International Joint Con- ference on Artiﬁcial Intelligence , 1995. 56. L. T orrey , J. Shavlik, S. Natarajan, P . Kuppili, and T. W alker. T ran sfer in rein- forcement learning via Markov Logic Networks. In AAAI Workshop on Transfer Learning for Complex Tasks , 2008. 57. L. T orrey , J. Shavlik, T. W alker, and R. Maclin. Relational skill tr ansfer via advice taking. In ICML Workshop on Structural Knowledge Transfer for Machine Learning, 2006. 58. L. T orrey , J. Shavlik, T. W alker, and R. Maclin. Relational macros for t ransfer in reinforcement learning. In International Conference on Inductive Logic Program- ming, 2007. 59. L. T orrey , T. W alker, J. Shavlik, and R. Maclin. Using advice to tr ansfer knowledge acquired in one reinforcement learning task to another. In European Conference on Machine Learning , 2005. 60. T. W alsh, L. Li, and M. Littman. T ransferring state abstractions bet ween MDPs. In ICML Workshop on Structural Knowledge Transfer for Machine Lear ning, 2006. 61. C. W ang and S. Mahadevan. Manifold alignment using Procrustes analysis . In International Conference on Machine Learning , 2008. 62. C. W atkins. Learning from delayed rewards . PhD thesis, University of Cambridge, 1989. 63. A. Wilson, A. F ern, S. Ray , and P . T adepalli. Multi-task reinforce ment learning: A hierarchical Bayesian approach. In International Conference on Machine Learning , 2007. 22",
    "page_start": null,
    "page_end": null,
    "word_count": 9754,
    "created_at": "2025-08-18T06:55:19",
    "updated_at": "2025-08-18T06:55:19"
  },
  {
    "id": "6ce8b2e10d27401b9cc394963d0e4832",
    "doc_id": "7379cac55d5c49bab8831cd705225ea9",
    "doc_name": "A_Comprehensive_Survey_on_Transfer_Learning.pdf",
    "heading": "Document",
    "content": "arXiv:1911.02685v3 [cs.LG] 23 Jun 2020 1 A Comprehensive Survey on T ransfer Learning Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Y ongchun Zh u, Hengshu Zhu, Senior Member, IEEE, Hui Xiong, Fellow, IEEE, and Qing He AbstractT ransfer learning aims at improving the performance of tar get learners on target domains by transferring the knowledg e contained in different but related source domains. In this w ay , the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide applicati on prospects, transfer learning has become a popular and pro mising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent a dvances in transfer learning. Due to the rapid expansion of t he transfer learning area, it is both necessary and challenging to compr ehensively review the relevant studies. This survey attemp ts to connect and systematize the existing transfer learning researches , as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way , which may help rea ders have a better understanding of the current research sta tus and ideas. Unlike previous surveys, this survey paper reviews m ore than forty representative transfer learning approache s, especially homogeneous transfer learning approaches, from the perspe ctives of data and model. The applications of transfer learn ing are also brieﬂy introduced. In order to show the performance of diffe rent transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-2 1578, and Ofﬁce-31. And the experimental results demonstrate the imp ortance of selecting appropriate transfer learning models for different applications in practice. Index Terms T ransfer learning, machine learning, domain adaptation, interpretation.  1 I NTRODUCTION A LT H O UGH traditional machine learning technology has achieved great success and has been successfully ap- plied in many practical applications, it still has some limi ta- tions for certain real-world scenarios. The ideal scenario of machine learning is that there are abundant labeled trainin g instances, which have the same distribution as the test data . However , collecting sufﬁcient training data is often expen - sive, time-consuming, or even unrealistic in many scenario s. Semi-supervised learning can partly solve this problem by relaxing the need of mass labeled data. T ypically , a semi- supervised approach only requires a limited number of labeled data, and it utilizes a large amount of unlabeled data to improve the learning accuracy . But in many cases, unlabeled instances are also difﬁcult to collect, which usu - ally makes the resultant traditional models unsatisfactor y . T ransfer learning, which focuses on transferring the knowledge across domains, is a promising machine learning methodology for solving the above problem. The concept about transfer learning may initially come from educationa l psychology . According to the generalization theory of tran s- fer , as proposed by psychologist C.H. Judd, learning to transfer is the result of the generalization of experience. It is possible to realize the transfer from one situation to anoth er ,  Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Y ongchun Zh u, and Qing He are with the Key Laboratory of Intelligent Inform ation Processing of Chinese Academy of Sciences (CAS), Institute of Computing T echnology, CAS, Beijing 100190, China and the University o f Chinese Academy of Sciences, Beijing 100049, China.  Hengshu Zhu is with Baidu Inc., No. 10 Shangdi 10th Street, Ha idian District, Beijing, China.  Hui Xiong is with Rutgers, the State University of New Jersey , 1 Washington Park, Newark, New Jersey, USA.  Zhiyuan Qi is with the equal contribution to the ﬁrst author .  Fuzhen Zhuang and Zhiyuan Qi are corresponding authors, zhu ang- fuzhenict.ac.cn and qizhyuangmail.com. Fig. 1. Intuitive examples about transfer learning. as long as a person generalizes his experience. According to this theory , the prerequisite of transfer is that there needs to be a connection between two learning activities. In practice, a person who has learned the violin can learn the piano faster than others, since both the violin and the piano are musical instruments and may share some common knowledge. Fig. 1 shows some intuitive examples about transfer learning. Inspired by human beings capabilities to transfer knowledge across domains, transfer learning aims to leverage knowledge from a related domain (called source domain) to improve the learning performance or minimize the number of labeled examples required in a target domain. It is worth mentioning that the transferred knowledge does not always bring a positive impact on new tasks. If there is little in common between domains, knowledge transfer could be unsuccessful. For example, learning to ride a bicy- cle cannot help us learn to play the piano faster . Besides, th e similarities between domains do not always facilitate lear n- ing, because sometimes the similarities may be misleading. For example, although Spanish and French have a close re- lationship with each other and both belong to the Romance 2 group of languages, people who learn Spanish may experi- ence difﬁculties in learning French, such as using the wrong vocabulary or conjugation. This occurs because previous successful experience in Spanish can interfere with learni ng the word formation, usage, pronunciation, conjugation, et c., in French. In the ﬁeld of psychology , the phenomenon that previous experience has a negative effect on learning new tasks is called negative transfer [1]. Similarly , in the tra nsfer learning area, if the target learner is negatively affected by the transferred knowledge, the phenomenon is also termed as negative transfer [2], [3]. Whether negative transfer wi ll occur may depend on several factors, such as the relevance between the source and the target domains and the learners capacity of ﬁnding the transferable and beneﬁcial part of th e knowledge across domains. In [3], a formal deﬁnition and some analyses of negative transfer are given. Roughly speaking, according to the discrepancy between domains, transfer learning can be further divided into two categories, i.e., homogeneous and heterogeneous transfer learning [4]. Homogeneous transfer learning approaches ar e developed and proposed for handling the situations where the domains are of the same feature space. In homogeneous transfer learning, some studies assume that domains differ only in marginal distributions. Therefore, they adapt the d o- mains by correcting the sample selection bias [5] or covaria te shift [6]. However , this assumption does not hold in many cases. For example, in sentiment classiﬁcation problem, a word may have different meaning tendencies in different domains. This phenomenon is also called context feature bias [7]. T o solve this problem, some studies further adapt the conditional distributions. Heterogeneous transfer le arn- ing refers to the knowledge transfer process in the situatio ns where the domains have different feature spaces. In additio n to distribution adaptation, heterogeneous transfer learn ing requires feature space adaptation [7], which makes it more complicated than homogeneous transfer learning. The survey aims to give readers a comprehensive un- derstanding about transfer learning from the perspectives of data and model. The mechanisms and the strategies of transfer learning approaches are introduced to allow readers grasp how the approaches work. And a number of the existing transfer learning researches are connected an d systematized. Speciﬁcally , over forty representative tra nsfer learning approaches are introduced. Besides, we conduct experiments to demonstrate on which dataset a transfer learning model performs well. In this survey , we focus more on homogeneous transfer learning. Some interesting transfer learning topics are no t covered in this survey , such as reinforcement transfer lear n- ing [8], lifelong transfer learning [9], and online transfe r learning [10]. The rest of this survey are organized into seven sections. Section 2 clariﬁes the difference between transfer learning and other related machine learning tech- niques. Section 3 introduces the notations used in this sur- vey and the deﬁnitions about transfer learning. Sections 4 and 5 interpret transfer learning approaches from the data and the model perspectives, respectively . Section 6 intro- duces some applications of transfer learning. Experiments are conducted and the results are provided in Section 7. The last section concludes this survey . The main contributions of this survey are summarized below .  Over forty representative transfer learning approaches are introduced and summarized, which can give read- ers a comprehensive overview about transfer learning.  Experiments are conducted to compare different trans- fer learning approaches. The performance of over twenty different approaches is displayed intuitively and then analyzed, which may be instructive for the readers to select the appropriate ones in practice. 2 R ELATED WORK Some areas related to transfer learning are introduced. The connections and difference between them and transfer learn - ing are clariﬁed. Semi-Supervised Learning [11]: Semi-supervised learning is a machine learning task and method that lies between supervised learning (with completely labeled instances) and unsupervised learning (without any labeled instances) . T ypically , a semi-supervised method utilizes abundant un- labeled instances combined with a limited number of la- beled instances to train a learner . Semi-supervised learn- ing relaxes the dependence on labeled instances, and thus reduces the expensive labeling costs. Note that, in semi- supervised learning, both the labeled and unlabeled in- stances are drawn from the same distribution. In contrast, i n transfer learning, the data distributions of the source and the target domains are usually different. Many transfer learn- ing approaches absorb the technology of semi-supervised learning. The key assumptions in semi-supervised learning , i.e., smoothness, cluster , and manifold assumptions, are a lso made use of in transfer learning. It is worth mentioning that semi-supervised transfer learning is a controversial term. The reason is that the concept of whether the label information is available in transfer learning is ambiguous because both the source and the target domains can be involved. Multi-View Learning [12]: Multi-view learning focuses on the machine learning problems with multi-view data. A view represents a distinct feature set. An intuitive exampl e about multiple views is that a video object can be described from two different viewpoints, i.e., the image signal and the audio signal. Brieﬂy , multi-view learning describes an object from multiple views, which results in abundant in- formation. By properly considering the information from al l views, the learner s performance can be improved. There are several strategies adopted in multi-view learning such as subspace learning, multi-kernel learning, and co-train ing [13], [14]. Multi-view techniques are also adopted in some transfer learning approaches. For example, Zhang et al. pro- posed a multi-view transfer learning framework, which im- poses the consistency among multiple views [15]. Y ang and Gao incorporated multi-view information across different domains for knowledge transfer [16]. The work by Feuz and Cook introduces a multi-view transfer learning approach for activity learning, which transfers activity knowledge between heterogeneous sensor platforms [17]. Multi-T ask Learning [18]: The thought of multi-task learn- ing is to jointly learn a group of related tasks. T o be more speciﬁc, multi-task learning reinforces each task by taking advantage of the interconnections between task, i.e ., considering both the inter-task relevance and the inter-ta sk 3 difference. In this way , the generalization of each task is e n- hanced. The main difference between transfer learning and multi-task learning is that the former transfer the knowl- edge contained in the related domains, while the latter transfer the knowledge via simultaneously learning some related tasks. In other words, multi-task learning pays equ al attention to each task, while transfer learning pays more attention to the target task than to the source task. There ar e some commons and associations between transfer learning and multi-task learning. Both of them aim to improve the performance of learners via knowledge transfer . Besides, they adopt some similar strategies for constructing models , such as feature transformation and parameter sharing. Note that some existing studies utilize both the transfer learn- ing and the multi-task learning technologies. For example, the work by Zhang et al. employs multi-task and transfer learning techniques for biological image analysis [19]. Th e work by Liu et al. proposes a framework for human action recognition based on multi-task learning and multi-source transfer learning [20]. 3 O VERVIEW In this section, the notations used in this survey are listed for convenience. Besides, some deﬁnitions and categorization s about transfer learning are introduced, and some related surveys are also provided. 3.1 Notation For convenience, a list of symbols and their deﬁnitions are shown in T able 1. Besides, we use to represent the norm and superscript T to denote the transpose of a vectormatrix. 3.2 Deﬁnition In this subsection, some deﬁnitions about transfer learnin g are given. Before giving the deﬁnition of transfer learning , let us review the deﬁnitions of a domain and a task. Deﬁnition 1. (Domain) A domain Dis composed of two parts, i.e., a feature space Xand a marginal distribution P (X). In other words, D {X, P (X)}. And the symbol X denotes an instance set, which is deﬁned as X  {xxi X , i  1,  , n }. Deﬁnition 2. (T ask) A task T consists of a label space Yand a decision function f, i.e., T  {Y, f }. The decision function f is an implicit one, which is expected to be learned from the sample data. Some machine learning models actually output the pre- dicted conditional distributions of instances. In this cas e, f(xj )  {P (ykxj )yk Y, k  1 ,  , Y}. In practice, a domain is often observed by a number of instances withwithout the label information. For ex- ample, a source domain DS corresponding to a source task TS is usually observed via the instance-label pairs, i.e., DS  {(x, y )xi  XS , y i  YS, i  1 ,  , n S}; an observation of the target domain usually consists of a number of unlabeled instances andor limited number of labeled instances. Deﬁnition 3. (T ransfer Learning) Given somean observation(s) corresponding to mS  N source domain(s) and task(s) T ABLE 1 Notations. Symbol Deﬁnition n Number of instances m Number of domains D Domain T T ask X Feature space Y Label space x Feature vector y Label X Instance set Y Label set corresponding to X S Source domain T T arget domain L Labeled instances U Unlabeled instances H Reproducing kernel Hilbert space θ MappingCoefﬁcient vector α W eighting coefﬁcient β W eighting coefﬁcient λ T radeoff parameter δ ParameterError b Bias B Boundary parameter N IterationKernel number f Decision function L Loss function η Scale parameter G Graph Φ Nonlinear mapping σ Monotonically increasing function Ω Structural risk κ Kernel function K Kernel matrix H Centering matrix C Covariance matrix d Document w W ord z Class variable z Noise D Discriminator G Generator S Function M Orthonormal bases Θ Model parameters P Probability E Expectation Q Matrix variable R Matrix variable W Mapping matrix (i.e., {(DSi , TSi )i  1 ,  , m S}), and somean observa- tion(s) about mT  N target domain(s) and task(s) (i.e., {(DTj , TTj )j  1 ,  , m T }), transfer learning utilizes the knowledge implied in the source domain(s) to improve the performance of the learned decision functions fTj (j  1,  , m T ) on the target domain(s). The above deﬁnition, which covers the situation of multi- source transfer learning, is an extension of the one present ed in the survey [2]. If mS equals 1, the scenario is called single- source transfer learning. Otherwise, it is called multi-so urce transfer learning. Besides, mT represents the number of the transfer learning tasks. A few studies focus on the setting that mT 2 [21]. The existing transfer learning studies pay more attention to the scenarios where mT  1 (especially where mS  mT  1 ). It is worth mentioning that the observation of a domain or a task is a concept with broad 4 Transfer Learning Problem Categorization Solution Categorization Homogeneous Transfer Learning Heterogeneous Transfer Learning Inductive Transfer Learning Transductive Transfer Learning Unsupervised Transfer Learning Instance-Based Approach Feature-Based Approach Parameter-Based Approach Relational-Based Approach Symmetric Transformation Asymmetric Transformation Label-Setting-Based Categorization Space-Setting-Based Categorization Fig. 2. Categorizations of transfer learning. sense, which is often cemented into a labeledunlabeled instance set or a pre-learned model. A common scenario is that we have abundant labeled instances or have a well- trained model on the source domain, and we only have limited labeled target-domain instances. In this case, the resources such as the instances and the model are actually the observations, and the goal of transfer learning is to lea rn a more accurate decision function on the target domain. Another term commonly used in the transfer learning area is domain adaptation. Domain adaptation refers to the process that adapting one or more source domains to trans- fer knowledge and improve the performance of the target learner [4]. T ransfer learning often relies on the domain adaptation process, which attempts to reduce the differenc e between domains. 3.3 Categorization of T ransfer Learning There are several categorization criteria of transfer lear ning. For example, transfer learning problems can be divided into three categories, i.e., transductive, inductive, and un- supervised transfer learning [2]. The complete deﬁnitions of these three categories are presented in [2]. These three categories can be interpreted from a label-setting aspect. Roughly speaking, transductive transfer learning refers t o the situations where the label information only comes from the source domain. If the label information of the target- domain instances is available, the scenario can be catego- rized into inductive transfer learning. If the label inform a- tion is unknown for both the source and the target domains, the situation is known as unsupervised transfer learning. Another categorization is based on the consistency between the source and the target feature spaces and label spaces. If XS  XT and YS  YT , the scenario is termed as homogeneous transfer learning. Otherwise, if XS  XT orand YS  YT , the scenario is referred to as heteroge- neous transfer learning. According to the survey [2], the transfer learning ap- proaches can be categorized into four groups: instance- based, feature-based, parameter-based, and relational-b ased approaches. Instance-based transfer learning approaches are mainly based on the instance weighting strategy . Feature- based approaches transform the original features to create a new feature representation; they can be further divided int o two subcategories, i.e., asymmetric and symmetric feature - based transfer learning. Asymmetric approaches transform the source features to match the target ones. In contrast, symmetric approaches attempt to ﬁnd a common latent feature space and then transform both the source and the target features into a new feature representation. The parameter-based transfer learning approaches transfer th e knowledge at the modelparameter level. Relational-based transfer learning approaches mainly focus on the problems in relational domains. Such approaches transfer the logica l relationship or rules learned in the source domain to the target domain. For better understanding, Fig. 2 shows the above-mentioned categorizations of transfer learning. Some surveys are provided for the readers who want to have a more complete understanding of this ﬁeld. The survey by Pan and Y ang [2], which is a pioneering work, cat- egorizes transfer learning and reviews the research progre ss before 2010. The survey by W eiss et al. introduces and summarizes a number of homogeneous and heterogeneous transfer learning approaches [4]. Heterogeneous transfer learning is specially reviewed in the survey by Day and Khoshgoftaar [7]. Some surveys review the literatures re- lated to speciﬁc themes such as reinforcement learning [8], computational intelligence [22], and deep learning [23], [ 24]. Besides, a number of surveys focus on speciﬁc application scenarios including activity recognition [25], visual cat ego- rization [26], collaborative recommendation [27], comput er vision [24], and sentiment analysis [28]. Note that the organization of this survey does not strictly follow the above-mentioned categorizations. In the next two sections, transfer learning approaches are interprete d from the data and the model perspectives. Roughly speak- ing, data-based interpretation covers the above-mentione d instance-based and feature-based transfer learning ap- proaches, but from a broader perspective. Model-based interpretation covers the above-mentioned parameter-bas ed approaches. Since there are relatively few studies concern - ing relational-based transfer learning and the representa tive approaches are well introduced in [2], [4], this survey does not focus on relational-based approaches. 5 Covariance ... Geometric Structure Cluster Structure ... Data-Based Interpretation Objective Measurement Type Distribution Adaptation Data Property PreservationAdjustment Marginal Distribution Adaptation Conditional Distribution Adaptation Kullback-Leibler Divergence Maximum Mean Discrepancy Jensen-Shannon Divergence ... Statistical Property Strategy Instance Weighting Feature Transformation Feature Clustering Feature Alignment Feature Augmentation Feature Reduction Joint Distribution Adaptation Feature Replication ... Feature Encoding Bregman Divergence Feature Stacking Mean Manifold Structure ... Feature Mapping Estimation Method ... Heuristic Method Space Adaptation Feature Space Adaptation Label Space Adaptation Spectral Feature Alignment Subspace Feature Alignment ... ... Statistic Feature Alignment Feature Selection Fig. 3. Strategies and the objectives of the transfer learni ng approaches from the data perspective. 4 D ATA-BASED INTERPRETAT ION Many transfer learning approaches, especially the data- based approaches, focus on transferring the knowledge via the adjustment and the transformation of data. Fig. 3 shows the strategies and the objectives of the approaches from the data perspective. As shown in Fig. 3, space adaptation is one of the objectives. This objective is required to be sat- isﬁed mostly in heterogeneous transfer learning scenarios . In this survey , we focus more on homogeneous transfer learning, and the main objective in this scenario is to reduc e the distribution difference between the source-domain and the target-domain instances. Furthermore, some advanced approaches may attempt to preserve the data properties in the adaptation process. There are generally two strategies for realizing the objectives from the data perspective, i.e ., in- stance weighting and feature transformation. In this secti on, some related transfer learning approaches are introduced i n proper order according to the strategies shown in Fig. 3. 4.1 Instance Weighting Strategy Let us ﬁrst consider a simple scenario in which a large number of labeled source-domain and a limited number of target-domain instances are available and domains diffe r only in marginal distributions (i.e., P S(X)  P T (X) and P S(Y X)  P T (Y X)). For example, suppose we need to build a model to diagnose cancer in a speciﬁc region where the elderly are the majority . Limited target-domain instances are given, and relevant data are available from another region where young people are the majority . Di- rectly transferring all the data from another region may be unsuccessful, since the marginal distribution differen ce exists, and the elderly have a higher risk of cancer than younger people. In this scenario, it is natural to consider adapting the marginal distributions. A simple idea is to assign weights to the source-domain instances in the loss function. The weighting strategy is based on the following equation [5]: E(x,y )P T [L(x, y ; f)]  E(x,y )P S [ P T (x, y ) P S(x, y ) L(x, y ; f) ]  E(x,y )P S [ P T (x) P S(x) L(x, y ; f) ] . Therefore, the general objective function of a learning tas k can be written as [5]: min f 1 nS nS  i1 βiL ( f(xS i ), y S i )  Ω( f), where βi (i  1 , 2,  , n S) is the weighting parameter . The theoretical value of βi is equal to P T (xi)P S(xi). However , this ratio is generally unknown and is difﬁcult to be obtained by using the traditional methods. Kernel Mean Matching (KMM) [5], which is proposed by Huang et al. , resolves the estimation problem of the above unknown ratios by matching the means between the source- domain and the target-domain instances in a Reproducing Kernel Hilbert Space (RKHS), i.e., arg min β i[0,B ]             1 nS nS  i1 βiΦ( xS i )  1 nT nT  j1 Φ( xT j )             2 H s.t. 1 nS nS  i1 βi 1 δ, where δ is a small parameter , and B is a parameter for con- straint. The above optimization problem can be converted 6 into a quadratic programming problem by expanding and using the kernel trick. This approach to estimating the rati os of distributions can be easily incorporated into many exist - ing algorithms. Once the weight βi is obtained, a learner can be trained on the weighted source-domain instances. There are some other studies attempting to estimate the weights. For example, Sugiyama et al. proposed an approach termed Kullback-Leibler Importance Estimation Procedure (KLIEP) [6]. KLIEP depends on the minimization of the Kullback-Leibler (KL) divergence and incorporates a built-in model selection procedure. Based on the studies of weight estimation, some instance-based transfer learni ng frameworks or algorithms are proposed. For example, Sun et al. proposed a multi-source framework termed 2-Stage W eighting Framework for Multi-Source Domain Adaptation (2SW-MDA) with the following two stages [29]. 1. Instance Weighting : The source-domain instances are as- signed with weights to reduce the marginal distribution difference, which is similar to KMM. 2. Domain Weighting : W eights are assigned to each source domain for reducing the conditional distribution differ- ence based on the smoothness assumption [30]. Then, the source-domain instances are reweighted based on the instance weights and the domain weights. These reweighted instances and the labeled target-domain in- stances are used to train the target classiﬁer . In addition to directly estimating the weighting param- eters, adjusting weights iteratively is also effective. Th e key is to design a mechanism to decrease the weights of the instances that have negative effects on the target learner . A representative work is T rAdaBoost [31], which is a framework proposed by Dai et al . This framework is an extension of AdaBoost [32]. AdaBoost is an effective boosting algorithm designed for traditional machine learn - ing tasks. In each iteration of AdaBoost, a learner is traine d on the instances with updated weights, which results in a weak classiﬁer . The weighting mechanism of instances ensures that the instances with incorrect classiﬁcation ar e given more attention. Finally , the resultant weak classiﬁe rs are combined to form a strong classiﬁer . T rAdaBoost ex- tends the AdaBoost to the transfer learning scenario; a new weighting mechanism is designed to reduce the impact of the distribution difference. Speciﬁcally , in T rAdaBoost, the labeled source-domain and labeled target-domain instance s are combined as a whole, i.e., a training set, to train the weak classiﬁer . The weighting operations are different for the source-domain and the target-domain instances. In each iteration, a temporary variable δ, which measures the classi- ﬁcation error rate on the labeled target-domain instances, is calculated. Then, the weights of the target-domain instanc es are updated based on δ and the individual classiﬁcation results, while the weights of the source-domain instances a re updated based on a designed constant and the individual classiﬁcation results. For better understanding, the form ulas used in the k-th iteration ( k  1 ,  , N ) for updating the weights are presented repeatedly as follows [31]: βS k,i  βS k1,i (1   2 ln nSN )fk (xS i )yS i (i  1 ,  , n S), βT k,j  βT k1,j (δk (1 δk))fk (xT j )yT j (j  1 ,  , n T ). Note that each iteration forms a new weak classiﬁer . The ﬁnal classiﬁer is constructed by combining and ensembling half the number of the newly resultant weak classiﬁers through voting scheme. Some studies further extend T rAdaBoost. The work by Y ao and Doretto [33] proposes a Multi-Source T rAdaBoost (MsT rAdaBoost) algorithm, which mainly has the following two steps in each iteration. 1. Candidate Classiﬁer Construction : A group of candi- date weak classiﬁers are respectively trained on the weighted instances in the pairs of each source domain and the target domain, i.e., DSi DT (i  1 ,  , m S). 2. Instance Weighting : A classiﬁer (denoted by j and trained on DSj DT ) which has the minimal classi- ﬁcation error rate δ on the target domain instances is selected, and then is used for updating the weights of the instances in DSj and DT . Finally , the selected classiﬁers from each iteration are co m- bined to form the ﬁnal classiﬁer . Another parameter-based algorithm, i.e., T askT rAdaBoost, is also proposed in the work [33], which is introduced in Section 5.3. Some approaches realize instance weighting strategy in a heuristic way . For example, Jiang and Zhai proposed a general weighting framework [34]. There are three terms in the frameworks objective function, which are designed to minimize the cross-entropy loss of three types of instances . The following types of instances are used to construct the target classiﬁer .  Labeled T arget-domain Instance : The classiﬁer should mini- mize the cross-entropy loss on them, which is actually a standard supervised learning task.  Unlabeled T arget-domain Instance : These instances true con- ditional distributions P (yxT,U i ) are unknown and should be estimated. A possible solution is to train an auxiliary classiﬁer on the labeled source-domain and target-domain instances to help estimate the conditional distributions o r assign pseudo labels to these instances.  Labeled Source-domain Instance : The authors deﬁne the weight of xS,L i as the product of two parts, i.e., α i and βi. The weight βi is ideally equal to P T (xi)P S(xi), which can be estimated by non-parametric methods such as KMM or can be set uniformly in the worst case. The weight α i is used to ﬁlter out the source-domain instances that differ greatly from the target domain. A heuristic method can be used to produce the value of α i, which contains the following three steps. 1. Auxiliary Classiﬁer Construction : An auxiliary classiﬁer trained on the labeled target-domain instances are used to classify the unlabeled source-domain instances. 2. Instance Ranking : The source-domain instances are ranked based on the probabilistic prediction results. 3. Heuristic Weighting ( βi): The weights of the top- k source-domain instances with wrong predictions are set to zero, and the weights of others are set to one. 4.2 Feature T ransformation Strategy Feature transformation strategy is often adopted in featur e- based approaches. For example, consider a cross-domain text classiﬁcation problem. The task is to construct a targe t 7 T ABLE 2 Metrics Adopted in T ransfer Learning. Measurement Related Algorithms Maximum Mean Discrepancy [35] [36] [37] [38] [39]    Kullback-Leibler Divergence [40] [41] [42] [43] [44]    Jensen-Shannon Divergence [45] [46] [47] [48] [49]    Bregman Divergence [50] [51] [52] [53] [54]    Hilbert-Schmidt Independence Criterion [55] [36] [56] [57] [58]   classiﬁer by using the labeled text data from a related domain. In this scenario, a feasible solution is to ﬁnd the common latent features (e.g., latent topics) through fea- ture transformation and use them as a bridge to transfer knowledge. Feature-based approaches transform each orig- inal feature into a new feature representation for knowl- edge transfer . The objectives of constructing a new feature representation include minimizing the marginal and the conditional distribution difference, preserving the prop er- ties or the potential structures of the data, and ﬁnding the correspondence between features. The operations of featur e transformation can be divided into three types, i.e., featu re augmentation, feature reduction, and feature alignment. B e- sides, feature reduction can be further divided into severa l types such as feature mapping, feature clustering, feature selection, and feature encoding. A complete feature trans- formation process designed in an algorithm may consist of several operations. 4.2.1 Distribution Difference Metric One primary objective of feature transformation is to reduc e the distribution difference of the source and the target do- main instances. Therefore, how to measure the distribution difference or the similarity between domains effectively i s an important issue. The measurement termed Maximum Mean Discrepancy (MMD) is widely used in the ﬁeld of transfer learning, which is formulated as follows [35]: MMD(XS, X T )              1 nS nS  i1 Φ( xS i )  1 nT nT  j1 Φ( xT j )             2 H . MMD can be easily computed by using kernel trick. Brieﬂy , MMD quantiﬁes the distribution difference by calculating the distance of the mean values of the instances in a RKHS. Note that the above-mentioned KMM actually produces the weights of instances by minimizing the MMD distance between domains. T able. 2 lists some commonly used metrics and the related algorithms. In addition to T able. 2, there are some other measurement criteria adopted in transfer learning, including W asserstein distance [59], [60], central moment discrepancy [61], etc. Some studies focus on optimizing and improving the existing measurements. T ake MMD as an example. Gretton et al. proposed a multi-kernel version of MMD, i.e., MK-MMD [62], which takes advantage of multiple kernels. Besides, Y an et al. proposed a weighted version of MMD [63], which attempts to address the issue of class weight bias. 4.2.2 Feature Augmentation Feature augmentation operations are widely used in fea- ture transformation, especially in symmetric feature-bas ed approaches. T o be more speciﬁc, there are several ways to realize feature augmentation such as feature replication a nd feature stacking. For better understanding, we start with a simple transfer learning approach which is established based on feature replication. The work by Daum  e proposes a simple domain adap- tation method, i.e., Feature Augmentation Method (F AM) [64]. This method transforms the original features by sim- ple feature replication. Speciﬁcally , in single-source tr ansfer learning scenario, the feature space is augmented to three times its original size. The new feature representation con - sists of general features, source-speciﬁc features, and ta rget- speciﬁc features. Note that, for the transformed source- domain instances, their target-speciﬁc features are set to zero. Similarly , for the transformed target-domain instan ces, their source-speciﬁc features are set to zero. The new featu re representation of F AM is presented as follows: Φ S(xS i )  xS i , xS i , 0, Φ T (xT j )  xT j , 0, xT j , where Φ S and Φ T denote the mappings to the new feature space from the source and the target domain, respectively . The ﬁnal classiﬁer is trained on the transformed labeled instances. It is worth mentioning that this augmentation method is actually redundant. In other words, augmenting the feature space in other ways (with fewer dimensions) may be able to produce competent performance. The supe- riority of F AM is that its feature expansion has an elegant form, which results in some good properties such as the generalization to multi-source scenarios. An extension of F AM is proposed in [65] by Daum  e et al. , which utilizes the unlabeled instances to further facilitate the knowledg e transfer process. However , F AM may not work well in handling het- erogeneous transfer learning tasks. The reason is that di- rectly replicating features and padding zero vectors are le ss effective when the source and the target domains have different feature representations. T o solve this problem, Li et al. proposed an approach termed Heterogeneous Feature Augmentation (HF A) [66], [67]. The feature representation of HF A is presented below: Φ S(xS i )  W SxS i , xS i , 0T , Φ T (xT j )  W T xT j , 0S, xT j , where W SxS i and W T xT j have the same dimension; 0S and 0T denote the zero vectors with the dimensions of xS and xT , respectively . HF A maps the original features into a common feature space, and then performs a feature stacking operation. The mapped features, original features, and zer o elements are stacked in a particular order to produce a new feature representation. 4.2.3 Feature Mapping In the ﬁeld of traditional machine learning, there are many feasible mapping-based methods of extracting fea- tures such as Principal Component Analysis (PCA) [68] and Kernelized-PCA (KPCA) [69]. However , these methods mainly focus on the data variance rather than the distribu- tion difference. In order to solve the distribution differe nce, 8 some feature extraction methods are proposed for transfer learning. Let us ﬁrst consider a simple scenario where there is little difference in the conditional distributions of th e do- mains. In this case, the following simple objective functio n can be used to ﬁnd a mapping for feature extraction: min Φ ( DIST(XS, X T ; Φ)  λΩ(Φ) )  ( V AR(XS XT ; Φ) ) , where Φ is a low-dimensional mapping function, DIST () represents a distribution difference metric, Ω(Φ) is a regular- izer controlling the complexity of Φ , and V AR () represents the variance of instances. This objective function aims to ﬁnd a mapping function Φ that minimizes the marginal distribution difference between domains and meanwhile makes the variance of the instances as large as possible. The objective corresponding to the denominator can be opti- mized in several ways. One possible way is to optimize the objective of the numerator with a variance constraint. For example, the scatter matrix of the mapped instances can be enforced as an identity matrix. Another way is to optimize the objective of the numerator in a high-dimensional featur e space at ﬁrst. Then, a dimension reduction algorithm such as PCA or KPCA can be performed to realize the objective of the denominator . Further , ﬁnding the explicit formulation of Φ( ) is non- trivial. T o solve this problem, some approaches adopt linea r mapping technique or turn to the kernel trick. In general, there are three main ideas to deal with the above optimiza- tion problems.  (Mapping Learning  Feature Extraction) A possible way is to ﬁnd a high-dimensional space at ﬁrst where the objec- tives are met by solving a kernel matrix learning problem or a transformation matrix ﬁnding problem. Then, the high-dimensional features are compacted to form a low- dimensional feature representation. For example, once the kernel matrix is learned, the principal components of the implicit high-dimensional features can be extracted to construct a new feature representation based on PCA.  (Mapping Construction  Mapping Learning) Another way is to map the original features to a constructed high- dimensional feature space, and then a low-dimensional mapping is learned to satisfy the objective function. For example, a kernel matrix can be constructed based on a selected kernel function at ﬁrst. Then, the transfor- mation matrix can be learned, which projects the high- dimensional features into a common latent subspace.  (Direct Low-dimensional Mapping Learning) It is usu- ally difﬁcult to ﬁnd a desired low-dimensional mapping directly . However , if the mapping is assumed to satisfy certain conditions, it may be solvable. For example, if the low-dimensional mapping is restricted to be a linear one, the optimization problem can be easily solved. Some approaches also attempt to match the conditional distributions and preserve the structures of the data. T o achieve this, the above simple objective function needs to incorporate new terms orand constraints. For example, the following general objective function is a possible choice: min Φ µDIST(XS, X T ; Φ)  λ1Ω GEO (Φ)  λ2Ω(Φ)  (1 µ)DIST(Y SXS, Y T XT ; Φ) , s.t. Φ( X)T HΦ( X)  I, with H  I (1n ) Rnn, where µ is a parameter balancing the marginal and the conditional distribution difference [70], Ω GEO(Φ) is a reg- ularizer controlling the geometric structure, Φ( X) is the matrix whose rows are the instances from both the source and the target domains with the extracted new feature representation, H is the centering matrix for constructing the scatter matrix, and the constraint is used to maximize the variance. The last term in the objective function denote s the measurement of the conditional distribution differenc e. Before the further discussion about the above objective function, it is worth mentioning that the label information of the target-domain instances is often limited or even unknown. The lack of the label information makes it difﬁcult to estimate the distribution difference. In order to solve this problem, some approaches resort to the pseudo-label strategy , i.e., assigning pseudo labels to the unlabeled ta rget- domain instances. A simple method of realizing this is to train a base classiﬁer to assign pseudo labels. By the way , there are some other methods of providing pseudo labels such as co-training [71], [72] and tri-training [73] , [74]. Once the pseudo-label information is complemented, the conditional distribution difference can be measured. F or example, MMD can be modiﬁed and extended to measure the conditional distribution difference. Speciﬁcally , fo r each label, the source-domain and the target-domain instances that belong to the same class are collected, and the estima- tion expression of the conditional distribution differenc e is given by [38]: Y k1             1 nS k nS k i1 Φ( xS i )  1 nT k nT k j1 Φ( xT j )             2 H , where nS k and nT k denote the numbers of the instances in the source and the target domains with the same label Yk, respectively . This estimation actually measures the class - conditional distribution (i.e., P (xy)) difference to approx- imate the conditional distribution (i.e., P (yx)) difference. Some studies improve the above estimation. For example, the work by W ang et al. uses a weighted method to ad- ditionally solve the class imbalance problem [70]. For bett er understanding, the transfer learning approaches that are t he special cases of the general objective function presented i n the previous paragraph are detailed as follows.  (µ  1 and λ1  0 ) The objective function of Maximum Mean Discrepancy Embedding (MMDE) is given by [75]: min K MMD(XS, X T ; Φ)  λ1 nS  nT  ij Φ( xi) Φ( xj )2 s.t. (xi k-NN(xj )) (xj k-NN(xi)), Φ( xi) Φ( xj )2  xi xj 2, (xi, xj XS XT ), where k-NN(x) represents the k nearest neighbors of the instance x. The authors design the above objective func- tion motivated by Maximum V ariance Unfolding (MVU) [76]. Instead of employing a scatter matrix constraint, the constraints and the second term of this objective function aim to maximize the distance between instances as well as preserve local geometry . The desired kernel matrix K can be learned by solving a Semi-Deﬁnite Programming (SDP) [77] problem. After obtaining the kernel matrix, 9 PCA is applied to it, and then the leading eigenvectors are selected to help construct a low-dimensional feature representation.  (µ  1 and λ1  0 ) The work by Pan et al. proposes an approach termed T ransfer Component Analysis (TCA) [36], [78]. TCA adopts MMD to measure the marginal dis- tribution difference and enforces the scatter matrix as the constraint. Different from MMDE that learns the kernel matrix and then further adopts PCA, TCA is a uniﬁed method that just needs to learn a linear mapping from an empirical kernel feature space to a low-dimensional fea- ture space. In this way , it avoids solving the SDP problem, which results in relatively low computational burden. The ﬁnal optimization problem can be easily solved via eigen- decomposition. TCA can also be extended to utilize the label information. In the extended version, the scatter matrix constraint is replaced by a new one that balances the label dependence (measured by HSIC) and the data variance. Besides, a graph Laplacian regularizer [30] is also added to preserve the geometry of the manifold. Sim- ilarly , the ﬁnal optimization problem can also be solved by eigen-decomposition.  (µ  0 . 5 and λ1  0 ) Long et al. proposed an ap- proach termed Joint Distribution Adaptation (JDA) [38]. JDA attempts to ﬁnd a transformation matrix that maps the instances to a low-dimensional space where both the marginal and the conditional distribution difference are minimized. T o realize it, the MMD metric and the pseudo- label strategy are adopted. The desired transformation matrix can be obtained by solving a trace optimization problem via eigen-decomposition. Further , it is obvious that the accuracy of the estimated pseudo labels affects the performance of JDA. In order to improve the labeling quality , the authors adopt the iterative reﬁnement oper- ations. Speciﬁcally , in each iteration, JDA is performed, and then a classiﬁer is trained on the instances with the extracted features. Next, the pseudo labels are updated based on the trained classiﬁer . After that, JDA is per- formed repeatedly with the updated pseudo labels. The iteration ends when convergence occurs. Note that JDA can be extended by utilizing the label and structure infor- mation [79], clustering information [80], various statist ical and geometrical information [81], etc.  (µ (0, 1) and λ1  0 ) The paper by W ang et al. proposes an approach termed Balanced Distribution Adaptation (BDA) [70], which is an extension of JDA. Different from JDA which assumes that the marginal and the conditional distributions have the same importance in adaptation, BDA attempts to balance the marginal and the condi- tional distribution adaptation. The operations of BDA are similar to JDA. In addition, the authors also proposed the W eighted BDA (WBDA). In WBDA, the conditional distribution difference is measured by a weighted version of MMD to solve the class imbalance problem. It is worth mentioning that some approaches transform the features into a new feature space (usually of a high dimension) and train an adaptive classiﬁer simultaneously . T o realize this, the mapping function of the features and the decision function of the classiﬁer need to be associated. On e possible way is to deﬁne the following decision function: f(x)  θ Φ( x)b, where θ denotes the classiﬁer parameter; b denotes the bias. In light of the representer theorem [82], the parameter θ can be deﬁned as θ   n i1 α iΦ( xi), and thus we have f(x)  n i1 α iΦ( xi) Φ( x)  b  n i1 α iκ(xi, x)  b, where κ denotes the kernel function. By using the kernel matrix as the bridge, the regularizers designed for the map- ping function can be incorporated into the classiﬁer s obje c- tive function. In this way , the ﬁnal optimization problem is usually about the parameter (e.g., α i) or the kernel function. For example, the paper by Long et al. proposes a general framework termed Adaptation Regularization Based T rans- fer Learning (ARTL) [39]. The goals of ARTL are to learn the adaptive classiﬁer , to minimize the structural risk, to jointly reduce the marginal and the conditional distributi on difference, and to maximize the manifold consistency be- tween the data structure and the predictive structure. The authors also proposed two speciﬁc algorithms under this framework based on different loss functions. In these two algorithms, the coefﬁcient matrix for computing MMD and the graph Laplacian matrix for manifold regularization are constructed at ﬁrst. Then, a kernel function is selected to construct the kernel matrix. After that, the classiﬁer lear ning problem is converted into a parameter (i.e., α i) solving problem, and the solution formula is also given in [39]. In ARTL, the choice of the kernel function affects the performance of the ﬁnal classiﬁer . In order to construct a robust classiﬁer , some studies turn to kernel learning. For example, the paper by Duan et al. proposes a uni- ﬁed framework termed Domain T ransfer Multiple Kernel Learning (DTMKL) [83]. In DTMKL, the kernel function is assumed to be a linear combination of a group of base kernels, i.e., κ(xi, xj )   N k1 βkκk(xi, xj ). DTMKL aims to minimize the distribution difference, the classiﬁcatio n error , etc., simultaneously . The general objective functi on of DTMKL can be written as follows: min β k,f σ ( MMD(XS, X T ; κ) )  λΩ L(βk, f ), where σ is any monotonically increasing function, f is the decision function with the same deﬁnition as the one in ARTL, and Ω L(βk, f ) is a general term representing a group of regularizers deﬁned on the labeled instances such as the ones for minimizing the classiﬁcation error and controllin g the complexity of the resultant model. The authors devel- oped an algorithm to learn the kernel and the decision function simultaneously by using the reduced gradient de- scent method [84]. In each iteration, the weight coefﬁcient s of base kernels are ﬁxed to update the decision function at ﬁrst. Then, the decision function is ﬁxed to update the weight coefﬁcients. Note that DTMKL can incorporate many existing kernel methods. The authors proposed two speciﬁc algorithms under this framework. The ﬁrst one implements the framework by using hinge loss and Support V ector Machine (SVM). The second one is an extension of the ﬁrst one with an additional regularizer utilizing pseudo- label information, and the pseudo labels of the unlabeled instances are generated by using base classiﬁers. 10 4.2.4 Feature Clustering Feature clustering aims to ﬁnd a more abstract feature representation of the original features. Although it can be regarded as a way of feature extraction, it is different from the above-mentioned mapping-based extraction. For example, some transfer learning approaches implic- itly reduce the features by using the co-clustering tech- nique, i.e., simultaneously clustering both the columns an d rows of (or say , co-cluster) a contingency table based on the information theory [85]. The paper by Dai et al. [41] proposes an algorithm termed Co-Clustering Based Clas- siﬁcation (CoCC), which is used for document classiﬁca- tion. In a document classiﬁcation problem, the transfer learning task is to classify the target-domain documents (represented by a document-to-word matrix) with the help of the labeled source document-to-word data. CoCC re- gards the co-clustering technique as a bridge to transfer the knowledge. In CoCC algorithm, both the source and the target document-to-word matrices are co-clustered. Th e source document-to-word matrix is co-clustered to generat e the word clusters based on the known label information, and these word clusters are used as constraints during the co-clustering process of the target-domain data. The co- clustering criterion is to minimize the loss in mutual infor - mation, and the clustering results are obtained by iteratio n. Each iteration contains the following two steps. 1. Document Clustering : Each row of the target document- to-word matrix is re-ordered based on the objective function for updating the document clusters. 2. Word Clustering : The word clusters are adjusted to min- imize the joint mutual-information loss of the source and the target document-word matrices. After several times of iterations, the algorithm converges , and the classiﬁcation results are obtained. Note that, in CoCC, the word clustering process implicitly extracts the word features to form uniﬁed word clusters. Dai et al. also proposed an unsupervised clustering ap- proach, which is termed as Self-T aught Clustering (STC) [42]. Similar to CoCC, this algorithm is also a co-clusterin g- based one. However , STC does not need the label infor- mation. STC aims to simultaneously co-cluster the source- domain and the target-domain instances with the assump- tion that these two domains share the same feature clusters in their common feature space. Therefore, two co-clusterin g tasks are separately performed at the same time to ﬁnd the shared feature clusters. Each iteration of STC has the following steps. 1. Instance Clustering : The clustering results of the source- domain and the target domain instances are updated to minimize their respective loss in mutual information. 2. Feature Clustering : The feature clusters are updated to minimize the joint loss in mutual information. When the algorithm converges, the clustering results of the target-domain instances are obtained. Different from the above-mentioned co-clustering-based ones, some approaches extract the original features into co n- cepts (or topics). In the document classiﬁcation problem, t he concepts represent the high-level abstractness of the word s (e.g., word clusters). In order to introduce the concept-ba sed transfer learning approaches easily , let us brieﬂy review t he Latent Semantic Analysis (LSA) [86], the Probabilistic LSA (PLSA) [87], and the Dual-PLSA [88].  LSA: LSA is an approach to mapping the document-to- word matrix to a low-dimensional space (i.e., a latent se- mantic space) based on the SVD technique. In short, LSA attempts to ﬁnd the true meanings of the words. T o realize this, SVD technique is used to reduce the dimensionality , which can remove the irrelevant information and ﬁlter out the noise information from the raw data.  PLSA: PLSA is developed based on a statistical view of LSA. PLSA assumes that there is a latent class variable z, which reﬂects the concept, associating the document d and the word w. Besides, d and w are independently con- ditioned on the concept z. The diagram of this graphical model is presented as follows: d P (dizk)  P (zk )  z P (wj zk) w, where the subscripts i, j and k represent the indexes of the document, the word, and the concept, respectively . PLSA constructs a Bayesian network, and the parameters are estimated by using the Expectation-Maximization (EM) algorithm [89].  Dual-PLSA: The Dual-PLSA is an extension of PLSA. This approach assumes there are two latent variables zd and zw associating the documents and the words. Speciﬁcally , the variables zd and zw reﬂect the concepts behind the documents and the words, respectively . The diagram of the Dual-PLSA is provided below: d P (dizd k1 ) zd P (zd k1 ,z w k2 ) zw P (wj zw k2 ) w. The parameters of the Dual-PLSA can also be obtained based on the EM algorithm. Some concept-based transfer learning approaches are established based on PLSA. For example, the paper by Xue et al. proposes a cross-domain text classiﬁcation approach termed T opic-Bridged Probabilistic Latent Semantic Analy - sis (TPLSA) [90]. TPLSA, which is an extension of PLSA, assumes that the source-domain and the target-domain instances share the same mixing concepts of the words. Instead of performing two PLSAs for the source domain and the target domain separately , the authors merge those two PLSAs as an integrated one by using the mixing concept z as a bridge, i.e., each concept has some probabilities to produ ce the source-domain and the target-domain documents. The diagram of TPLSA is provided below:    dS dT տ ւ P (dS i zk)        P (dT i zk) z P (zkwj ) w. Note that PLSA does not require the label information. In order to exploit the label information, the authors add the concept constraints, which include must-link and cannot- link constraints, as the penalty terms in the objective function of TPLSA. Finally , the objective function is iter- atively optimized to obtain the classiﬁcation results (i.e ., arg maxzP (zdT i )) by using the EM algorithm. The work by Zhuang et al. proposes an approach termed Collaborative Dual-PLSA (CD-PLSA) for multi-domain text classiﬁcation ( mS source domains and mT target domains) 11 [91], [92]. CD-PLSA is an extension of Dual-PLSA. Its dia- gram is shown below: P (Dk0 )  D  P (dizd k1 , Dk0 )  d zd P (zd k1 ,z w k2 ) zw  P (wj zw k2 , Dk0 )  w ց ր , where 1  k0  mS  mT denotes the domain index. The domain Dconnects both the variables d and w, but is independent of the variables zd and zw. The label in- formation of the source-domain instances is utilized by initializing the value P (dizd k1 , Dk0 ) (k0  1 ,  , m S). Due to the lack of the target-domain label information, the value P (dizd k1 , Dk0 ) (k0  mS  1 ,  , m S  mT ) can be initialized based on any supervised classiﬁer . Similarl y , the authors adopt the EM algorithm to ﬁnd the param- eters. Through the iterations, all the parameters in the Bayesian network are obtained. Thus, the class label of the i-th document in a target domain (denoted by Dk) can be predicted by computing the posterior probabilities, i.e ., arg maxzd P (zddi, Dk). Zhuang et al. further proposed a general framework that is termed as Homogeneous-Identical-Distinct-Concep t Model (HIDC) [93]. This framework is also an extension of Dual-PLSA. HIDC is composed of three generative models, i.e., identical-concept, homogeneous-concept, and disti nct- concept models. These three graphical models are presented below: Identical-Concept Model: D d zd ց ր zw IC w, Homogeneous-Concept Model: ր ց D d zd ց ր zw HC w, Distinct-Concept Model: ր ց ց D d zd ց ր zw DC w . The original word concept zw is divided into three types, i.e., zw IC, zw HC , and zw DC. In the identical-concept model, the word distributions only rely on the word concepts, and the word concepts are independent of the domains. However , in the homogeneous-concept model, the word distributions also depend on the domains. The difference between the identical and the homogeneous concepts is that zw IC is di- rectly transferable, while zw HC is the domain-speciﬁc transfer- able one that may have different effects on the word distri- butions for different domains. In the distinct-concept mod el, zw DC is actually the nontransferable domain-speciﬁc one, which may only appear in a speciﬁc domain. The above- mentioned three models are combined as an integrated one, i.e., HIDC. Similar to other PLSA-related algorithms, HIDC also uses EM algorithm to get the parameters. 4.2.5 Feature Selection Feature selection is another kind of operation for feature reduction, which is used to extract the pivot features. The pivot features are the ones that behave in the same way in different domains. Due to the stability of these features , they can be used as the bridge to transfer the knowledge. For example, Blitzer et al. proposed an approach termed Structural Correspondence Learning (SCL) [94]. Brieﬂy , SC L consists of the following steps to construct a new feature representation. 1. Feature Selection : SCL ﬁrst performs feature selection operations to obtain the pivot features. 2. Mapping Learning : The pivot features are utilized to ﬁnd a low-dimensional common latent feature space by using the structural learning technique [95]. 3. Feature Stacking : A new feature representation is con- structed by feature augmentation, i.e., stacking the original features with the obtained low-dimensional features. T ake the part-of-speech tagging problem as an example. The selected pivot features should occur frequently in source and target domains. Therefore, determiners can be included in pivot features. Once all the pivot features are deﬁned and selected, a number of binary linear classiﬁers whose function is to predict the occurrence of each pivot feature a re constructed. Without losing generality , the decision func tion of the i-th classiﬁer , which is used to predict the i-th pivot feature, can be formulated as fi(x)  sign(θi x), where x is assumed to be a binary feature input. And the i-th classiﬁer is trained on all the instances excluding the features deriv ed from the i-th pivot feature. The following formula can be used to estimate the i-th classiﬁer s parameters, i.e., θi  arg min θ 1 n n j1 L(θ xj , Rowi(xj ))  λθ2, where Row i(xj ) denotes the true value of the unlabeled instance xj in terms of the i-th pivot feature. By stacking the obtained parameter vectors as column elements, a matrix W is obtained. Next, based on singular value decomposition (SVD), the top- k left singular vectors, which are the prin- cipal components of the matrix W , are taken to construct the transformation matrix W . At last, the ﬁnal classiﬁer is trained on the labeled instances in an augmented feature space, i.e., ([xL i ; W T xL i ]T , y L i ). 4.2.6 Feature Encoding In addition to feature extraction and selection, feature en - coding is also an effective tool. For example, autoencoders , which are often adopted in deep learning area, can be used for feature encoding. An autoencoder consists of an encoder and a decoder . The encoder tries to produce a more abstract representation of the input, while the decoder aims to map back that representation and to minimize the reconstructio n error . Autoencoders can be stacked to build a deep learning architecture. Once an autoencoder completes the training process, another autoencoder can be stacked at the top of it. The newly added autoencoder is then trained by using the encoded output of the upper-level autoencoder as its input. In this way , deep learning architectures can thus be constructed. Some transfer learning approaches are developed based on autoencoders. For example, the paper by Glorot et al. proposes an approach termed Stacked Denoising Autoen- coder (SDA) [96]. The denoising autoencoder , which can enhance the robustness, is an extension of the basic one [97] . This kind of autoencoder contains a randomly corrupting mechanism that adds noise to the input before mapping. For 12 example, an input can be corrupted or partially destroyed by adding a masking noise or Gaussian noise. The denoising autoencoder is then trained to minimize the denoising re- construction error between the original clean input and the output. The SDA algorithm proposed in the paper mainly encompasses the following steps. 1. Autoencoder T raining : The source-domain and target- domain instances are used to train a stack of denoising autoencoders in a greedy layer-by-layer way . 2. Feature Encoding  Stacking: A new feature representa- tion is constructed by stacking the encoding output of intermediate layers, and the features of the instances are transformed into the obtained new representation. 3. Learner T raining : The target classiﬁer is trained on the transformed labeled instances. Although the SDA algorithm has excellent performance for feature extraction, it still has some drawbacks such as high computational and parameter-estimation cost. In orde r to shorten the training time and to speed up traditional SDA algorithms, Chen et al. proposed a modiﬁed version of SDA, i.e., Marginalized Stacked Linear Denoising Au- toencoder (mSLDA) [98], [99]. This algorithm adopts linear autoencoders and marginalizes the randomly corrupting step in a closed form. It may seem that linear autoencoders are too simple to learn complex features. However , the authors observe that linear autoencoders are often sufﬁcie nt to achieve competent performance when encountering high dimensional data. The basic architecture of mSLDA is a single-layer linear autoencoder . The corresponding singl e- layer mapping matrix W (augmented with a bias column for convenience) should minimize the expected squared reconstruction loss function, i.e., W  arg min W 1 2n n i1 EP (xix) [ xi W xi2] , where xi denotes the corrupted version of the input xi. The solution of W is given by [98], [99]: W  ( n i1 xiE[xi]T )( n i1 E [ xi xT i ] ) 1 . When the corruption strategy is determined, the above for- mulas can be further expanded and simpliﬁed into a speciﬁc form. Note that, in order to insert nonlinearity , a nonlinea r function is used to squash the output of each autoencoder after we obtain the matrix W in a closed form. Then, the next linear autoencoder is stacked to the current one in a similar way to SDA. In order to deal with high dimensional data, the authors also put forward an extension approach to further reduce the computational complexity . 4.2.7 Feature Alignment Note that feature augmentation and feature reduction mainly focus on the explicit features in a feature space. In contrast, in addition to the explicit features, feature alignment also focuses on some implicit features such as the statistic features and the spectral features. Therefor e, feature alignment can play various roles in the feature transformation process. For example, the explicit feature s can be aligned to generate a new feature representation, or the implicit features can be aligned to construct a satisﬁed feature transformation. There are several kinds of features that can be aligned, which includes subspace features, spectral features, and statistic features. T ake the subspace feature alignment as an example. A typical approach mainly has the following steps. 1. Subspace Generation : In this step, the instances are used to generate the respective subspaces for the source and the target domains. The orthonormal bases of the source and the target domain subspaces are then obtained, which are denoted by MS and MT , respectively . These bases are used to learn the shift between the subspaces. 2. Subspace Alignment : In the second step, a mapping, which aligns the bases MS and MT of the subspaces, is learned. And the features of the instances are pro- jected to the aligned subspaces to generate new feature representation. 3. Learner T raining: Finally , the target learner is trained on the transformed instances. For example, the work by Fernando et al. proposes an approach termed Subspace Alignment (SA) [100]. In SA, the subspaces are generated by performing PCA; the bases MS and MT are obtained by selecting the leading eigenvectors. Then, a transformation matrix W is learned to align the subspaces, which is given by [100]: W  arg min W MSW MT 2 F  MT SMT , where  F denotes the Frobenius norm. Note that the matrix W aligns MS with MT , or say , transforms the source subspace coordinate system into the target subspace coor- dinate system. The transformed low-dimensional source- domain and target-domain instances are given by XSMSW and XT MT , respectively . Finally , a learner can be trained on the resultant transformed instances. In light of SA, a number of transfer learning approaches are established. For example, the paper by Sun and Saenko proposes an approach that aligns both the subspace bases and the distributions [101], which is termed as Subspace Distribution Alignment between T wo Subspaces (SDA-TS). In SDA-TS, the transformation matrix W is formulated as W  MT SMT Q, where Q is a matrix used to align the distribution difference. The transformation matrix W in SA is a special case of the one in SDA-TS by setting Q to an identity matrix. Note that SA is a symmetrical feature-base d approach, while SDA-TS is an asymmetrical one. In SDA- TS, the labeled source-domain instances are projected to th e source subspace, then mapped to the target subspace, and ﬁnally mapped back to the target domain. The transformed source-domain instances are formulated as XSMSW M T T . Another representative subspace feature alignment ap- proach is Geodesic Flow Kernel (GFK) [102], which is pro- posed by Gong et al . GFK is closely related to a previous ap- proach termed Geodesic Flow Subspaces (GFS) [103]. Before introducing GFK, let us review the steps of GFS at ﬁrst. GFS is inspired by incremental learning. Intuitively , utilizi ng the information conveyed by the potential path between two domains may be beneﬁcial to the domain adaptation. GFS generally takes the following steps to align features. 13 1. Subspace Generation : GFS ﬁrst generates two subspaces of the source and the target domains by performing PCA, respectively . 2. Subspace Interpolation : The two obtained subspaces can be viewed as two points on the Grassmann manifold [104]. A ﬁnite number of the interpolated subspaces are generated between these two subspaces based on the geometric properties of the manifold. 3. Feature Projection  Stacking: The original features are transformed by stacking the corresponding projections from all the obtained subspaces. Despite the usefulness and superiority of GFS, there is a problem about how to determine the number of the interpo- lated subspaces. GFK resolves this problem by integrating inﬁnite number of the subspaces located on the geodesic curve from the source subspace to the target one. The key of GFK is to construct an inﬁnite-dimensional feature space that incorporating the information of all the subspaces lyi ng on the geodesic ﬂow . In order to compute the inner product in the resultant inﬁnite-dimensional space, the geodesic- ﬂow kernel is deﬁned and derived. In addition, a subspace- disagreement measure is proposed to select the optimal dimensionality of the subspaces; a rank-of-domain metric is also proposed to select the optimal source domain when multi-source domains are available. Statistic feature alignment is another kind of feature alignment. For example, Sun et al. proposed an approach termed Co-Relation Alignment (CORAL) [105]. CORAL constructs the transformation matrix of the source feature s by aligning the second-order statistic features, i.e., the co- variance matrices. The transformation matrix W is given by [105]: W  arg min W W T CS W CT 2 F , where C denotes the covariance matrix. Note that, com- pared to the above subspace-based approaches, CORAL avoids subspace generation as well as projection and is very easy to implement. Some transfer learning approaches are established based on spectral feature alignment. In traditional machine lear n- ing area, spectral clustering is a clustering technique bas ed on graph theory . The key of this technique is to utilize the spectrum, i.e., eigenvalues, of the similarity matrix t o reduce the dimension of the features before clustering. The similarity matrix is constructed to quantitatively assess the relative similarity of each pair of datavertices. On the basis of spectral clustering and feature alignment, Spectr al Feature Alignment (SF A) [106] is proposed by Pan et al . SF A is an algorithm for sentiment classiﬁcation. This algorith m tries to identify the domain-speciﬁc words and domain- independent words in different domains, and then aligns these domain-speciﬁc word features to construct a low- dimensional feature representation. SF A generally contai ns the following ﬁve steps. 1. Feature Selection : In this step, feature selection operations are performed to select the domain- independentpivot features. The paper presents three strategies to select domain-independent features. These strategies are based on the occurrence frequency of words, the mutual information between features and labels [107], and the mutual information between fea- tures and domains, respectively . 2. Similarity Matrix Construction : Once the domain-speciﬁc and the domain-independent features are identiﬁed, a bipartite graph is constructed. Each edge of this bipar- tite graph is assigned with a weight that measures the co-occurrence relationship between a domain-speciﬁc word and a domain-independent word. Based on the bipartite graph, a similarity matrix is then constructed. 3. Spectral Feature Alignment : In this step, a spectral clus- tering algorithm is adapted and performed to align domain-speciﬁc features [108], [109]. Speciﬁcally , based on the eigenvectors of the graph Laplacian, a feature alignment mapping is constructed, and the domain- speciﬁc features are mapped into a low-dimensional feature space. 4. Feature Stacking : The original features and the low- dimensional features are stacked to produce the ﬁnal feature representation. 5. Learner T raining : The target learner is trained on the labeled instances with the ﬁnal feature representation. There are some other spectral transfer learning ap- proaches. For example, the work by Ling et al. proposes an approach termed Cross-Domain Spectral Classiﬁer (CDSC) [110]. The general ideas and steps of this approach are presented as follows. 1. Similarity Matrix Construction : In the ﬁrst step, two similarity matrices are constructed corresponding to the whole instances and the target-domain instances, respectively . 2. Spectral Feature Alignment : An objective function is de- signed with respect to a graph-partition indicator vec- tor; a constraint matrix is constructed, which contains pair-wise must-link information. Instead of seeking the discrete solution of the indicator vector , the solution is relaxed to be continuous, and the eigen-system problem corresponding to the objective function is solved to construct the aligned spectral features [111]. 3. Learner T raining: A traditional classiﬁer is trained on the transformed instances. T o be more speciﬁc, the objective function has a form of the generalized Rayleigh quotient, which aims to ﬁnd the optimal graph partition that respects the label informatio n with small cut-size [112], to maximize the separation of the target-domain instances, and to ﬁt the constraints of the pair-wise property . After eigen-decomposition, the la st eigenvectors are selected and combined as a matrix, and then the matrix is normalized. Each row of the normalized matrix represents a transformed instance. 5 M ODEL -BASED INTERPRETATIO N T ransfer learning approaches can also be interpreted from the model perspective. Fig. 4 shows the corresponding strategies and the objectives. The main objective of a trans fer learning model is to make accurate prediction results on the target domain, e.g., classiﬁcation or clustering results. Note that a transfer learning model may consist of a few sub- modules such as classiﬁers, extractors, or encoders. These sub-modules may play different roles, e.g., feature adapta - tion or pseudo label generation. In this section, some relat ed 14 Model-Based Interpretation Objective Domain Adaptation ... Strategy Model Ensemble Model Selection Parameter Sharing Prediction Making Parameter Control Deep Learning Technique Parameter Restriction Voting Strategy Weighting Strategy ... ... Traditional Deep Learning Adversarial Deep Learning ... Model Control Consensus Regularizer Domain-Dependent Regularizer ... Pseudo Label Generation Fig. 4. Strategies and objectives of the transfer learning a pproaches from the model perspective. transfer learning approaches are introduced in proper orde r according to the strategies shown in Fig. 4. 5.1 Model Control Strategy From the perspective of model, a natural thought is to directly add the model-level regularizers to the learner s objective function. In this way , the knowledge contained in the pre-obtained source models can be transferred into the target model during the training process. For example, the paper by Duan et al. proposes a general framework termed Domain Adaptation Machine (DAM) [113], [114], which is designed for multi-source transfer learning. The goal of DAM is to construct a robust classiﬁer for the target domain with the help of some pre-obtained base classiﬁers that are respectively trained on multiple source domains. The objective function is given by: min fT LT,L (fT )  λ1Ω D (fT )  λ2Ω( fT ), where the ﬁrst term represents the loss function used to min- imize the classiﬁcation error of the labeled target-domain in- stances, the second term denotes different regularizers, a nd the third term is used to control the complexity of the ﬁnal decision function fT . Different types of the loss functions can be adopted in LT,L (fT ) such as the square error or the cross-entropy loss. Some transfer learning approaches can be regarded as the special cases of this framework to some extent.  (Consensus Regularizer) The work by Luo et al. proposes a framework termed Consensus Regularization Frame- work (CRF) [115], [116]. CRF is designed for multi-source transfer learning with no labeled target-domain instances . The framework constructs mS classiﬁers corresponding to each source domain, and these classiﬁers are required to reach mutual consensuses on the target domain. The objective function of each source classiﬁer , denoted by fS k (with k  1 ,  , m S), is similar to that of DAM, which is presented below: min fS k  nSk  i1 log P (ySk i xSk i ; fS k )  λ2Ω( fS k ) λ1 nT,U  i1  yj Y S ( 1 mS mS  k01 P (yj xT,U i ; fS k0 ) ) , where fS k denotes the decision function corresponding to the k-th source domain, and S(x)  x log x. The ﬁrst term is used to quantify the classiﬁcation error of the k-th classiﬁer on the k-th source domain, and the last term is the consensus regularizer in the form of cross- entropy . The consensus regularizer can not only enhance the agreement of all the classiﬁers, but also reduce the uncertainty of the predictions on the target domain. The authors implement this framework based on the logistic regression. A difference between DAM and CRF is that DAM explicitly constructs the target classiﬁer , while CRF makes the target predictions based on the reached consen- sus from the source classiﬁers.  (Domain-dependent Regularizer) Fast-DAM is a speciﬁc algorithm of DAM [113]. In light of the manifold assump- tion [30] and the graph-based regularizer [117], [118], Fast-DAM designs a domain-dependent regularizer . The objective function is given by: min fT nT,L  j1 ( fT (xT,L j ) yT,L j ) 2  λ2Ω( fT ) λ1 mS  k1 βk nT,U  i1 ( fT (xT,U i ) fS k (xT,U i ) ) 2 , where fS k (k  1 , 2,  , m S) denotes the pre-obtained source decision function for the k-th source domain and βk represents the weighting parameter that is determined by the relevance between the target domain and the k- th source domain and can be measured based on the MMD metric. The third term is the domain-dependent regularizer , which transfers the knowledge contained in 15 the source classiﬁer motivated by domain dependence. In [113], the authors also introduce and add a new term to the above objective function based on ε-insensitive loss function [119], which makes the resultant model have high computational efﬁciency .  (Domain-dependent Regularizer  Universum Regular- izer) Univer-DAM is an extension of the Fast-DAM [114]. Its objective function contains an additional regularizer , i.e., Universum regularizer . This regularizer usually uti - lizes an additional dataset termed Universum where the instances do not belong to either the positive or the negative class [120]. The authors treat the source-domain instances as the Universum for the target domain, and the objective function of Univer-DAM is presented as follows: min fT nT,L  j1 ( fT (xT,L j ) yT,L j ) 2  λ2 nS  j1 ( fT (xS j ) ) 2 λ1 mS  k1 βk nT,U  i1 ( fT (xT,U i ) fS k (xT,U i ) ) 2  λ3Ω( fT ). Similar to Fast-DAM, the ε-insensitive loss function can also be utilized [114]. 5.2 Parameter Control Strategy The parameter control strategy focuses on the parameters of models. For example, in the application of object categoriz a- tion, the knowledge from known source categories can be transferred into target categories via object attributes s uch as shape and color [121]. The attribute priors, i.e., probabil istic distribution parameters of the image features correspondi ng to each attribute, can be learned from the source domain and then used to facilitate learning the target classiﬁer . The parameters of a model actually reﬂect the knowledge learned by the model. Therefore, it is possible to transfer t he knowledge at the parametric level. 5.2.1 Parameter Sharing An intuitive way of controlling the parameters is to directl y share the parameters of the source learner to the target learner . Parameter sharing is widely employed especially in the network-based approaches. For example, if we have a neural network for the source task, we can freeze (or say , share) most of its layers and only ﬁnetune the last few layers to produce a target network. The network-based approaches are introduced in Section 5.4. In addition to network-based parameter sharing, matrix- factorization-based parameter sharing is also workable. F or example, Zhuang et al. proposed an approach for text clas- siﬁcation, which is referred to as Matrix T ri-Factorizatio n Based Classiﬁcation Framework (MT rick) [122]. The au- thors observe that, in different domains, different words o r phrases sometimes express the same or similar connotative meaning. Thus, it is more effective to use the concepts be- hind the words rather than the words themselves as a bridge to transfer the knowledge in source domains. Different from PLSA-based transfer learning approaches that utilize the concepts by constructing Bayesian networks, MT rick attempts to ﬁnd the connections between the document classes and the concepts conveyed by the word clusters through matrix tri-factorization. These connections are c on- sidered to be the stable knowledge that is supposed to be transferred. The main idea is to decompose a document-to- word matrix into three matrices, i.e., document-to-cluste r , connection, and cluster-to-word matrices. Speciﬁcally , b y performing the matrix tri-factorization operations on the source and the target document-to-word matrices respec- tively , a joint optimization problem is constructed, which is given by min Q,R,W XS QSRW S2  λ1XT QT RW T 2 λ2QS QS2 s.t. Normalization Constraints , where X denotes the document-to-word matrix, Q denotes the document-to-cluster matrix, R represents the transfor- mation matrix from document clusters to word clusters, W denotes the cluster-to-word matrix, nd denotes the number of the documents, and QS represents the label matrix. The matrix QS is constructed based on the class information of the source-domain documents. If the i-th document belongs to the k-th class, QS [i,k ]  1 . In the above objective function, the matrix R is actually the shared parameter . The ﬁrst term aims to tri-factorize the source document-to-word matrix, and the second term decomposes the target document-to- word matrix. The last term incorporates the source-domain label information. The optimization problem is solved base d on the alternating iteration method. Once the solution of QT is obtained, the class index of the k-th target-domain instance is the one with the maximum value in the k-th row of QT . Further , Zhuang et al. extended MT rick and proposed an approach termed T riplex T ransfer Learning (T riTL) [123] . MT rick assumes that the domains share the similar con- cepts behind their word clusters. In contrast, T riTL as- sumes that the concepts of these domains can be further divided into three types, i.e., domain-independent, trans fer- able domain-speciﬁc, and nontransferable domain-speciﬁc concepts, which is similar to HIDC. This idea is motivated by Dual T ransfer Learning (DTL), where the concepts are assumed to be composed of the domain-independent ones and the transferable domain-speciﬁc ones [124]. The objec- tive function of T riTL is provided as follows: min Q,R,W mS mT  k1 Xk Qk [ RDI RTD RND k ]   W DI W TD k W ND k  2 s.t. Normalization Constraints , where the deﬁnitions of the symbols are similar to those of MT rick and the subscript k denotes the index of the domains with the assumption that the ﬁrst mS domains are the source domains and the last mT domains are the target do- mains. The authors proposed an iterative algorithm to solve the optimization problem. And in the initialization phase, W DI and W TD k are initialized based on the clustering results of the PLSA algorithm, while W UT k is randomly initialized; the PLSA algorithm is performed on the combination of the instances from all the domains. There are some other approaches developed based on matrix factorization. W ang et al. proposed a transfer learn- ing framework for image classiﬁcation [125]. W ang et al. 16 proposed a softly associative approach that integrates two matrix tri-factorizations into a joint framework [126]. Do et al. utilized matrix tri-factorization to discover both the implicit and the explicit similarities for cross-domain re c- ommendation [127]. 5.2.2 Parameter Restriction Another parameter-control-type strategy is to restrict th e parameters. Different from the parameter sharing strategy that enforces the models share some parameters, parame- ter restriction strategy only requires the parameters of th e source and the target models to be similar . T ake the approaches to category learning as examples. The category-learning problem is to learn a new decision function for predicting a new category (denoted by the (k  1) -th category) with only limited target-domain in- stances and k pre-obtained binary decision functions. The function of these pre-obtained decision functions is to pre - dict which of the k categories an instance belongs to. In order to solve the category-learning problem, T ommasi et al. proposed an approach termed Single-Model Knowledge T ransfer (SMKL) [128]. SMKL is based on Least-Squares SVM (LS-SVM). The advantage of LS-SVM is that LS-SVM transforms inequality constraints to equality constraint s and has high computational efﬁciency; its optimization is equi v- alent to solving a linear equation system problem instead of a quadratic programming problem. SMKL selects one of the pre-obtained binary decision functions, and transfers the knowledge contained in its parameters. The objective function is given by min f 1 2      θ β θ       2  λ 2 nT,L  j1 ηj ( f(xT,L j ) yT,L j ) 2 , where f(x)  θ Φ( x)  b, β is the weighting parameter controlling the transfer degree, θ is the parameter of a selected pre-obtained model, and ηj is the coefﬁcient for resolving the label imbalance problem. The kernel param- eter and the tradeoff parameter are chosen based on cross- validation. In order to ﬁnd the optimal weighting parameter , the authors refer to an earlier work [129]. In [129], Cawley proposed a model selection mechanism for LS-SVM, which is based on the leave-one-out cross-validation method. The superiority of this method is that the leave-one-out error for each instance can be obtained in a closed form without performing the real cross-validation experiment. Motivat ed by Cawleys work, the generalization error can be easily estimated to guide the parameter setting in SMKL. T ommasi et al. further extended SMKL by utilizing all the pre-obtained decision functions. In [130], an approach tha t is referred to as Multi-Model Knowledge T ransfer (MMKL) is proposed. Its objective function is presented as follows : min f 1 2          θ  k i1 βiθi           2  λ 2 nT,L  j1 ηj ( f(xT,L j ) yT,L j ) 2 , where θi and βi are the model parameter and the weighting parameter of the i-th pre-obtained decision function, respec- tively . The leave-one-out error can also be obtained in a closed form, and the optimal value of βi (i  1 , 2,  , k ) is the one that maximizes the generalization performance. 5.3 Model Ensemble Strategy In sentiment analysis applications related to product re- views, data or models from multiple product domains are available and can be used as the source domains [131]. Com- bining data or models directly into a single domain may not be successful because the distributions of these domain s are different from each other . Model ensemble is another commonly used strategy . This strategy aims to combine a number of weak classiﬁers to make the ﬁnal predictions. Some previously mentioned transfer learning approaches already adopted this strategy . For example, T rAdaBoost and MsT rAdaBoost ensemble the weak classiﬁers via voting and weighting, respectively . In this subsection, several typi cal ensemble-based transfer learning approaches are introduc ed to help readers better understand the function and the appliance of this strategy . As mentioned in Section 4.1, T askT rAdaBoost, which is an extension of T rAdaBoost for handling multi-source scenarios, is proposed in the paper [33]. T askT rAdaBoost mainly has the following two stages. 1. Candidate Classiﬁer Construction : In the ﬁrst stage, a group of candidate classiﬁers are constructed by per- forming AdaBoost on each source domain. Note that, for each source domain, each iteration of AdaBoost re- sults in a new weak classiﬁer . In order to avoid the over- ﬁtting problem, the authors introduced a threshold to pick the suitable classiﬁers into the candidate group. 2. Classiﬁer Selection and Ensemble : In the second stage, a revised version of AdaBoost is performed on the target- domain instances to construct the ﬁnal classiﬁer . In each iteration, an optimal candidate classiﬁer which has the lowest classiﬁcation error on the labeled target-domain instances is picked out and assigned with a weight based on the classiﬁcation error . Then, the weight of each target-domain instance is updated based on the performance of the selected classiﬁer on the target do- main. After the iteration process, the selected classiﬁers are ensembled to produce the ﬁnal predictions. The difference between the original AdaBoost and the sec- ond stage of T askT rAdaBoost is that, in each iteration, the former constructs a new candidate classiﬁer on the weighted target-domain instances, while the latter selects one pre- obtained candidate classiﬁer which has the minimal clas- siﬁcation error on the weighted target-domain instances. The paper by Gao et al. proposes another ensemble- based framework that is referred to as Locally W eighted En- semble (L WE) [132]. L WE focuses on the ensemble process of various learners; these learners could be constructed on different source domains, or be built by performing differe nt learning algorithms on a single source domain. Different from T askT rAdaBoost that learns the global weight of each learner , the authors adopted the local-weight strategy , i. e., assigning adaptive weights to the learners based on the loca l manifold structure of the target-domain test set. In L WE, a learner is usually assigned with different weights when classifying different target-domain instances. Speciﬁca lly , the authors adopt a graph-based approach to estimate the weights. The steps for weighting are outlined below . 1. Graph Construction : For the i-th source learner , a graph GT Si is constructed by using the learner to classify the 17 target-domain instances in the test set; if two instances are classiﬁed into the same class, they are connected in the graph. Another graph GT is constructed for the target-domain instances as well by performing a clustering algorithm. 2. Learner Weighting : The weight of the i-th learner for the j-th target-domain instance xT j is proportional to the similarity between the instances local structures in GT Si and GT . And the similarity can be measured by the percentage of the common neighbors of xT j in these two graphs. Note that this weighting scheme is based on the clustering- manifold assumption, i.e., if two instances are close to eac h other in a high-density region, they often have similar labels. In order to check the validity of this assumption for the task, the target task is tested on the source-domain training set(s). Speciﬁcally , the clustering quality of th e training set(s) is quantiﬁed and checked by using a metric such as purity or entropy . If the clustering quality is not satisfactory , uniform weights are assigned to the learners instead. Besides, it is intuitive that if the measured struc ture similarity is particularly low for every learner , weightin g and combining these learners seems unwise. Therefore, the authors introduce a threshold and compare it to the average similarity . If the similarity is lower than the threshold, t he label of xT j is determined by the voting scheme among its reliable neighbors, where the reliable neighbors are the on es whose label predictions are made by the combined classiﬁer . The above-mentioned T askT rAdaBoost and L WE ap- proaches mainly focus on the ensemble process. In con- trast, some studies focus more on the construction of weak learners. For example, Ensemble Framework of Anchor Adapters (ENCHOR) [133] is a weighting ensemble frame- work proposed by Zhuang et al . An anchor is a speciﬁc instance. Different from T rAdaBoost which adjusts weights of instances to train and produce a new learner iteratively , ENCHOR constructs a group of weak learners via using dif- ferent representations of the instances produced by anchor s. The thought is that the higher similarity between a certain instance and an anchor , the more likely the feature of that in - stance remains unchanged relative to the anchor , where the similarity can be measured by using the cosine or Gaussian distance function. ENCHOR contains the following steps. 1. Anchor Selection : In this step, a group of anchors are selected. These anchors can be selected based on some rules or even randomly . In order to improve the ﬁ- nal performance of ENCHOR, the authors proposed a method of selecting high-quality anchors [133]. 2. Anchor-based Representation Generation : For each anchor and each instance, the feature vector of an instance is directly multiplied by a coefﬁcient that measures the distance from the instance to the anchor . In this way , each anchor produces a new pair of anchor-adapted source and target instance sets. 3. Learner T raining and Ensemble : The obtained pairs of instance sets can be respectively used to train learners. Then, the resultant learners are weighted and combined to make the ﬁnal predictions. The framework ENCHOR is easy to be realized in a parallel manner in that the operations performed on each anchor are independent. 5.4 Deep Learning T echnique Deep learning methods are particularly popular in the ﬁeld of machine learning. Many researchers utilize the deep learning techniques to construct transfer learning models . For example, the SDA and the mSLDA approaches men- tioned in Section 4.2.6 utilize the deep learning technique s. In this subsection, we speciﬁcally discuss the deep-learni ng- related transfer learning models. The deep learning ap- proaches introduced are divided into two types, i.e., non- adversarial (or say , traditional) ones and adversarial one s. 5.4.1 T raditional Deep Learning As said earlier , autoencoders are often used in deep learnin g area. In addition to SDA and mSLDA, there are some other reconstruction-based transfer learning approaches. For e x- ample, the paper by Zhuang et al. proposes an approach termed T ransfer Learning with Deep Autoencoders (TLDA) [44], [134]. TLDA adopts two autoencoders for the source and the target domains, respectively . These two autoen- coders share the same parameters. The encoder and the decoder both have two layers with activation functions. The diagram of the two autoencoders is presented as follows: XS (W1,b 1) QS (W2,b 2)  Softmax Regression RS ( ˆW2, ˆb2) QS ( ˆW1, ˆb1) XS,  KL Divergence  XT (W1,b 1) QT (W2,b 2)  Softmax Regression RT ( ˆW2, ˆb2) QT ( ˆW1, ˆb1) XT . There are several objectives of TLDA, which are listed as follows. 1. Reconstruction Error Minimization : The output of the de- coder should be extremely close to the input of encoder . In other words, the distance between XS and XS as well as the distance between XT and XT should be minimized. 2. Distribution Adaptation : The distribution difference be- tween QS and QT should be minimized. 3. Regression Error Minimization : The output of the encoder on the labeled source-domain instances, i.e., RS, should be consistent with the corresponding label information Y S. Therefore, the objective function of TLDA is given by min Θ LREC (X, X)  λ1KL(QSQT )  λ2Ω( W, b, ˆW , ˆb) λ3LREG (RS, Y S), where the ﬁrst term represents the reconstruction error , KL() represents the KL divergence, the third term controls the complexity , and the last term represents the regression error . TLDA is trained by using a gradient descent method. The ﬁnal predictions can be made in two different ways. The ﬁrst way is to directly use the output of the encoder to make predictions. And the second way is to treat the autoencoder as a feature extractor , and then train the target classiﬁer on the labeled instances with the feature representation produced by the encoder s ﬁrst-layer output. 18 In addition to the reconstruction-based domain adapta- tion, discrepancy-based domain adaptation is also a popula r direction. In earlier research, the shallow neural network s are tried to learn the domain-independent feature repre- sentation [135]. It is found that the shallow architectures often make it difﬁcult for the resultant models to achieve excellent performance. Therefore, many studies turn to uti - lize deep neural networks. Tzeng et al. [136] added a single adaptation layer and a discrepancy loss to the deep neural network, which improves the performance. Further , Long et al. performed multi-layer adaptation and utilized multi- kernel technique, and they proposed an architecture termed Deep Adaptation Networks (DAN) [137]. For better understanding, let us review DAN in detail. DAN is based on AlexNet [138] and its architecture is presented below [137]. full  6th RS 6 full  7th RS 7 full  8th RS 8 ( f(XS) ) XS XT conv  1st QS 1 QT 1 conv   QS 5 QT 5   ր ց  MK-MMD   MK-MMD   MK-MMD  Five Convolutional Layers full  6th RT 6 full  7th RT 7 full  8th RT 8 ( f(XT ) )    Three Fully Connected Layers In the above network, the features are ﬁrst extracted by ﬁve convolutional layers in a general-to-speciﬁc manner . Next , the extracted features are fed into one of the two fully connected networks switched by their original domains. These two networks both consist of three fully connected layers that are specialized for the source and the target domains. DAN has the following objectives. 1. Classiﬁcation Error Minimization : The classiﬁcation error of the labeled instances should be minimized. The cross-entropy loss function is adopted to measure the prediction error of the labeled instances. 2. Distribution Adaptation : Multiple layers, which include the representation layers and the output layer , can be jointly adapted in a layer-wise manner . Instead of using the single-kernel MMD to measure the distribution difference, the authors turn to MK-MMD. The authors adopt the linear-time unbiased estimation of MK-MMD to avoid numerous inner product operations [62]. 3. Kernel Parameter Optimization : The weighting parame- ters of the multiple kernels in MK-MMD should be optimized to maximize the test power [62]. The objective function of the DAN network is given by: min Θ max κ nL  i1 L ( f(xL i ), y L i )  λ 8 l6 MK-MMD(RS l , R T l ; κ), where l denotes the index of the layer . The above opti- mization is actually a minimax optimization problem. The maximization of the objective function with respect to the kernel function κ aims to maximize the test power . After this step, the subtle difference between the source and the targe t domains are magniﬁed. This train of thought is similar to the Generative Adversarial Network (GAN) [139]. In the training process, the DAN network is initialized by a pre- trained AlexNet [138]. There are two categories of param- eters that should be learned, i.e., the network parameters and the weighting parameters of the multiple kernels. Given that the ﬁrst three convolutional layers output the general features and are transferable, the authors freeze them and ﬁne-turn the last two convolutional layers and the two fully connected layers [140]. The last fully connected layer (or s ay , the classiﬁer layer) is trained from scratch. Long et al. further extended the above DAN approach and proposed the DAN framework [141]. The new charac- teristics are summarized as follows. 1. Regularizer Adding : The framework introduces an ad- ditional regularizer to minimize the uncertainty of the predicted labels of the unlabeled target-domain in- stances, which is motivated by entropy minimization criterion [142]. 2. Architecture Generalizing : The DAN framework can be applied to many other architectures such as GoogLeNet [143] and ResNet [144]. 3. Measurement Generalizing : The distribution difference can be estimated by other metrics. For example, in addition to MK-MMD, the authors also present the Mean Embedding test for distribution adaptation [145]. The objective function of the DAN framework is given by: min Θ max κ nL  i1 L ( f(xL i ), y L i )  λ1 lend llstrt DIST(RS l , R T l )  λ2 nT,U  i1  yj Y S ( P (yj f(xT,U i )) ) , where lstrt and lend denote the boundary indexes of the fully connected layers for adapting the distributions. There are some other impressive works. For example, Long et al. constructed residual transfer networks for do- main adaptation, which is motivated by deep residual learn- ing [146]. Besides, another work by Long et al. proposes the Joint Adaptation Network (JAN) [147], which adapts the joint distribution difference of multiple layers. Sun a nd Saenko extended CORAL for deep domain adaptation and proposed an approach termed Deep CORAL (DCORAL), in which the CORAL loss is added to minimize the feature covariance [148]. Chen et al. realized that the instances with the same label should be close to each other in the feature space, and they not only add the CORAL loss but also add an instance-based class-level discrepancy loss [149]. Pan et al. constructed three prototypical networks (corresponding to DS, DT and DS DT ) and incorporated the thought of multi-model consensus. They also adopt pseudo-label strategy and adapt both the instance-level and class-level discrepancy [150]. Kang et al. proposed the Contrastive Adaptation Network (CAN), which is based on the dis- crepancy metric termed contrastive domain discrepancy [151]. Zhu et al. aimed to adapt the extracted multiple fea- ture representations and proposed the Multi-Representati on Adaptation Network (MRAN) [152]. Deep learning technique can also be used for multi- source transfer learning. For example, the work by Zhu et al. proposes a framework that is referred to as Multiple Feature Spaces Adaptation Network (MFSAN) [153]. The architec- ture of MFSAN consists of a common-feature extractor , mS domain-speciﬁc feature extractors, and mS domain-speciﬁc 19 classiﬁers. The corresponding schematic diagram is shown below . XS 1  XS k  XS mS XT Common  Extractor QS 1 QS k QS mS QT Domain-Speciﬁc   Extractors RS 1 RS k  RS mS RT 1 RT k  RT mS Domain-Speciﬁc  Classiﬁers ˆY S 1  ˆY S k  ˆY S mS ˆY T 1  ˆY T k  ˆY T mS In each iteration, MFSAN has the following steps. 1. Common Feature Extraction : For each source domain (denoted by DSk with k  1 ,  , m S), the source- domain instances (denoted by XS k ) are separately input to the common-feature extractor to produce instances in a common latent feature space (denoted by QS k ). Similar operations are also performed on the target-domain instances (denoted by XT ), which produces QT . 2. Speciﬁc Feature Extraction : For each source domain, the extracted common features QS k is fed to the k- th domain-speciﬁc feature extractor . Meanwhile, QT is fed to all the domain-speciﬁc feature extractors, which results in RT k with k  1 ,  , m S. 3. Data Classiﬁcation : The output of the k-th domain- speciﬁc feature extractor is input to the k-th classiﬁer . In this way , mS pairs of the classiﬁcation results are predicted in the form of probability . 4. Parameter Updating : The parameters of the network are updated to optimize the objective function. There are three objectives in MFSAN, i.e., classiﬁcation error minimization, distribution adaptation, and consens us regularization. The objective function is given by: min Θ mS  i1 L( ˆY S i , Y S i )  λ1 mS  i1 MMD(RS i , R T i )  λ2 mS  ij   ˆY T i ˆY T j   , where the ﬁrst term represents the classiﬁcation error of th e labeled source-domain instances, the second term measures the distribution difference, and the third term measures the discrepancy of the predictions on the target-domain instances. 5.4.2 Adversarial Deep Learning The thought of adversarial learning can be integrated into deep-learning-based transfer learning approaches. As men - tioned above, in the DAN framework, the network Θ and the kernel κ play a minimax game, which reﬂects the thought of adversarial learning. However , the DAN frame- work is a little different from the traditional GAN-based methods in terms of the adversarial matching. In the DAN framework, there is only a few parameters to be optimized in the max game, which makes the optimization easier to achieve equilibrium. Before introducing the adversaria l transfer learning approaches, let us brieﬂy review the orig i- nal GAN framework and the related work. The original GAN [139], which is inspired by the two- player game, is composed of two models, a generator G and a discriminator D . The generator produces the counterfeits of the true data for the purpose of confusing the discrimina- tor and making the discriminator produce wrong detection. The discriminator is fed with the mixture of the true data and the counterfeits, and it aims to detect whether a data is the true one or the fake one. These two models actually play a two-player minimax game, and the objective function is as follows: min G max D ExPtrue [log D (x)]  EzPz [log (1 D (G (z)))], where z represents the noise instances (sampled from a certain noise distribution) used as the input of the generat or for producing the counterfeits. The entire GAN can be trained by using the back-propagation algorithm. When the two-player game achieves equilibrium, the generator can produce almost true-looking instances. Motivated by GAN, many transfer learning approaches are established based on the assumption that a good feature representation contains almost no discriminative informa - tion about the instances original domains. For example, th e work by Ganin et al. proposes a deep architecture termed Domain-Adversarial Neural Network (DANN) for domain adaptation [154], [155]. DANN assumes that there is no labeled target-domain instance to work with. Its architec- ture consists of a feature extractor , a label predictor , and a domain classiﬁer . The corresponding diagram is as follows. Label  Predictor ˆY S,L ˆY T,U XS,L XT,U Feature  Extractor  QS,L QT,U } Domain  Classiﬁer ˆS ˆT (Domain Label ) The feature extractor acts like the generator , which aims to produce the domain-independent feature representation fo r confusing the domain classiﬁer . The domain classiﬁer plays the role like the discriminator , which attempts to detect whether the extracted features come from the source domain or the target domain. Besides, the label predictor produces the label prediction of the instances, which is trained on th e extracted features of the labeled source-domain instances , i.e., QS,L . DANN can be trained by inserting a special gra- dient reversal layer (GRL). After the training of the whole system, the feature extractor learns the deep feature of the instances, and the output ˆY T,U is the predicted labels of the unlabeled target-domain instances. There are some other related impressive works. The work by Tzeng et al. proposes a uniﬁed adversarial domain adaptation framework [156]. The work by Shen et al. adopts W asserstein distance for domain adaptation [59]. Hoffman et al. adopted cycle-consistency loss to ensure the structural and semantic consistency [157]. Long et al. proposed the Conditional Domain Adversarial Network (CDAN), which utilizes a conditional domain discriminator to assist adve r- sarial adaptation [158]. Zhang et al. adopted a symmetric design for the source and the target classiﬁers [159]. Zhao et al. utilized domain adversarial networks to solve the multi- source transfer learning problem [160]. Y u et al. proposed a dynamic adversarial adaptation network [161]. Some approaches are designed for some special scenar- ios. T ake the partial transfer learning as an example. The partial transfer learning approaches are designed for the s ce- nario that the target-domain classes are less than the sourc e- domain classes, i.e., YS  YT . In this case, the source- domain instances with different labels may have different 20 importance for domain adaptation. T o be more speciﬁc, the source-domain and the target-domain instances with the same label are more likely to be potentially associated. How - ever , since the target-domain instances are unlabeled, how to identify and partially transfer the important informati on from the labeled source-domain instances is a critical issu e. The paper by Zhang et al. proposes an approach for partial domain adaptation, which is called Impor- tance W eighted Adversarial Nets-Based Domain Adaptation (IW ANDA) [162]. The architecture of IW ANDA is different from that of DANN. DANN adopts one common feature extractor based on the assumption that there exists a com- mon feature space where QS,L and QT,U have the similar distribution. However , IW ANDA uses two domain-speciﬁc feature extractors for the source and the target domains, respectively . Speciﬁcally , IW ANDA consists of two feature extractors, two domain classiﬁers, and one label predictor . The diagram of IW ANDA is presented below . Label  Predictor ˆY S,L ˆY T,U XS,L Source Feature  Extractor  QS,L XT,U T arget Feature  Extractor QT,U  } β S   ˆY T,U 2nd Domain  Classiﬁer ˆS2 ˆT2 1st Domain  Classiﬁer ˆS1 ˆT1 W eight    Function β S Before training, the source feature extractor and the label predictor are pre-trained on the labeled source-domain in- stances. These two components are frozen in the training process, which means that only the target feature extractor and the domain classiﬁers should be optimized. In each iteration, the above network is optimized by taking the following steps. 1. Instance Weighting : In order to solve the partial transfer issue, the source-domain instances are assigned with weights based on the output of the ﬁrst domain clas- siﬁer . The ﬁrst domain classiﬁer is fed with QS,L and QT,U , and then outputs the probabilistic predictions of their domains. If a source domain instance is predicted with a high probability of belonging to the target do- main, this instance is highly likely to associate with the target domain. Thus, this instance is assigned with a larger weight and vice versa. 2. Prediction Making : The label predictor outputs the label predictions of the instances. The second classiﬁer pre- dicts which domain an instance belongs to. 3. Parameter Updating : The ﬁrst classiﬁer is optimized to minimize the domain classiﬁcation error . The second classiﬁer plays a minmax game with the target fea- ture extractor . This classiﬁer aims to detect whether a instance is the instance from the target domain or the weighted instance from the source domain, and to reduce the uncertainty of the label prediction ˆY T,U . The target feature extractor aims to confuse the second classiﬁer . These components can be optimized in a similar way to GAN or by inserting a GRL. In addition to IW ANDA, the work by Cao et al. con- structs the selective adversarial network for partial tran sfer learning [163]. There are some other studies related to transfer learning. For example, the work by W ang et al. proposes a minimax-based approach to select high-quality source-domain data [164]. Chen et al. investigated the trans- ferability and the discriminability in the adversarial dom ain adaptation, and proposed a spectral penalization approach to boost the existing adversarial transfer learning method s [165]. 6 A PPLICATIO N In previous sections, a number of representative trans- fer learning approaches are introduced, which have been applied to solving a variety of text-relatedimage-relate d problems in their original papers. For example, MT rick [122 ] and T riTL [123] utilize the matrix factorization technique to solve cross-domain text classiﬁcation problems; the deep- learning-based approaches such as DAN [137], DCORAL [148], and DANN [154], [155] are applied to solving image classiﬁcation problems. Instead of focusing on the general text-related or image-related applications, in this secti on, we mainly focus on the transfer learning applications in speci ﬁc areas such as medicine, bioinformatics, transportation, a nd recommender systems. 6.1 Medical Application Medical imaging plays an important role in the medical area, which is a powerful tool for diagnosis. With the de- velopment of computer technology such as machine learn- ing, computer-aided diagnosis has become a popular and promising direction. Note that medical images are gener- ated by special medical equipment, and their labeling often relies on experienced doctors. Therefore, in many cases, it is expensive and hard to collect sufﬁcient training data. T ran s- fer learning technology can be utilized for medical imaging analysis. A commonly used transfer learning approach is to pre-train a neural network on the source domain (e.g., ImageNet, which is an image database containing more than fourteen million annotated images with more than twenty thousand categories [166]) and then ﬁnetune it based on the instances from the target domain. For example, Maqsood et al. ﬁnetuned the AlexNet [138] for the detection of Alzheimer s disease [167]. Their ap- proach has the following four steps. First, the MRI images from the target domain are pre-processed by performing contrast stretching operations. Second, the AlexNet archi tec- ture [138] is pre-trained over ImageNet [166] (i.e., the sou rce domain) as a starting point to learn the new task. Third, the convolutional layers of AlexNet are ﬁxed, and the last three fully connected layers are replaced by the new ones including one softmax layer , one fully connected layer , and one output layer . Finally , the modiﬁed AlexNet is ﬁnetuned by training on the Alzheimer s dataset [168] (i.e., the targ et domain). The experimental results show that the proposed approach achieves the highest accuracy for the multi-class classiﬁcation problem (i.e, Alzheimer s stage detection) . Similarly , Shin et al. ﬁnetuned the pre-trained deep neural network for solving the computer-aided detection problems [169]. Byra et al. utilized the transfer learning tech- nology to help assess knee osteoarthritis [170]. In additio n to imaging analysis, transfer learning has some other applica - tions in the medical area. For example, the work by T ang et 21 al. combines the active learning and the domain adaptation technologies for the classiﬁcation of various medical data [171]. Zeng et al. utilized transfer learning for automatically encoding ICD-9 codes that are used to describe a patients diagnosis [172]. 6.2 Bioinformatics Application Biological sequence analysis is an important task in the bioinformatics area. Since the understanding of some or- ganisms can be transferred to other organisms, transfer learning can be applied to facilitate the biological sequen ce analysis. The distribution difference problem exists sign iﬁ- cantly in this application. For example, the function of som e biological substances may remain unchanged but with the composition changed between two organisms, which may result in the marginal distribution difference. Besides, i f two organisms have a common ancestor but with long evo- lutionary distance, the conditional distribution differe nce would be signiﬁcant. The work by Schweikert et al. uses the mRNA splice site prediction problem as the example to analyze the effectiveness of transfer learning approach es [173]. In their experiments, the source domain contains the sequence instances from a well-studied model organism, i.e ., C. elegans , and the target organisms include two additional nematodes (i.e., C. remanei and P . paciﬁcus ), D. melanogaster , and the plant A. thaliana . A number of transfer learning approaches, e.g., F AM [64] and the variant of KMM [5], are compared with each other . The experimental results show that transfer learning can help improve the classiﬁcation performance. Another widely encountered task in the bioinformatics area is gene expression analysis, e.g., predicting associa tions between genes and phenotypes. In this application, one of the main challenges is the data sparsity problem, since there is usually very little data of the known associations. T ransfer learning can be used to leverage this problem by providing additional information and knowledge. For example, Petegrosso et al. [174] proposed a transfer learn- ing approach to analyze and predict the gene-phenotype associations based on the Label Propagation Algorithm (LP A) [175]. LP A utilizes the Protein-Protein Interaction (PPI) network and the initial labeling to predict the target associations based on the assumption that the genes that are connected in the PPI network should have the similar labels. The authors extended LP A by incorporating multi-task and transfer-learning technologies. First, Human Phenotype O n- tology (HPO), which provides a standardized vocabulary of phenotypic features of human diseases, is utilized to form the auxiliary task. In this way , the associations can be predicted by utilizing phenotype paths and both the linkage knowledge in HPO and in the PPI network; the interacted genes in PPI are more likely to be associated with the same phenotype and the connected phenotypes in HPO are more likely to be associated with the same gene. Second, Gene Ontology (GO), which contains the association information between gene functions and genes, is used as the source domain. Additional regularizers are designed, and the PPI network and the common genes are used as the bridge for knowledge transfer . The gene-GO term and gene-HPO phenotype associations are constructed simultaneously fo r all the genes in the PPI network. By transferring additional knowledge, the predicted gene-phenotype associations can be more reliable. T ransfer learning can also be applied to solving the PPI prediction problems. Xu et al. [176] proposed an approach to transfer the linkage knowledge from the source PPI network to the target one. The proposed approach is based on the collective matrix factorization technique [177], wh ere a factor matrix is shared across domains. 6.3 T ransportation Application One application of transfer learning in the transportation area is to understand the trafﬁc scene images. In this applic a- tion, a challenge problem is that the images taken from a cer- tain location often suffer from variations because of diffe rent weather and light conditions. In order to solve this problem , Di et al. proposed an approach that attempts to transfer the information of the images that were taken from the same location in different conditions [178]. In the ﬁrst step, a p re- trained network is ﬁnetuned to extract the feature represen - tations of images. In the second step, the feature transfor- mation strategy is adopted to construct a new feature rep- resentation. Speciﬁcally , the dimension reduction algori thm (i.e., partial least squares regression [179]) is performe d on the extracted features to generate low-dimension features . Then, a transformation matrix is learned to minimize the domain discrepancy of the dimension-reduced data. Next, the subspace alignment operations are adopted to further reduce the domain discrepancy . Note that, although images under different conditions often have different appearanc es, they often have the similar layout structure. Therefore, in the ﬁnal step, the cross-domain dense correspondences are established between the test image and the retrieved best matching image at ﬁrst, and then the annotations of the best matching image are transferred to the test image via the Markov random ﬁeld model [180], [181]. T ransfer learning can also be applied to the task of driver behavior modeling. In this task, sufﬁcient personalized da ta of each individual driver are usually unavailable. In such situations, transferring the knowledge contained in the hi s- torical data for the newly-involved driver is a promising alternative. For example, Lu et al. proposed an approach to driver model adaptation in lane-changing scenarios [182]. The source domain contains the sufﬁcient data describing the behavior of the source drivers, while the target domain has a few numbers of data about the target driver . In the ﬁrst step, the data from both domains are pre-processed by performing PCA to generate low-dimension features. The authors assume that the source and the target data are from two manifolds. Therefore, in the second step, a manifold alignment approach is adopted for domain adap- tation. Speciﬁcally , the dynamic time warping algorithm [183] is applied to measuring similarity and ﬁnding the corresponding source-domain data point of each target- domain data point. Then, local Procrustes analysis [184] is adopted to align the two manifolds based on the obtained correspondences between data points. In this way , the data from the source domain can be transferred to the target domain. And in the ﬁnal step, a stochastic modeling method (e.g., Gaussian mixture regression [185]) is used to model 22 the behavior of the target driver . The experimental results demonstrate that the transfer learning approach can help the target driver even when few target-domain data are available. Besides, the results also show that when the number of target instances are very small or very large, the superiority of their approach is not obvious. This may because the relationship across domains cannot be found exactly with few target-domain instances, and in the case of sufﬁcient target-domain instances, the necessity of trans fer learning is reduced. Besides, there are some other applications of transfer learning in the transportation area. For example, Liu et al. applied transfer learning to driver poses recognition [186 ]. W ang et al. adopted the regularization technique in transfer learning for vehicle type recognition [187]. T ransfer lear ning can also be utilized for anomalous activity detection [188] , [189], trafﬁc sign recognition [190], etc. 6.4 Recommender-System Application Due to the rapid increase of the amount of information, how to effectively recommend the personalized content for individual users is an important issue. In the ﬁeld of recommender systems, some traditional recommendation methods, e.g., factorization-based collaborative ﬁlteri ng, of- ten rely on the factorization of the user-item interaction matrix to obtain the predictive function. These methods often require a large amount of training data to make accurate recommendations. However , the necessary trainin g data, e.g., the historical interaction data, are often spar se in real-world scenarios. Besides, for new registered users or new items, traditional methods are often hard to make effective recommendations, which is also known as the cold- start problem. Recognizing the above-mentioned problems in recom- mender systems, kinds of transfer learning approaches, e.g ., instance-based and feature-based approaches, have been proposed. These approaches attempt to make use of the data from other recommender systems (i.e., the source domains) to help construct the recommender system in the target domain. Instance-based approaches mainly focus on transferring different types of instances, e.g., rating s, feedbacks, and examinations, from the source domain to the target domain. The work by Pan et al. [191] leverages the uncertain ratings (represented as rating distribution s) of the source domain for knowledge transfer . Speciﬁcally , the source-domain uncertain ratings are used as constraints to help complete the rating matrix factorization task on the target domain. Hu et al. [192] proposed an approach termed transfer meeting hybrid, which extracts the knowledge from unstructured text by using an attentive memory network and selectively transfer the useful information. Feature-based approaches often leverage and transfer the information from a latent feature space. For example, Pan et al. proposed an approach termed Coordinate System T ransfer (CST) [193] to leverage both the user-side and the item-side latent features. The source-domain instances co me from another recommender system, sharing common users and items with the target domain. CST is developed based on the assumption that the principle coordinates, which reﬂect the tastes of users or the factors of items, character - ize the domain-independent structure and are transferable across domains. CST ﬁrst constructs two principle coordi- nate systems, which are actually the latent features of user s and items, by applying sparse matrix tri-factorization on the source-domain data, and then transfer the coordinate systems to the target domain by setting them as constraints. The experimental results show that CST signiﬁcantly out- performs the non-transfer baselines (i.e., average ﬁlling model and latent factorization model) in all data sparsity levels [193]. There are some other studies about cross-domain rec- ommendation [194], [195], [196], [197]. For example, He et al. proposed a transfer learning framework based on the Bayesian neural network [198]. Zhu et al. [199] proposed a deep framework, which ﬁrst generates the user and item feature representations based on the matrix factorization technique, and then employs a deep neural network to learn the mapping of features across domains. Y uan et al. [200] proposed a deep domain adaptation approach based on autoencoders and a modiﬁed DANN [154], [155] to extract and transfer the instances from rating matrices. 6.5 Other Applications Communication Application : In addition to WiFi localiza- tion tasks [2], [36], transfer learning has also been employ ed in wireless-network applications. For example, Bastug et al. proposed a caching mechanism [201]; the knowledge contained in contextual information, which is extracted fr om the interactions between devices, is transferred to the tar get domain. Besides, some studies focus on the energy saving problems. The work by Li et al. proposes an energy saving scheme for cellular radio access networks, which utilizes the transfer-learning expertise [202]. The work by Zhao and Grace applies transfer learning to topology management for reducing energy consumption [203]. Urban-Computing Application : With a large amount of data related to our cities, urban-computing is a promis- ing researching track in directions of trafﬁc monitoring, health care, social security , etc. T ransfer learning has be en applied to alleviate the data scarcity problem in many urban computing applications. For example, Guo et al. [204] proposed an approach for chain store site recommendation, which leverages the knowledge from semantically-relevant domains (e.g., other cities with the same store and other chain stores in the target city) to the target city . W ei et al. [205] proposed a ﬂexible multi-modal transfer learning approach that transfers knowledge from a city that have sufﬁcient multi-model data and labels to the target city to alleviate the data sparsity problem. T ransfer learning has been applied to some recognition tasks such as hand gesture recognition [206], face recogni- tion [207], activity recognition [208], and speech emotion recognition [209]. Besides, transfer-learning expertise has also been incorporated into some other areas such as sen- timent analysis [28], [96], [210], fraud detection [211], s ocial network [212], and hyperspectral image analysis [54], [213 ]. 7 E XPERIMENT T ransfer learning techniques have been successfully appli ed in many real-world applications. In this section, we perfor m 23 experiments to evaluate the performance of some represen- tative transfer learning models 1 [214] of different categories on two mainstream research areas, i.e., object recognition and text classiﬁcation. The datasets are introduced at ﬁrst . Then, the experimental results and further analyses are provided. 7.1 Dataset and Preprocessing Three datasets are studied in the experiments, i.e., Ofﬁce- 31, Reuters-21578, and Amazon Reviews. For simplicity , we focus on the classiﬁcation tasks. The statistical informat ion of the preprocessed datasets is listed in T able 3.  Amazon Reviews 2 [107] is a multi-domain sentiment dataset which contains product reviews taken from Ama- zon.com of four domains (Books, Kitchen, Electronics and DVDs). Each review in the four domains has a text and a rating from zero to ﬁve. In the experiments, the ratios that are less than three are deﬁned as the negative ones, while others are deﬁned as the positive ones. The frequency of each word in all reviews is calculated. Then, the ﬁve thousand words with the highest frequency are selected as the attributes of each review . In this way , we ﬁnally have 1000 positive instances, 1000 negative instances, and about 5000 unlabeled instances in each domain. In the experiments, every two of the four domains are selected to generate twelve tasks.  Reuters-215783 is a dataset for text categorization, which has a hierarchical structure. The dataset contains 5 top categories (Exchanges, Orgs, People, Places, T opics). In out experiment, we use the top three big category Orgs, People and Places to generate three classiﬁcation tasks (Orgs vs People, Orgs vs Places and People vs Places). In each task, the subcategories in the corresponding two categories are separately divided into two parts. Then, the resultant four parts are used as the components to form two domains. Each domain has about 1000 instances, and each instance has about 4500 features. Speciﬁcally , taking the task Orgs vs People as an example, one part from Orgs and one part from People and combined to form the source domain; similarly , the rest two parts form the target domain. Note that the instances in the three categories are all labeled. In order to generate the unlabeled instances, the labeled instances are selected from the dataset, and their labels are ignored.  Ofﬁce-31 [215] is an object recognition dataset which contains thirty-one categories and three domains, i.e., Amazon, W ebcam, and DSLR. These three domains have 2817, 498, and 795 instances, respectively . The images in Amazon are the online e-commerce pictures taken from Amazon.com. The images in W ebcam are the low- resolution pictures taken by web cameras. And the im- ages in DSLR are the high-resolution pictures taken by DSLR cameras. In the experiments, every two of the three domains (with the order considered) are selected as the source and the target domains, which results in six tasks. 1. https:github.comFuzhenZhuangT ransfer-Learning-T oolkit 2. http:www .cs.jhu.edu mdredzedatasetssentiment 3. https:archive.ics.uci.edumldatasetsReuters- 21578T extCategorizationCollection Model .ĺ .ĺ .ĺ( ĺ. ĺ ĺ( ĺ. ĺ ĺ( (ĺ. (ĺ (ĺ HIDC 0.88 0.875 0.88 0.7925 0.81 0.8025 0.7925 0.8175 0.8075 0.8075 0.87 0.87 濃濁濋濆濆濋 TriTL 0.715 0.725 0.6775 0.5725 0.525 0.5775 0.615 0.6125 0. 6 0.625 0.61 0.615 濃濁濉濅濅濈 CD-PLSA 0.7475 0.7225 0.72 0.6075 0.6175 0.6075 0.575 0.61 0 .6425 0.7225 0.745 0.7 濃濁濉濉濋濄 MTrick 0.82 0.835 0.8125 0.7725 0.7475 0.7275 0.755 0.745 0.7 8 0.79 0.7975 0.81 濃濁濊濋濅濊 SFA 0.8525 0.8575 0.8675 0.7825 0.805 0.775 0.7925 0.785 0.7 775 0.84 0.8525 0.84 濃濁濋濄濌濃 mSLDA 0.7975 0.7825 0.7925 0.635 0.645 0.6325 0.6525 0.6675 0.6625 0.7225 0.715 0.7125 濃濁濊濃濄濈 SDA 0.8425 0.7925 0.8025 0.745 0.76 0.765 0.7625 0.7475 0.74 25 0.8175 0.805 0.81 濃濁濊濋濅濊 GFK 0.62 0.6275 0.6325 0.62 0.61 0.6225 0.58 0.565 0.5725 0.6 575 0.65 0.6325 濃濁濉濄濈濋 SCL 0.8575 0.8625 0.8725 0.78 0.785 0.7825 0.7925 0.7925 0.7 825 0.8425 0.8525 0.845 濃濁濋濅濃濉 TCA 0.755 0.755 0.755 0.6475 0.6475 0.65 0.58 0.5825 0.585 0. 7175 0.715 0.7125 濃濁濉濊濈濅 Baseline 0.727 0.709 0.827 0.74 0.728 0.73 0.745 0.772 0.708 0 .84 0.706 0.707 濃濁濊濇濇濌 .ĺ .ĺ .ĺ( ĺ. ĺ ĺ( ĺ. ĺ ĺ( (ĺ. (ĺ (ĺ HIDC TriTL CD-PLSA MTrick SFA mSLDA SDA GFK SCL TCA Baseline Fig. 5. Comparison results on Amazon Reviews. 7.2 Experiment Setting Experiments are conducted to compare some representative transfer learning models. Speciﬁcally , eight algorithms a re performed on the dataset Ofﬁce-31 for solving the object recognition problem. Besides, fourteen algorithms are per - formed and evaluated on the dataset Reuters-21578 for solving the text classiﬁcation problem. In the sentiment classiﬁcation problem, eleven algorithms are performed on Amazon Reviews. The classiﬁcation results are evaluated by accuracy , which is deﬁned as follows: accuracy  {xxi Dtest f(xi)  yi} Dtest  where Dtest denotes the test data and y denotes the truth classiﬁcation label; f(x) represents the predicted classiﬁca- tion result. Note that some algorithms need the base classi- ﬁer . In these cases, an SVM with a linear kernel is adopted as the base classiﬁer in the experiments. Besides, the sourc e- domain instances are all labeled. And for the performed al- gorithms (except T rAdaBoost), the target-domain instance s are unlabeled. Each algorithm was executed three times, and the average results are adopted as our experimental results . The evaluated transfer learning models include: HIDC [93], T riTL [123], CD-PLSA [91], [92], MT rick [122], SF A [106], mSLDA [98], [99], SDA [96], GFK [102], SCL [94], TCA [36], [78], CoCC [41], JDA [38], T rAdaBoost [31], DAN [137], DCORAL [148], MRAN [152], CDAN [158], DANN [154], [155], JAN [147], and CAN [151]. 7.3 Experiment Result In this subsection, we compare over twenty algorithms on three datasets in total. The parameters of all algorithms are set to the default values or the recommended values mentioned in the original papers. The experimental results are presented in T ables 4, 5, and 6 corresponding to Amazon Reviews, Reuters-21578, and Ofﬁce-31, respectively . In or der to allow readers to understand the experimental results more intuitively , three radar maps, i.e., Figs. 5, 6, and 7, a re provided, which visualize the experimental results. In the radar maps, each direction represents a task. The general performance of an algorithm is demonstrated by a polygon 24 T ABLE 3 Statistical information of the preprocessed datasets. Area Dataset Domain Attribute T otal Instances T asks Sentiment Classiﬁcation Amazon Reviews 4 5000 27677 12 T ext Classiﬁcation Reuters-21578 3 4772 6570 3 Object Recognition Ofﬁce-31 3 800 4110 6 Orgs vs Places People vs Places Orgs vs People HIDC 0.7698 0.6945 0.8375 GFK 0.622 0.5417 0.6446 CD-PLSA 0.5624 0.5749 0.7826 MTrick 0.7494 0.6457 0.793 CoCC 0.6704 0.8264 0.7644 SFA 0.7468 0.6768 0.7906 mSLDA 0.5645 0.6064 0.5289 Orgs vs Places People vs Places Orgs vs People HIDC GFK CD-PLSA MTrick CoCC SFA mSLDA Orgs vs Places People vs Places Orgs vs People SDA 0.6603 0.5556 0.5992 濃濁濉濃濈濃 TriTL 0.7338 0.5517 0.7505 濃濁濉濊濋濊 SCL 0.6794 0.5046 0.6694 濃濁濉濄濊濋 TCA 0.7368 0.6065 0.7562 濃濁濉濌濌濋 JDA 0.5694 0.6296 0.7424 濃濁濉濇濊濄 TrAdaBoost 0.7336 0.7052 0.7879 濃濁濊濇濅濅 Baseline 0.6683 0.5198 0.6696 HIDC 0.7698 0.6945 0.8375 濃濁濊濉濊濆 GFK 0.622 0.5417 0.6446 濃濁濉濃濅濋 CD-PLSA 0.5624 0.5749 0.7826 濃濁濉濇濃濃 MTrick 0.7494 0.6457 0.793 濃濁濊濅濌濇 CoCC 0.6704 0.8264 0.7644 濃濁濊濈濆濊 SFA 0.7468 0.6768 0.7906 濃濁濊濆濋濄 mSLDA 0.5645 0.6064 0.5289 濃濁濈濉濉濉 Orgs vs Places People vs Places Orgs vs People SDA TriTL SCL TCA JDA TrAdaBoost Baseline Fig. 6. Comparison results on Reuters-21578. T ABLE 4 Accuracy performance on the Amazon Reviews of four domains: Kitchen (K), Electronics (E), D VDs (D) and Books (B). Model K  D K  B K  E D  K D  B D  E B  K B  D B  E E  K E  D E  B A verage HIDC 0.8800 0.8750 0.8800 0.7925 0.8100 0.8025 0.7925 0.817 5 0.8075 0.8075 0.8700 0.8700 0.8338 T riTL 0.7150 0.7250 0.6775 0.5725 0.5250 0.5775 0.6150 0.61 25 0.6000 0.6250 0.6100 0.6150 0.6225 CD-PLSA 0.7475 0.7225 0.7200 0.6075 0.6175 0.6075 0.5750 0.6 100 0.6425 0.7225 0.7450 0.7000 0.6681 MT rick 0.8200 0.8350 0.8125 0.7725 0.7475 0.7275 0.7550 0.7 450 0.7800 0.7900 0.7975 0.8100 0.7827 SF A 0.8525 0.8575 0.8675 0.7825 0.8050 0.7750 0.7925 0.7850 0 .7775 0.8400 0.8525 0.8400 0.8190 mSLDA 0.7975 0.7825 0.7925 0.6350 0.6450 0.6325 0.6525 0.667 5 0.6625 0.7225 0.7150 0.7125 0.7015 SDA 0.8425 0.7925 0.8025 0.7450 0.7600 0.7650 0.7625 0.7475 0 .7425 0.8175 0.8050 0.8100 0.7827 GFK 0.6200 0.6275 0.6325 0.6200 0.6100 0.6225 0.5800 0.5650 0.5725 0.6575 0.6500 0.6325 0.6158 SCL 0.8575 0.8625 0.8725 0.7800 0.7850 0.7825 0.7925 0.7925 0 .7825 0.8425 0.8525 0.8450 0.8206 TCA 0.7550 0.7550 0.7550 0.6475 0.6475 0.6500 0.5800 0.5825 0.5850 0.7175 0.7150 0.7125 0.6752 Baseline 0.7270 0.7090 0.8270 0.7400 0.7280 0.7300 0.7450 0 .7720 0.7080 0.8400 0.7060 0.7070 0.7449 whose vertices representing the accuracy of the algorithm for dealing with different tasks. T able 4 shows the experimental results on Amazon Re- views. The baseline is a linear classiﬁer trained only on the source domain (here we directly use the results from the paper [107]). Fig. 5 visualizes the results. As shown in Fig. 5, most algorithms are relatively well-performed when the source domain is electronics or kitchen, which indicate s that these two domains may contains more transferable information than the other two domains. In addition, it can be observed that HIDC, SCL, SF A, MT rick and SDA perform well and relatively stable in all the twelve tasks. Meanwhil e, other algorithms, especially mSLDA, CD-PLSA, and T riTL, are relatively unstable; the performance of them ﬂuctuates in a range about twenty percent. T riTL has a relatively high accuracy on the tasks where the source domain is kitchen, but has a relatively low accuracy on other tasks. The algorithms TCA, mSLDA, and CD-PLSA have similar performance on all the tasks with an accuracy about seventy percent on average. Among the well-performed algorithms, HIDC and MT rick are based on feature reduction (feature clustering), while the others are based on feature encoding (SDA), feature alignment (SF A), and feature selection (SCL ). Those strategies are currently the mainstreams of feature- based transfer learning. T able 5 presents the comparison results on Reuter-21578 (here we directly use the results of the baseline and CoCC from papers [78] and [41]). The baseline is a regularized lea st square regression model trained only on the labeled target domain instances [78]. Fig. 6, which has the same structure of Fig. 5, visualizes the performance. For clarity , thirtee n algorithms are divided into two parts that correspond to the two subﬁgures in Fig. 6. It can be observed that most algorithms are relatively well-performed for Orgs vs Place s 25 Method ĺ: ĺ: :ĺ ĺ ĺ :ĺ DAN 0.826 0.977 0.831 0.668 0.666 0.828 DCORAL 0.79 0.98 0.827 0.653 0.645 0.816 MRAN 0.914 0.969 0.998 0.864 0.683 0.709 0.856 CDAN 0.931 0.982 0.898 0.701 0.68 0.865 DANN 0.826 0.978 0.833 0.668 0.661 0.828 JAN 0.854 0.974 0.998 0.847 0.686 0.7 0.843 CAN 0.945 0.991 0.998 0.95 0.78 0.77 0.906 Baseline 0.616 0.954 0.99 0.638 0.511 0.498 0.701166667 ĺ: ĺ: :ĺ ĺ ĺ :ĺ DAN DCORAL MRAN CDAN DANN JAN CAN Baseline Fig. 7. Comparison results on Ofﬁce-31. T ABLE 5 Accuracy performance on the Reuters-21578 of three domains : Orgs, People, and Places. Model Orgs vs Places People vs Places Orgs vs People A verage HIDC 0.7698 0.6945 0.8375 0.7673 T riTL 0.7338 0.5517 0.7505 0.6787 CD-PLSA 0.5624 0.5749 0.7826 0.6400 MT rick 0.7494 0.6457 0.7930 0.7294 CoCC 0.6704 0.8264 0.7644 0.7537 SF A 0.7468 0.6768 0.7906 0.7381 mSLDA 0.5645 0.6064 0.5289 0.5666 SDA 0.6603 0.5556 0.5992 0.6050 GFK 0.6220 0.5417 0.6446 0.6028 SCL 0.6794 0.5046 0.6694 0.6178 TCA 0.7368 0.6065 0.7562 0.6998 JDA 0.5694 0.6296 0.7424 0.6471 T rAdaBoost 0.7336 0.7052 0.7879 0.7422 Baseline 0.6683 0.5198 0.6696 0.6192 T ABLE 6 Accuracy performance on Ofﬁce-31 of three domains: Amazon ( A), Webcam (W), and DSLR (D). Model A  W D  W W  D A  D D  A W  A A verage DAN 0.826 0.977 1.00 0.831 0.668 0.666 0.828 DCORAL 0.790 0.980 1.00 0.827 0.653 0.645 0.816 MRAN 0.914 0.969 0.998 0.864 0.683 0.709 0.856 CDAN 0.931 0.982 1.00 0.898 0.701 0.680 0.865 DANN 0.826 0.978 1.00 0.833 0.668 0.661 0.828 JAN 0.854 0.974 0.998 0.847 0.686 0.700 0.843 CAN 0.945 0.991 0.998 0.950 0.780 0.770 0.906 Baseline 0.616 0.954 0.990 0.638 0.511 0.498 0.701 and Orgs vs People, but poor for People vs Places. This phe- nomenon indicates that the discrepancy between People and Places may be relatively large. T rAdaBoost has a relatively good performance in this experiment because it uses the labels of the instances in the target domain to reduce the impact of the distribution difference. Besides, the algori thms HIDC, SF A, and MT rick have relatively consistent perfor- mance in the three tasks. These algorithms are also well- performed in the previous experiment on Amazon Reviews. In addition, the top two well-performed algorithms in terms of People vs Places are CoCC and T rAdaBoost. In the third experiment, seven deep-learning-based transfer learning models (i.e., DAN, DCORAL, MRAN, CDAN, DANN, JAN, and CAN) and the baseline (i.e., the Alexnet [138], [140] pre-trained on ImageNet [166] and then directly trained on the target domain) are performed on the dataset Ofﬁce-31 (here we directly use the results of CDAN, JAN, CAN, and the baseline from the original papers [137], [147], [151], [158]). The ResNet-50 [144] is used as the back - bone network for all these three models. The experimental results are provided in T able 6 and the average performance is visualized in Fig. 7. As shown in Fig. 7, all of these seven algorithms have excellent performance, especially o n the tasks D W and W D, whose accuracy is very close to one hundred percent. This phenomenon reﬂects the superiority of the deep-learning based approaches, and is consistent with the fact that the difference between W ebcam and DSLR is smaller than that between W ebcamDSLR and Amazon. Clearly , CAN outperforms the other six al- gorithms. In all the six tasks, the performance of DANN is similar to that of DAN, and is better than that of DCORAL, which indicates the effectiveness and the practicability o f incorporating adversarial learning. It is worth mentioning that, in the above experiments, the performance of some algorithms is not ideal. One reason is that we use the default parameter settings provided in the algorithms original papers, which may not be suitable for the dataset we selected. For example, GFK was originally designed for object recognition, and we directly adopt it into text classiﬁcation in the ﬁrst experiment, which turns out to produce an unsatisfactory result (having about sixty - two percent accuracy on average). The above experimental results are just for reference. These results demonstrate t hat some algorithms may not be suitable for the datasets of certain domains. Therefore, it is important to choose the appropriate algorithms as the baselines in the process of re - search. Besides, in practical applications, it is also nece ssary to ﬁnd a suitable algorithm. 8 C ONCLUSION AND FUTURE DIRECTION In this survey paper , we have summarized the mechanisms and the strategies of transfer learning from the perspectiv es of data and model. The survey gives the clear deﬁnitions about transfer learning and manages to use a uniﬁed sym- bol system to describe a large number of representative transfer learning approaches and related works. W e have basically introduced the objectives and strategies in tran sfer learning based on data-based interpretation and model- based interpretation. Data-based interpretation introdu ces the objectives, the strategies, and some transfer learning 26 approaches from the data perspective. Similarly , model- based interpretation introduces the mechanisms and the strategies of transfer learning but from the model level. The applications of transfer learning have also been intro- duced. At last, experiments have been conducted to evaluate the performance of representative transfer learning model s on two mainstream area, i.e., object recognition and text categorization. The comparisons of the models have also been given, which reﬂects that the selection of the transfer learning model is an important research topic as well as a complex issue in practical applications. Several directions are available for future research in the transfer learning area. First, transfer learning techn iques can be further explored and applied to a wider range of applications. And new approaches are needed to solve the knowledge transfer problems in more complex scenarios. For example, in real-world scenarios, sometimes the user- relevant source-domain data comes from another company . In this case, how to transfer the knowledge contained in the source domain while protecting user privacy is an important issue. Second, how to measure the transferability across do - mains and avoid negative transfer is also an important issue . Although there have been some studies on negative transfer , negative transfer still needs further systematic analyses [3]. Third, the interpretability of transfer learning also need s to be investigated further [216]. Finally , theoretical stu dies can be further conducted to provide theoretical support for the effectiveness and applicability of transfer learning. As a popular and promising area in machine learning, transfer learning shows some advantages over traditional machine learning such as less data dependency and less label depen- dency . W e hope our work can help readers have a better understanding of the research status and the research ideas . ACKNOWLEDGME NT S The research work is supported by the National Key Re- search and Development Program of China under Grant No. 2018YFB1004300, the National Natural Science Foundation of China under Grant No. U1836206, U1811461, 61773361, 61836013, and the Project of Y outh Innovation Promotion Association CAS under Grant No. 2017146. REFERENCES [1] D.N. Perkins and G. Salomon, T ransfer of Learning. Oxford, England: Pergamon, 1992. [2] S.J. Pan and Q. Y ang, A survey on transfer learning, IEEE T rans. Knowl. Data Eng. , vol. 22, no. 10, pp. 13451359, Oct. 2010. [3] Z. W ang, Z. Dai, B. Poczos, and J. Carbonell, Characterizing and avoiding negative transfer , in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Long Beach, Jun. 2019, pp. 11293 11302. [4] K. W eiss, T .M. Khoshgoftaar , and D. W ang, A survey of tra nsfer learning, J. Big Data , vol. 3, no. 1, Dec. 2016. [5] J. Huang, A.J. Smola, A. Gretton, K.M. Borgwardt, and B. Sch  olkopf, Correcting sample selection bias by unlabeled data, in Proc. 20th Annual Conference on Neural Information Processing Sy stems, V ancouver , Dec. 2006, pp. 601608. [6] M. Sugiyama, T . Suzuki, S. Nakajima, H. Kashima, P . Bnau, and M. Kawanabe, Direct importance estimation for covariate s hift adaptation, Ann. Inst. Stat. Math. , vol. 60, no. 4, pp. 699746, Dec. 2008. [7] O. Day and T .M. Khoshgoftaar , A survey on heterogeneous trans- fer learning, J. Big Data , vol. 4, no. 1, Dec. 2017. [8] M.E. T aylor and P . Stone, T ransfer learning for reinforc ement learning domains: A survey , J. Mach. Learn. Res. , vol. 10, pp. 1633 1685, Sep. 2009. [9] H.B. Ammar , E. Eaton, J.M. Luna, and P . Ruvolo, Autonomo us cross-domain knowledge transfer in lifelong policy gradie nt rein- forcement learning, in Proc. 24th International Joint Conference on Artiﬁcial Intelligence , Buenos Aires, Jul. 2015, pp. 33453351. [10] P . Zhao and S.C.H. Hoi, OTL: A framework of online transf er learning, in Proc. 27th International Conference on Machine Learning , Haifa, Jun. 2010, pp. 12311238. [11] O. Chapelle, B. Schlkopf, and A. Zien, Semi-supervised Learning . Cambridge: MIT Press, 2010. [12] S. Sun, A survey of multi-view machine learning, Neural Com- put. Appl. , vol. 23, no. 78, pp. 20312038, Dec. 2013. [13] C. Xu, D. T ao, and C. Xu, A survey on multi-view learning , 2013, arXiv:1304.5634v1. [14] J. Zhao, X. Xie, X. Xu, and S. Sun, Multi-view learning ove rview: Recent progress and new challenges, Inf. Fusion , vol. 38, pp. 4354, Nov . 2017. [15] D. Zhang, J. He, Y . Liu, L. Si, and R. Lawrence, Multi-vie w transfer learning with a large margin approach, in Proc. 17th ACM SIGKDD International Conference on Knowledge Discovery an d Data Mining, San Diego, Aug. 2011, pp. 12081216. [16] P . Y ang and W . Gao, Multi-view discriminant transfer l earning, in Proc. 23rd International Joint Conference on Artiﬁcial Int elligence, Beijing, Aug. 2013, pp. 18481854. [17] K.D. Feuz and D.J. Cook, Collegial activity learning b etween heterogeneous sensors, Knowl. Inf. Syst. , vol. 53, pp. 337364, Mar . 2017. [18] Y . Zhang and Q. Y ang, An overview of multi-task learnin g, Natl. Sci. Rev . , vol. 5, no. 1, pp. 3043, Jan. 2018. [19] W . Zhang, R. Li, T . Zeng, Q. Sun, S. Kumar , J. Y e, and S. Ji, De ep model based transfer and multi-task learning for biologica l image analysis, in Proc. 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Sydney , Aug. 2015, pp. 1475 1484. [20] A. Liu, N. Xu, W . Nie, Y . Su, and Y . Zhang, Multi-domain an d multi-task learning for human action recognition, IEEE T rans. Image Process. , vol. 28, no. 2, pp. 853867, Feb. 2019. [21] X. Peng, Z. Huang, X. Sun, and K. Saenko, Domain agnostic learning with disentangled representations, in Proc. 36th Interna- tional Conference on Machine Learning , Long Beach, Jun. 2019, pp. 51025112. [22] J. Lu, V . Behbood, P . Hao, H. Zuo, S. Xue, and G. Zhang, T ra nsfer learning using computational intelligence: A survey , Knowledge- Based Syst. , vol. 80, pp. 1423, May 2015. [23] C. T an, F . Sun, T . Kong, W . Zhang, C. Y ang, and C. Liu, A Surv ey on deep transfer learning, in Proc. 27th International Conference on Artiﬁcial Neural Networks , Rhodes, Oct. 2018, pp. 270279. [24] M. W ang and W . Deng, Deep visual domain adaptation: A survey , Neurocomputing, vol. 312, pp. 135153, Oct. 2018. [25] D. Cook, K.D. Feuz, and N.C. Krishnan, T ransfer learni ng for activity recognition: A survey , Knowl. Inf. Syst. , vol. 36, no. 3, pp. 537556, Sep. 2013. [26] L. Shao, F . Zhu, and X. Li, T ransfer learning for visual c ategoriza- tion: A survey , IEEE T rans. Neural Netw . Learn. Syst. , vol. 26, no. 5, pp. 10191034, May 2015. [27] W . Pan, A survey of transfer learning for collaborativ e recom- mendation with auxiliary data, Neurocomputing, vol. 177, pp. 447 453, Feb. 2016. [28] R. Liu, Y . Shi, C. Ji, and M. Jia, A Survey of sentiment anal ysis based on transfer learning, IEEE Access , vol. 7, pp. 8540185412, Jun. 2019. [29] Q. Sun, R. Chattopadhyay , S. Panchanathan, and J. Y e, A tw o- stage weighting framework for multi-source domain adaptat ion, in Proc. 25th Annual Conference on Neural Information Process ing Systems, Granada, Dec. 2011, pp. 505513. [30] M. Belkin, P . Niyogi, and V . Sindhwani, Manifold regula rization: A geometric framework for learning from labeled and unlabel ed examples, J. Mach. Learn. Res. , vol. 7, pp. 23992434, Nov . 2006. [31] W . Dai, Q. Y ang, G. Xue, and Y . Y u, Boosting for transfer learning, in Proc. 24th International Conference on Machine Learning , Corvalis, Jun. 2007, pp. 193200. [32] Y . Freund and R.E. Schapire, A decision-theoretic gene ralization of on-line learning and an application to boosting, J. Comput. Syst. Sci., vol. 55, no. 1, pp. 119139, Aug. 1997. 27 [33] Y . Y ao and G. Doretto, Boosting for transfer learning w ith multi- ple sources, in Proc. IEEE Conference on Computer V ision and Pattern Recognition, San Francisco, Jun. 2010, pp. 18551862. [34] J. Jiang and C. Zhai, Instance weighting for domain ada ptation in NLP , in Proc. 45th Annual Meeting of the Association of Computation al Linguistics, Prague, Jun. 2007, pp. 264271. [35] K.M. Borgwardt, A. Gretton, M.J. Rasch, H.-P . Kriegel, B. Scholkopf, and A.J. Smola, Integrating structured biologic al data by kernel maximum mean discrepancy , Bioinformatics, vol. 22, no. 14, pp. 4957, Jul. 2006. [36] S.J. Pan, I.W . T sang, J.T . Kwok, and Q. Y ang, Domain adap tation via transfer component analysis, IEEE T rans. Neural Netw . , vol. 22, no. 2, pp. 199210, Feb. 2011. [37] M. Ghifary , W .B. Kleijn, and M. Zhang, Domain adaptive neural networks for object recognition, in Proc. Paciﬁc Rim International Conference on Artiﬁcial Intelligence , Gold Coast, Dec. 2014, pp. 898 904. [38] M. Long, J. W ang, G. Ding, J. Sun, and P .S. Y u, T ransfer fea ture learning with joint distribution adaptation,in Proc. IEEE Interna- tional Conference on Computer V ision , Sydney , Dec. 2013, pp. 2200 2207. [39] M. Long, J. W ang, G. Ding, S.J. Pan, and P .S. Y u, Adaptatio n regularization: A general framework for transfer learning , IEEE T rans. Knowl. Data Eng. , vol. 26, no. 5, pp. 1076-1089, May 2014. [40] S. Kullback and R.A. Leibler , On information and sufﬁci ency , Ann. Math. Statist. , vol. 22, no. 1, pp. 7986, 1951. [41] W . Dai, G.-R. Xue, Q. Y ang, and Y . Y u, Co-clustering bas ed classiﬁcation for out-of-domain documents, in Proc. 13th ACM SIGKDD International Conference on Knowledge Discovery an d Data Mining, San Jose, Aug. 2007, pp. 210219. [42] W . Dai, Q. Y ang, G. Xue, and Y . Y u, Self-taught clusterin g, in Proc. 25th International Conference of Machine Learning , Helsinki, Jul. 2008, pp. 200207. [43] J. Davis and P . Domingos, Deep transfer via second-ord er Markov logic, in Proc. 26th International Conference on Machine Learning, Montreal, Jun. 2009, pp. 217224. [44] F . Zhuang, X. Cheng, P . Luo, S.J. Pan, and Q. He, Supervise d rep- resentation learning: T ransfer learning with deep autoenc oders, in Proc. 24th International Joint Conference on Artiﬁcial Int elligence, Buenos Aires, Jul. 2015, pp. 41194125. [45] I. Dagan, L. Lee, and F . Pereira, Similarity-based meth ods for word sense disambiguation, in Proc. 35th Annual Meeting of the Association of Computational Linguistics and 8th Conferen ce of the European Chapter of the Association for Computational Ling uistics (ACLEACL), Madrid, Jul. 1997, pp. 5663. [46] B. Chen, W . Lam, I. T sang, and T . W ong, Location and scat ter matching for dataset shift in text mining, in Proc. 10th IEEE International Conference on Data Mining , Sydney , Dec. 2010, pp. 773 778. [47] S. Dey , S. Madikeri, and P . Motlicek, Information theore tic clus- tering for unsupervised domain-adaptation, in Proc. IEEE Interna- tional Conference on Acoustics, Speech and Signal Processi ng, Shanghai, Mar . 2016, pp. 55805584. [48] W .-H. Chen, P .-C. Cho, and Y .-L. Jiang, Activity recog nition using transfer learning, Sens. Mater . , vol. 29, no. 7, pp. 897904, Jul. 2017. [49] J. Giles, K.K. Ang, L.S. Mihaylova, and M. Arvaneh, A sub ject- to-subject transfer learning framework based on Jensen-Sha nnon divergence for improving brain-computer interface, in Proc. IEEE International Conference on Acoustics, Speech and Signal P rocessing, Brighton, May 2019, pp. 30873091. [50] L.M. Bregman, The relaxation method of ﬁnding the comm on point of convex sets and its application to the solution of pr oblems in convex programming, USSR Comput. Math. Math. Phys. , vol. 7, no. 3, pp. 200217, 1967. [51] S. Si, D. T ao, and B. Geng, Bregman divergence-based regu lariza- tion for transfer subspace learning, IEEE T rans. Knowl. Data Eng. , vol. 22, no. 7, pp. 929942, Jul. 2010. [52] H. Sun, S. Liu, S. Zhou, and H. Zou, Unsupervised cross-vie w semantic transfer for remote sensing image classiﬁcation,  IEEE Geosci. Remote Sens. Lett. , vol. 13, no. 1, pp. 1317, Jan. 2016. [53] H. Sun, S. Liu, and S. Zhou, Discriminative subspace align ment for unsupervised visual domain adaptation, Neural Process. Lett. , vol. 44, no. 3, pp. 779793, Dec. 2016. [54] Q. Shi, Y . Zhang, X. Liu, and K. Zhao, Regularised transf er learning for hyperspectral image classiﬁcation, IET Comput. V is. , vol. 13, no. 2, pp. 188193, Feb. 2019. [55] A. Gretton, O. Bousquet, A.J. Smola, and B. Schlkopf, Mea suring statistical dependence with Hilbert-Schmidt norms, in Proc. 18th International Conference on Algorithmic Learning Theory , Singapore, Oct. 2005, pp. 6377. [56] H. W ang and Q. Y ang, T ransfer learning by structural an alogy , in Proc. 25th AAAI Conference on Artiﬁcial Intelligence , San Francisco, Aug. 2011, pp. 513518. [57] M. Xiao and Y . Guo, Feature space independent semi-sup ervised domain adaptation via kernel matching, IEEE T rans. Pattern Anal. Mach. Intell. , vol. 37, no. 1, pp. 5466, Jan. 2015. [58] K. Y an, L. Kou, and D. Zhang, Learning domain-invarian t sub- space using domain features and independence maximization , IEEE T . Cybern. , vol. 48, no. 1, pp. 288299, Jan. 2018. [59] J. Shen, Y . Qu, W . Zhang, and Y . Y u, W asserstein distance guided representation learning for domain adaptation, in Proc. 32nd AAAI Conference on Artiﬁcial Intelligence , New Orleans, Feb. 2018, pp. 40584065. [60] C.-Y . Lee, T . Batra, M.H. Baig, and D. Ulbricht, Sliced W asserstein discrepancy for unsupervised domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Long Beach, Jun. 2019, pp. 1028510295. [61] W . Zellinger , T . Grubinger , E. Lughofer , T . Natschlger , and S. Saminger-Platz, Central moment discrepancy (CMD) for doma in- invariant representation learning, in Proc. 5th International Confer- ence on Learning Representations , T oulon, Apr . 2017, pp. 113. [62] A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan , M. Pon- til, K. Fukumizu, and B.K. Sriperumbudur , Optimal kernel ch oice for large-scale two-sample tests, in Proc. 26th Annual Conference on Neural Information Processing Systems , Lake T ahoe, Dec. 2012, pp. 12051213. [63] H. Y an, Y . Ding, P . Li, Q. W ang, Y . Xu, and W . Zuo, Mind the class weight bias: W eighted maximum mean discrepancy for un su- pervised domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Honolulu, Jul. 2017, pp. 22722281. [64] H. Daum  e III, Frustratingly easy domain adaptation,  in Proc. 45th Annual Meeting of the Association for Computational Li nguistics, Prague, Jun. 2007, pp. 256263. [65] H. Daum  e III, A. Kumar , and A. Saha, Co-regularization based semi-supervised domain adaptation, in Proc. 24th Annual Confer- ence on Neural Information Processing Systems , V ancouver , Dec. 2010, pp. 478486. [66] L. Duan, D. Xu, and I.W . T sang, Learning with augmented features for heterogeneous domain adaptation, in Proc. 29th Inter- national Conference on Machine Learning , Edinburgh, Jun. 2012, pp. 18. [67] W . Li, L. Duan, D. Xu, and I.W . T sang, Learning with augm ented features for supervised and semi-supervised heterogeneou s do- main adaptation, IEEE T rans. Pattern Anal. Mach. Intell. , vol. 36, no. 6, pp. 11341148, Jun. 2014. [68] K.I. Diamantaras and S.Y . Kung, Principal Component Neural Net- works. New Y ork: Wiley , 1996. [69] B. Schlkopf, A. Smola, and K. Mller , Nonlinear component anal- ysis as a kernel eigenvalue problem, Neural Comput. , vol. 10, no. 5, pp. 12991319, Jul. 1998. [70] J. W ang, Y . Chen, S. Hao, W . Feng, and Z. Shen, Balanced distribution adaptation for transfer learning, in Proc. 17th IEEE International Conference on Data Mining , New Orleans, Nov . 2017, pp. 11291134. [71] A. Blum and T . Mitchell, Combining labeled and unlabel ed data with co-training, in Proc. 11th Annual Conference on Computational Learning Theory , Madison, Jul. 1998, pp. 92100. [72] M. Chen, K.Q. W einberger , and J.C. Blitzer , Co-traini ng for domain adaptation, in Proc. 25th Annual Conference on Neural Information Processing Systems , Granada, Dec. 2011, pp. 24562464. [73] Z.-H. Zhou and M. Li, T ri-training: Exploiting unlabe led data using three classiﬁers, IEEE T rans. Knowl. Data Eng. , vol. 17, no. 11, pp. 15291541, Nov . 2005. [74] K. Saito, Y . Ushiku, and T . Harada, Asymmetric tri-trai ning for unsupervised domain adaptation, in Proc. 34th International Conference on Machine Learning , Sydney , Aug. 2017, pp. 29882997. [75] S.J. Pan, J.T . Kwok, and Q. Y ang, T ransfer learning via d imen- sionality reduction, in Proc. 23rd AAAI Conference on Artiﬁcial Intelligence, Chicago, Jul. 2008, pp. 677682. [76] K.Q. W einberger , F . Sha, and L.K. Saul, Learning a kernel matrix for nonlinear dimensionality reduction, in Proc. 21st International Conference on Machine Learning , Banff, Jul. 2004, pp. 106113. 28 [77] L. V andenberghe and S. Boyd, Semideﬁnite programming, SIAM Rev . , vol. 38, no. 1, pp. 4995, Mar . 1996. [78] S.J. Pan, I.W . T sang, J.T . Kwok, and Q. Y ang, Domain adap tation via transfer component analysis, in Proc. 21st International Joint Conference on Artiﬁcial Intelligence , Pasadena, Jul. 2009, pp. 1187 1192. [79] C. Hou, Y .H. T sai, Y . Y eh, and Y .F . W ang, Unsupervised d omain adaptation with label and structural consistency , IEEE T rans. Image Process., vol. 25, no. 12, pp. 55525562, Dec. 2016. [80] J. T ahmoresnezhad and S. Hashemi, V isual domain adapta tion via transfer feature learning, Knowl. Inf. Syst. , vol. 50, no. 2, pp. 585605, Feb. 2017. [81] J. Zhang, W . Li, and P . Ogunbona, Joint geometrical and statistical alignment for visual domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Honolulu, Jul. 2017, pp. 51505158. [82] B. Schlkopf, R. Herbrich, and A.J. Smola, A generalized r epre- senter theorem, in Proc. International Conference on Computational Learning Theory , Amsterdam, Jul. 2001, pp. 416426. [83] L. Duan, I.W . T sang, and D. Xu, Domain transfer multipl e kernel learning, IEEE T rans. Pattern Anal. Mach. Intell. , vol. 34, no. 3, pp. 465479, Mar . 2012. [84] A. Rakotomamonjy , F .R. Bach, S. Canu, and Y . Grandvalet, Sim- pleMKL, J. Mach. Learn. Res. , vol. 9, pp. 2491-2521, Nov . 2008. [85] I.S. Dhillon, S. Mallela, and D.S. Modha, Information-th eoretic co-clustering, in Proc. 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , W ashington, Aug. 2003, pp. 8998. [86] S. Deerwester , S.T . Dumais, G.W . Furnas, T .K. Landauer , a nd R. Harshman, Indexing by latent semantic analysis, J. Am. Soc. Inf. Sci., vol. 41, pp. 391407, Sep. 1990. [87] T . Hofmann, Probabilistic latent semantic analysis,  in Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence , Stockholm, Jul. 1999, pp. 289296. [88] J. Y oo and S. Choi, Probabilistic matrix tri-factoriza tion, in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, T aipei, Apr . 2009, pp. 15531556. [89] A. Dempster , N. Laird, and D. Rubin, Maximum likelihoo d from incomplete data via the EM algorithm, J. R. Stat. Soc. - Ser . B , vol. 39, no. 1, pp. 138, 1977. [90] G.-R. Xue, W . Dai, Q. Y ang, and Y . Y u, T opic-bridged PLSA for cross-domain text classiﬁcation, in Proc. 31st Annual International ACM SIGIR Conference on Research and Development in Informa tion Retrieval, Singapore, Jul. 2008, pp. 627634. [91] F . Zhuang, P . Luo, Z. Shen, Q. He, Y . Xiong, Z. Shi, and H. Xio ng, Collaborative Dual-PLSA: Mining distinction and commonal ity across multiple domains for text classiﬁcation, in Proc. 19th ACM International Conference on Information and Knowledge Man agement, T oronto, Oct. 2010, pp. 359368. [92] F . Zhuang, P . Luo, Z. Shen, Q. He, Y . Xiong, Z. Shi, and H. Xio ng, Mining distinction and commonality across multiple domai ns using generative model for text classiﬁcation, IEEE T rans. Knowl. Data Eng. , vol. 24, no. 11, pp. 20252039, Nov . 2012. [93] F . Zhuang, P . Luo, P . Y in, Q. He, and Z. Shi, Concept learn - ing for cross-domain text classiﬁcation: A general probabi listic framework, in Proc. 23rd International Joint Conference on Artiﬁcial Intelligence, Beijing, Aug. 2013, pp. 19601966. [94] J. Blitzer , R. McDonald, and F . Pereira, Domain adapta tion with structural correspondence learning, in Proc. Conference on Empirical Methods in Natural Language Processing , Sydney , Jul. 2006, pp. 120 128. [95] R.K. Ando and T . Zhang, A framework for learning predic tive structures from multiple tasks and unlabeled data, J. Mach. Learn. Res., vol. 6, pp. 18171853, Dec. 2005. [96] X. Glorot, A. Bordes, and Y . Bengio, Domain adaptation for large-scale sentiment classiﬁcation: A deep learning appr oach, in Proc. 28th International Conference on Machine Learning , Bellevue, Jun. 2011, pp. 513520. [97] P . V incent, H. Larochelle, Y . Bengio, and P .-A. Manzago l, Extract- ing and composing robust features with denoising autoencod ers, in Proc. 25th International Conference on Machine Learning , Helsinki, Jul. 2008, pp. 10961103. [98] M. Chen, Z. Xu, K. W einberger , and F . Sha, Marginalized d enois- ing autoencoders for domain adaptation, in Proc. 29th International Conference on Machine Learning , Edinburgh, Jun. 2012, pp. 767774. [99] M. Chen, K.Q. W einberger , Z. Xu, and F . Sha, Marginalizi ng stacked linear denoising autoencoders, J. Mach. Learn. Res. , vol. 16, no. 1, pp. 38493875, Jan. 2015. [100] B. Fernando, A. Habrard, M. Sebban, and T . T uytelaars,  Unsu- pervised visual domain adaptation using subspace alignmen t, in Proc. IEEE International Conference on Computer V ision , Sydney , Dec. 2013, pp. 29602967. [101] B. Sun and K. Saenko, Subspace distribution alignment fo r unsupervised domain adaptation, in Proc. British Machine V ision Conference, Swansea, Sep. 2015, pp. 24.124.10. [102] B. Gong, Y . Shi, F . Sha, and K. Grauman, Geodesic ﬂow kern el for unsupervised domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Providence, Jun. 2012, pp. 20662073. [103] R. Gopalan, Ruonan Li, and R. Chellappa, Domain adapt ation for object recognition: An unsupervised approach, in Proc. IEEE International Conference on Computer V ision , Barcelona, Jun. 2011, pp. 999-1006. [104] M.I. Zelikin, Control Theory and Optimization I in Encyclopaedia of Mathematical Sciences, vol. 86, Berlin: Springer , 2000. [105] B. Sun, J. Feng, and K. Saenko, Return of frustratingly e asy domain adaptation, in Proc. 30th AAAI Conference on Artiﬁcial Intelligence, Phoenix, Feb. 2016, pp. 20582065. [106] S.J. Pan, X. Ni, J.-T . Sun, Q. Y ang, and Z. Chen, Cross-do main sentiment classiﬁcation via spectral feature alignment, in Proc. 19th International Conference on World Wide Web , Raleigh, Apr . 2010, pp. 751760. [107] J. Blitzer , M. Dredze, and F . Pereira, Biographies, b ollywood, boom-boxes and blenders: Domain adaptation for sentiment c lassi- ﬁcation, in Proc. 45th Annual Meeting of the Association of Computa- tional Linguistics , Prague, Jun. 2007, pp. 440447. [108] F .R.K. Chung, Spectral Graph Theory . Providence: American Math- ematical Society , 1997. [109] A.Y . Ng, M.I. Jordan, and Y . W eiss, On spectral cluste ring: Analysis and an algorithm, in Proc. 15th Annual Conference on Neural Information Processing Systems , V ancouver , Dec. 2001, pp. 849- 856. [110] X. Ling, W . Dai, G.-R. Xue, Q. Y ang, and Y . Y u, Spectral d omain- transfer learning, in Proc. 14th ACM SIGKDD International Confer- ence on Knowledge Discovery and Data Mining , Las V egas, Aug. 2008, pp. 488496. [111] S.D. Kamvar , D. Klein, and C.D. Manning, Spectral learn ing, in Proc. 18th International Joint Conference on Artiﬁcial Int elligence, Acapulco, Aug. 2003, pp. 561566. [112] J. Shi and J. Malik, Normalized cuts and image segmenta tion, IEEE T rans. Pattern Anal. Mach. Intell. , vol .22, no. 8, pp. 888905, Aug. 2000. [113] L. Duan, I.W . T sang, D. Xu, and T .-S. Chua, Domain adapt ation from multiple sources via auxiliary classiﬁers, in Proc. 26th In- ternational Conference on Machine Learning , Montreal, Jun. 2009, pp. 289296. [114] L. Duan, D. Xu, and I.W . T sang, Domain adaptation from multiple sources: A domain-dependent regularization appr oach, IEEE T rans. Neural Netw . Learn. Syst. , vol. 23, no. 3, pp. 504518, Mar . 2012. [115] P . Luo, F . Zhuang, H. Xiong, Y . Xiong, and Q. He, T ransf er learn- ing from multiple source domains via consensus regularizat ion, in Proc. 17th ACM Conference on Information and Knowledge Mana gement, Napa V alley , Oct. 2008, pp. 103112. [116] F . Zhuang, P . Luo, H. Xiong, Y . Xiong, Q. He, and Z. Shi, C ross- domain learning from multiple sources: A consensus regular ization perspective, IEEE T rans. Knowl. Data Eng. , vol. 22, no. 12, pp. 1664 1678, Dec. 2010. [117] T . Evgeniou, C.A. Micchelli, and M. Pontil, Learning multiple tasks with kernel methods, J. Mach. Learn. Res. , vol. 6, pp. 615-637, Apr . 2005. [118] T . Kato, H. Kashima, M. Sugiyama, and K. Asai, Multi-ta sk learning via conic programming, in Proc. 21st Annual Conference on Neural Information Processing Systems , V ancouver , Dec. 2007, pp. 737744. [119] A.J. Smola and B. Schlkopf, A tutorial on support vector regres- sion, Stat. Comput. , vol. 14, no. 3, pp. 199222, Aug. 2004. [120] J. W eston, R. Collobert, F . Sinz, L. Bottou, and V . V apni k, Infer- ence with the universum, in Proc. 23rd International Conference on Machine Learning , Pittsburgh, Jun. 2006, pp. 10091016. [121] X. Y u and Y . Aloimonos, Attribute-based transfer lea rning for object categorization with zeroone training example, in Proc. 29 European Conference on Computer V ision , Heraklion, Sep. 2010, pp. 127140. [122] F . Zhuang, P . Luo, H. Xiong, Q. He, Y . Xiong, and Z. Shi, E x- ploiting associations between word clusters and document c lasses for cross-domain text categorization, Stat. Anal. Data Min. , vol. 4, no. 1, pp. 100114, Feb. 2011. [123] F . Zhuang, P . Luo, C. Du, Q. He, Z. Shi, and H. Xiong, T rip lex transfer learning: Exploiting both shared and distinct con cepts for text classiﬁcation, IEEE T . Cybern. , vol. 44, no. 7, pp. 11911203, Jul. 2014. [124] M. Long, J. W ang, G. Ding, W . Cheng, X. Zhang, and W . W ang , Dual transfer learning, in Proc. 12th SIAM International Conference on Data Mining , Anaheim, Apr . 2012, pp. 540551. [125] H. W ang, F . Nie, H. Huang, and C. Ding, Dyadic transfer learn- ing for cross-domain image classiﬁcation, in Proc. International Conference on Computer V ision , Barcelona, Nov . 2011, pp. 551556. [126] D. W ang, C. Lu, J. Wu, H. Liu, W . Zhang, F . Zhuang, and H. Zhang, Softly associative transfer learning for cros s- domain classiﬁcation, IEEE T . Cybern. , to be published. doi: 10.1109TCYB.2019.2891577. [127] Q. Do, W . Liu, J. Fan, and D. T ao, Unveiling hidden impl icit similarities for cross-domain recommendation, IEEE T rans. Knowl. Data Eng. , to be published. doi: 10.1109TKDE.2019.2923904. [128] T . T ommasi and B. Caputo, The more you know , the less you learn: from knowledge transfer to one-shot learning of o bject categories in Proc. British Machine V ision Conference , London, Sep. 2009, pp. 80.180.11. [129] G.C. Cawley , Leave-one-out cross-validation based model selec- tion criteria for weighted LS-SVMs, in Proc. IEEE International Joint Conference on Neural Network , V ancouver , Jul. 2006, pp. 16611668. [130] T . T ommasi, F . Orabona, and B. Caputo, Safety in number s: Learning categories from few examples with multi model know l- edge transfer , in Proc. IEEE Conference on Computer V ision and Pattern Recognition , San Francisco, Jun. 2010, pp. 30813088. [131] C.-K. Lin, Y .-Y . Lee, C.-H. Y u, and H.-H. Chen, Explor ing ensemble of models in taxonomy-based cross-domain sentime nt classiﬁcation, in Proc. 23rd ACM International on Conference on Information and Knowledge Management , Shanghai, Nov . 2014, pp. 12791288. [132] J. Gao, W . Fan, J. Jiang, and J. Han, Knowledge transfe r via mul- tiple model local structure mapping, in Proc. 14th ACM SIGKDD International Conference on Knowledge Discovery and Data M ining, Las V egas, Aug. 2008, pp. 283291. [133] F . Zhuang, P . Luo, S.J. Pan, H. Xiong, and Q. He. Ensembl e of anchor adapters for transfer learning, in Proc. 25th ACM In- ternational on Conference on Information and Knowledge Man agement, Indianapolis, Oct. 2016, pp. 23352340. [134] F . Zhuang, X. Cheng, P . Luo, S.J. Pan, and Q. He, Supervis ed representation learning with double encoding-layer autoe ncoder for transfer learning, ACM T rans. Intell. Syst. T echnol. , vol. 9, no. 2, pp. 117, Jan. 2018. [135] M. Ghifary , W .B. Kleijn, and M. Zhang, Domain adaptiv e neural networks for object recognition, in Proc. 13th Paciﬁc Rim Interna- tional Conference on Artiﬁcial Intelligence , Gold Coast, Dec. 2014, pp. 898904. [136] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T . Darrell , Deep domain confusion: Maximizing for domain invariance,  2014, arXiv:1412.3474v1. [137] M. Long, Y . Cao, J. W ang, and M.I. Jordan, Learning tra nsferable features with deep adaptation networks, in Proc. 32nd International Conference on Machine Learning , Lille, Jul. 2015, pp. 97105. [138] A. Krizhevsky , I. Sutskever , and G.E. Hinton, Imagene t classi- ﬁcation with deep convolutional neural networks, in Proc. 26th Annual Conference on Neural Information Processing System s, Lake T ahoe, Dec. 2012, pp. 10971105. [139] I. Goodfellow , J. Pouget-Abadie, M. Mirza, B. Xu, D. W a rde- Farley , S. Ozair , A. Courville, and Y . Bengio, Generative ad ver- sarial nets, in Proc. 28th Annual Conference on Neural Information Processing Systems , Montreal, Dec. 2014, pp. 26722680. [140] J. Y osinski, J. Clune, Y . Bengio, and H. Lipson, How tr ansferable are features in deep neural networks? in Proc. 28th Annual Confer- ence on Neural Information Processing Systems , Montreal, Dec. 2014, pp. 33203328. [141] M. Long, Y . Cao, Z. Cao, J. W ang, and M.I. Jordan, T rans - ferable representation learning with deep adaptation netw orks, IEEE T rans. Pattern Anal. Mach. Intell. , to be published. doi: 10.1109TP AMI.2018.2868685. [142] Y . Grandvalet and Y . Bengio, Semi-supervised learnin g by en- tropy minimization, in Proc. 18th Annual Conference on Neural Information Processing Systems , V ancouver , Dec. 2004, pp. 529536. [143] C. Szegedy , W . Liu, Y . Jia, P . Sermanet, S. Reed, D. Anguelo v , D. Erhan, V . V anhoucke, and A. Rabinovich, Going deeper wit h convolutions, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Boston, Jun. 2015, pp. 19. [144] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learnin g for image recognition, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Las V egas, Jun. 2016, pp. 770778. [145] K.P . Chwialkowski, A. Ramdas, D. Sejdinovic, and A. Gre tton, Fast two-sample testing with analytic representations of probabil- ity measures, in Proc. 29th Annual Conference on Neural Information Processing Systems , Montreal, Dec. 2015, pp. 19811989. [146] M. Long, H. Zhu, J. W ang, and M.I. Jordan, Unsupervise d do- main adaptation with residual transfer networks, in Proc. 30th An- nual Conference on Neural Information Processing Systems , Barcelona, Dec. 2016, pp. 136144. [147] M. Long, H. Zhu, J. W ang, and M.I. Jordan, Deep transfe r learning with joint adaptation networks, in Proc. 34th International Conference on Machine Learning , Sydney , Aug. 2017, pp. 22082217. [148] B. Sun and K. Saenko, Deep CORAL: Correlation alignment for deep domain adaptation, in Proc. European Conference on Computer V ision Workshops , Amsterdam, Oct. 2016, pp. 443450. [149] C. Chen, Z. Chen, B. Jiang, and X. Jin, Joint domain ali gnment and discriminative feature learning for unsupervised deep domain adaptation, in Proc. 33rd AAAI Conference on Artiﬁcial Intelligence , Honolulu, Jan. 2019, pp. 32963303. [150] Y . Pan, T . Y ao, Y . Li, Y . W ang, C.-W . Ngo, and T . Mei, T ra ns- ferrable prototypical networks for unsupervised domain ad ap- tation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition, Long Beach, Jun. 2019, pp. 22392247. [151] G. Kang, L. Jiang, Y . Y ang, and A.G. Hauptmann, Contra stive adaptation network for unsupervised domain adaptation, i n Proc. IEEE Conference on Computer V ision and Pattern Recognition , Long Beach, Jun. 2019, pp. 48934902. [152] Y . Zhu, F . Zhuang, J. W ang, J. Chen, Z. Shi, W . Wu, and Q. He , Multi-representation adaptation network for cross-doma in image classiﬁcation, Neural Netw . , vol. 119. pp. 214221, Nov . 2019. [153] Y . Zhu, F . Zhuang, and D. W ang, Aligning domain-speci ﬁc dis- tribution and classiﬁer for cross-domain classiﬁcation fr om multiple sources, in Proc. 33rd AAAI Conference on Artiﬁcial Intelligence , Honolulu, Jan. 2019, pp. 59895996. [154] Y . Ganin and V . Lempitsky , Unsupervised domain adapt ation by backpropagation, in Proc. 32nd International Conference on Machine Learning, Lille, Jul. 2015, pp. 11801189. [155] Y . Ganin, E. Ustinova, H. Ajakan, P . Germain, H. Laroch elle, F .Laviolette, M. Marchand, and V . Lempitsky , Domain-adve rsarial training of neural networks, J. Mach. Learn. Res. , vol. 17, pp. 135, Apr . 2016. [156] E. Tzeng, J. Hoffman, K. Saenko, and T . Darrell, Advers arial discriminative domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Honolulu, Jul. 2017, pp. 29622971. [157] J. Hoffman, E. Tzeng, T . Park, J.-Y . Zhu, P . Isola, K. Sae nko, A.A. Efros, and T . Darrell, CyCADA: Cycle-consistent adve rsarial domain adaptation, in Proc. 35th International Conference on Machine Learning, Stockholm, Jul. 2018, pp. 19942003. [158] M. Long, Z. Cao, J. W ang, and M.I. Jordan, Conditional adver- sarial domain adaptation, in Proc. 32nd Annual Conference on Neural Information Processing Systems , Montreal, Dec. 2018, pp. 16401650. [159] Y . Zhang, H. T ang, K. Jia, and M. T an, Domain-symmetri c net- works for adversarial domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Long Beach, Jun. 2019, pp. 50315040. [160] H. Zhao, S. Zhang, G. Wu, J.M.F . Moura, J.P . Costeira, an d G.J. Gordon, Adversarial multiple source domain adaptation, in Proc. 32nd Annual Conference on Neural Information Processing Sy stems, Montreal, Dec. 2018, pp. 85598570. [161] C. Y u, J. W ang, Y . Chen, and M. Huang, T ransfer learnin g with dynamic adversarial adaptation network, in Proc. 19th IEEE International Conference on Data Mining , Beijing, Nov . 2019, pp. 19. [162] J. Zhang, Z. Ding, W . Li, and P . Ogunbona, Importance w eighted adversarial nets for partial domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Salt Lake City , Jun. 2018, pp. 81568163. 30 [163] Z. Cao, M. Long, J. W ang, and M.I. Jordan, Partial tran sfer learn- ing with selective adversarial networks, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Salt Lake City , Jun. 2018, pp. 27242732. [164] B. W ang, M. Qiu, X. W ang, Y . Li, Y . Gong, X. Zeng, J. Huang , B. Zheng, D. Cai, and J. Zhou, A minimax game for instance based selective transfer learning, in Proc. 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Anchorage, Aug. 2019, pp. 3443. [165] X. Chen, S. W ang, M. Long, and J. W ang, T ransferability vs. discriminability: Batch spectral penalization for advers arial domain adaptation, in Proc. 36th International Conference on Machine Learn- ing, Long Beach, Jun. 2019, pp. 10811090. [166] J. Deng, W . Dong, R. Socher , L.-J. Li, K. Li, and L. Fei-Fe i, ImageNet: A large-scale hierarchical image database, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Miami, Jun. 2009, pp. 248255. [167] M. Maqsood, F . Nazir , U. Khan, F . Aadil, H. Jamal, I. Meh mood, and O. Song, T ransfer learning assisted classiﬁcation and d etection of Alzheimer s disease stages using 3D MRI scans, Sensors, vol. 19, no. 11, pp. 119, Jun. 2019. [168] D.S. Marcus, A.F . Fotenos, J.G. Csernansky , J.C. Morri s, and R.L. Buckner , Open access series of imaging studies: Longitudi nal MRI data in nondemented and demented older adults, J. Cogn. Neurosci. , vol. 22, no. 12, pp. 26772684, Dec. 2010. [169] H.-C. Shin, H.R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Y ao, D. Mollura, and R.M. Summers, Deep convolutional neura l networks for computer-aided detection: CNN architectures , dataset characteristics and transfer Learning, IEEE T rans. Med. Imaging , vol. 35, no. 5, pp. 12851298, May 2016. [170] M. Byra, M. Wu, X. Zhang, H. Jang, Y .-J. Ma, E.Y . Chang, S. Shah, and Jiang Du, Knee menisci segmentation and relaxomet ry of 3D ultrashort echo time cones MR imaging using attention UNet with transfer learning, Magn. Reson. Med. , Sep. 2019, doi: 10.1002mrm.27969. [171] X. T ang, B. Du, J. Huang, Z. W ang, and L. Zhang, On combi ning active and transfer learning for medical data classiﬁcatio n, IET Comput. V is. , vol. 13, no. 2, pp. 194205, Feb. 2019. [172] M. Zeng, M. Li, Z. Fei, Y . Y u, Y . Pan, and J. W ang, Automa tic ICD-9 coding via deep transfer learning, Neurocomputing, vol. 324, pp. 4350, Jan. 2019. [173] G. Schweikert, G. Ratsch, C. Widmer , and B. Scholkopf, A n empirical analysis of domain adaptation algorithms for gen omic sequence analysis, in Proc. 22nd Annual Conference on Neural Infor- mation Processing Systems , V ancouver , Dec. 2008, pp. 14331440. [174] R. Petegrosso, S. Park, T .H. Hwang, and R. Kuang, T rans fer learning across ontologies for phenome-genome associatio n predic- tion, Bioinformatics, vol. 33, no. 4, pp. 529536, Feb. 2017. [175] T . Hwang and R. Kuang, A heterogeneous label propagat ion al- gorithm for disease gene discovery , in Proc. 10th SIAM International Conference on Data Mining , Columbus, Apr . 2010, pp. 583594. [176] Q. Xu, E.W . Xiang, and Q. Y ang, Protein-protein inter action prediction via collective matrix factorization, in Proc. IEEE Interna- tional Conference on Bioinformatics and Biomedicine , Hong Kong, Dec. 2010, pp. 6267. [177] A.P . Singh and G.J. Gordon, Relational learning via co llective matrix factorization, in Proc. 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Las V egas, Aug. 2008, pp. 650658. [178] S. Di, H. Zhang, C. Li, X. Mei, D. Prokhorov , and H. Ling,  Cross- domain trafﬁc scene understanding: A dense correspondence -based transfer learning approach, IEEE T rans. Intell. T ransp. Syst. , vol. 19, no. 3, pp. 745757, Mar . 2018. [179] H. Abdi, Partial least squares regression and projec tion on latent structure regression (PLS Regression), Wiley Interdiscip. Rev .- Comput. Statist. , vol. 2, no. 1, pp. 97106, Jan. 2010. [180] S.D. Pietra, V .D. Pietra, and J. Lafferty , Inducing fe atures of random ﬁelds, IEEE T rans. Pattern Anal. Mach. Intell. , vol. 19, no. 4, pp. 380393, Apr . 1997. [181] C. Liu, J. Y uen, and A. T orralba, Nonparametric scene parsing via label transfer , IEEE T rans. Pattern Anal. Mach. Intell. , vol. 33, no. 12, pp. 23682382, Dec. 2011. [182] C. Lu, F . Hu, D. Cao, J. Gong, Y . Xing, and Z. Li, T ransfe r learning for driver model adaptation in lane-changing scen arios using manifold alignment, IEEE T rans. Intell. T ransp. Syst. , to be published. doi: 10.1109TITS.2019.2925510. [183] D.J. Berndt and J. Clifford, Using dynamic time warpi ng to ﬁnd patterns in time series, in Proc. Knowledge Discovery in Databases Workshop, Seattle, Jul. 1994, pp. 359370. [184] N. Makondo, M. Hiratsuka, B. Rosman, and O. Hasegawa,  A non-linear manifold alignment approach to robot learning f rom demonstrations, J. Robot. Mechatron. , vol. 30, no. 2, pp. 265281, Apr . 2018. [185] P . Angkititrakul, C. Miyajima, and K. T akeda, Modeli ng and adaptation of stochastic driver-behavior model with appli cation to car following, in Proc. IEEE Intelligent V ehicles Symposium (IV) , Baden-Baden, Jun. 2011, pp. 814819. [186] Y . Liu, P . Lasang, S. Pranata, S. Shen, and W . Zhang, Drive r pose estimation using recurrent lightweight network and virtua l data augmented transfer learning, IEEE T rans. Intell. T ransp. Syst. , vol. 20, no. 10, pp. 38183831, Oct. 2019. [187] J. W ang, H. Zheng, Y . Huang, and X. Ding, V ehicle type r ecog- nition in surveillance images from labeled web-nature data using deep transfer learning, IEEE T rans. Intell. T ransp. Syst. , vol. 19, no. 9, pp. 29132922, Sep. 2018. [188] K. Gopalakrishnan, S.K. Khaitan, A. Choudhary , and A. A grawal, Deep convolutional neural networks with transfer learnin g for computer vision-based data-driven pavement distress dete ction, Constr . Build. Mater . , vol. 157, pp. 322330, Dec. 2017. [189] S. Bansod and A. Nandedkar , T ransfer learning for vide o anomaly detection, J. Intell. Fuzzy Syst. , vol. 36, no. 3, pp. 1967 1975, Mar . 2019. [190] G. Rosario, T . Sonderman, and X. Zhu, Deep transfer lea rning for trafﬁc sign recognition, in Proc. IEEE International Conference on Information Reuse and Integration , Salt Lake City , Jul. 2018, pp. 178185. [191] W . Pan, E.W . Xiang, and Q. Y ang, T ransfer learning in c ollabora- tive ﬁltering with uncertain ratings, in Proc. 26th AAAI Conference on Artiﬁcial Intelligence , T oronto, Jul. 2012, pp. 662668. [192] G. Hu, Y . Zhang, and Q. Y ang, T ransfer meets hybrid: A synthetic approach for cross-domain collaborative ﬁlteri ng with text, in Proc. 28th International Conference on World Wide Web , San Francisco, May 2019, pp. 28222829. [193] W . Pan, E.W . Xiang, N.N. Liu, and Q. Y ang, T ransfer lea rning in collaborative ﬁltering for sparsity reduction, in Proc. 24th AAAI Conference on Artiﬁcial Intelligence , Atlanta, Jul. 2010, pp. 230235. [194] W . Pan and Q. Y ang, T ransfer learning in heterogeneou s col- laborative ﬁltering domains, Artif. Intell. , vol. 197, pp. 3955, Apr . 2013. [195] F . Zhuang, Y . Zhou, F . Zhang, X. Ao, X. Xie, and Q. He, Seq uen- tial transfer learning: Cross-domain novelty seeking trai t mining for recommendation, in Proc. 26th International Conference on World Wide Web Companion , Perth, Apr . 2017, pp. 881882. [196] J. Zheng, F . Zhuang, and C. Shi, Local ensemble across m ultiple sources for collaborative ﬁltering, in Proc. 26th ACM International on Conference on Information and Knowledge Management , Singapore, Nov . 2017, pp. 24312434. [197] F . Zhuang, J. Zheng, J. Chen, X. Zhang, C. Shi, and Q. He, T ransfer collaborative ﬁltering from multiple sources vi a consen- sus regularization, Neural Netw . , vol. 108, pp. 287295, Dec. 2018. [198] J. He, R. Liu, F . Zhuang, F . Lin, C. Niu, and Q. He, A gene ral cross-domain recommendation framework via Bayesian neura l net- work, in Proc. 18th IEEE International Conference on Data Mining , Singapore, Nov . 2018, pp. 10011006. [199] F . Zhu, Y . W ang, C. Chen, G. Liu, M.A. Orgun, and J. Wu, A deep framework for cross-domain and cross-system recommendati ons, in Proc. 27th International Joint Conference on Artiﬁcial Int elligence, Stockholm, Jul. 2018, pp. 37113717. [200] F . Y uan, L. Y ao, and B. Benatallah, DARec: Deep domain adap- tation for cross-domain recommendation via transferring r ating patterns, in Proc. 29th International Joint Conference on Artiﬁcial Intelligence, Macao, Aug. 2019, pp. 42274233. [201] E. Bastug, M. Bennis, and M. Debbah, A transfer learni ng approach for cache-enabled wireless networks, in Proc. 13th Inter- national Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks , Mumbai, May 2015, pp. 161166. [202] R. Li, Z. Zhao, X. Chen, J. Palicot, and H. Zhang, T ACT: A transfer actor-critic learning framework for energy savin g in cel- lular radio access networks, IEEE T rans. Wirel. Commun. , vol. 13, no. 4, pp. 20002011, Apr . 2014. [203] Q. Zhao and D. Grace, T ransfer learning for QoS aware t opology management in energy efﬁcient 5G cognitive radio networks,  in 31 Proc. 1st International Conference on 5G for Ubiquitous Con nectivity, Akaslompolo, Nov . 2014, pp. 152157. [204] B. Guo, J. Li, V .W . Zheng, Z. W ang, and Z. Y u, Citytrans fer: T ransferring inter- and intra-city knowledge for chain sto re site recommendation based on multi-source urban data, in Proc. ACM on Interactive, Mobile, Wearable and Ubiquitous T echnolog ies, Jan. 2018, pp. 123. [205] Y . W ei, Y . Zheng, and Q. Y ang, T ransfer knowledge betw een cities, in Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , San Francisco, Aug. 2016, pp. 19051914. [206] U. Cote-Allard, C.L. Fall, A. Drouin, A. Campeau-Leco urs, C. Gosselin, K. Glette, F . Laviolette, and B. Gosselin, Deep l earn- ing for electromyographic hand gesture signal classiﬁcati on using transfer learning, IEEE T rans. Neural Syst. Rehabil. Eng. , vol. 27, no. 4, pp. 760771, Apr . 2019. [207] C. Ren, D. Dai, K. Huang, and Z. Lai, T ransfer learning of structured representation for face recognition, IEEE T rans. Image Process., vol. 23, no. 12, pp. 54405454, Dec. 2014. [208] J. W ang, Y . Chen, L. Hu, X. Peng, and P .S. Y u, Stratiﬁed tr ans- fer learning for cross-domain activity recognition, in Proc. IEEE International Conference on Pervasive Computing and Commu nications, Athens, Mar . 2018, pp. 110. [209] J. Deng, Z. Zhang, E. Marchi, and B. Schuller , Sparse autoencoder-based feature transfer learning for speech em otion recognition, in Proc. Humaine Association Conference on Affective Computing and Intelligent Interaction , Geneva, Sep. 2013, pp. 511 516. [210] D. Xi, F . Zhuang, G. Zhou, X. Cheng, F . Lin, and Q. He, Do main adaptation with category attention network for deep sentim ent analysis, in Proc. The Web Conference , T aipei, Apr . 2020, pp. 3133 3139. [211] Y . Zhu, D. Xi, B. Song, F . Zhuang, S. Chen, X. Gu, and Q. He, Modeling users behavior sequences with hierarchical exp lainable network for cross-domain fraud detection, in Proc. The Web Con- ference, T aipei, Apr . 2020, pp. 928938. [212] J. T ang, T . Lou, J. Kleinberg, and S. Wu, T ransfer learn ing to infer social ties across heterogeneous networks, ACM T rans. Inf. Syst., vol. 34, no. 2, pp. 143, Apr . 2016. [213] L. Zhang, L. Zhang, D. T ao, and X. Huang, Sparse transfe r man- ifold embedding for hyperspectral target detection, IEEE T rans. Geosci. Remote Sensing , vol. 52, no. 2, pp. 10301043, Feb. 2014. [214] F . Zhuang, K. Duan, T . Guo, Y . Zhu, D. Xi, Z. Qi, and Q. He, T ransfer learning toolkit: Primers and benchmarks, 2 019, arXiv:1911.08967v1. [215] K. Saenko, B. Kulis, M. Fritz, and T . Darrell, Adapting visual category models to new domains, in Proc. 11th European Conference on Computer V ision , Heraklion, Sep. 2010, pp. 213226. [216] Z.C. Lipton, The mythos of model interpretability , ACM Que. , vol. 16, no. 3, May 2018, pp. 127.",
    "page_start": null,
    "page_end": null,
    "word_count": 31499,
    "created_at": "2025-08-18T07:03:25",
    "updated_at": "2025-08-18T07:03:25"
  },
  {
    "id": "9ebebd1e02064761beab9fff4c14547d",
    "doc_id": "f1cd85dd818c4d75a18788d6af0992d5",
    "doc_name": "A_Comprehensive_Survey_on_Transfer_Learning_1.pdf",
    "heading": "Document",
    "content": "arXiv:1911.02685v3 [cs.LG] 23 Jun 2020 1 A Comprehensive Survey on T ransfer Learning Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Y ongchun Zh u, Hengshu Zhu, Senior Member, IEEE, Hui Xiong, Fellow, IEEE, and Qing He AbstractT ransfer learning aims at improving the performance of tar get learners on target domains by transferring the knowledg e contained in different but related source domains. In this w ay , the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide applicati on prospects, transfer learning has become a popular and pro mising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent a dvances in transfer learning. Due to the rapid expansion of t he transfer learning area, it is both necessary and challenging to compr ehensively review the relevant studies. This survey attemp ts to connect and systematize the existing transfer learning researches , as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way , which may help rea ders have a better understanding of the current research sta tus and ideas. Unlike previous surveys, this survey paper reviews m ore than forty representative transfer learning approache s, especially homogeneous transfer learning approaches, from the perspe ctives of data and model. The applications of transfer learn ing are also brieﬂy introduced. In order to show the performance of diffe rent transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-2 1578, and Ofﬁce-31. And the experimental results demonstrate the imp ortance of selecting appropriate transfer learning models for different applications in practice. Index Terms T ransfer learning, machine learning, domain adaptation, interpretation.  1 I NTRODUCTION A LT H O UGH traditional machine learning technology has achieved great success and has been successfully ap- plied in many practical applications, it still has some limi ta- tions for certain real-world scenarios. The ideal scenario of machine learning is that there are abundant labeled trainin g instances, which have the same distribution as the test data . However , collecting sufﬁcient training data is often expen - sive, time-consuming, or even unrealistic in many scenario s. Semi-supervised learning can partly solve this problem by relaxing the need of mass labeled data. T ypically , a semi- supervised approach only requires a limited number of labeled data, and it utilizes a large amount of unlabeled data to improve the learning accuracy . But in many cases, unlabeled instances are also difﬁcult to collect, which usu - ally makes the resultant traditional models unsatisfactor y . T ransfer learning, which focuses on transferring the knowledge across domains, is a promising machine learning methodology for solving the above problem. The concept about transfer learning may initially come from educationa l psychology . According to the generalization theory of tran s- fer , as proposed by psychologist C.H. Judd, learning to transfer is the result of the generalization of experience. It is possible to realize the transfer from one situation to anoth er ,  Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Y ongchun Zh u, and Qing He are with the Key Laboratory of Intelligent Inform ation Processing of Chinese Academy of Sciences (CAS), Institute of Computing T echnology, CAS, Beijing 100190, China and the University o f Chinese Academy of Sciences, Beijing 100049, China.  Hengshu Zhu is with Baidu Inc., No. 10 Shangdi 10th Street, Ha idian District, Beijing, China.  Hui Xiong is with Rutgers, the State University of New Jersey , 1 Washington Park, Newark, New Jersey, USA.  Zhiyuan Qi is with the equal contribution to the ﬁrst author .  Fuzhen Zhuang and Zhiyuan Qi are corresponding authors, zhu ang- fuzhenict.ac.cn and qizhyuangmail.com. Fig. 1. Intuitive examples about transfer learning. as long as a person generalizes his experience. According to this theory , the prerequisite of transfer is that there needs to be a connection between two learning activities. In practice, a person who has learned the violin can learn the piano faster than others, since both the violin and the piano are musical instruments and may share some common knowledge. Fig. 1 shows some intuitive examples about transfer learning. Inspired by human beings capabilities to transfer knowledge across domains, transfer learning aims to leverage knowledge from a related domain (called source domain) to improve the learning performance or minimize the number of labeled examples required in a target domain. It is worth mentioning that the transferred knowledge does not always bring a positive impact on new tasks. If there is little in common between domains, knowledge transfer could be unsuccessful. For example, learning to ride a bicy- cle cannot help us learn to play the piano faster . Besides, th e similarities between domains do not always facilitate lear n- ing, because sometimes the similarities may be misleading. For example, although Spanish and French have a close re- lationship with each other and both belong to the Romance 2 group of languages, people who learn Spanish may experi- ence difﬁculties in learning French, such as using the wrong vocabulary or conjugation. This occurs because previous successful experience in Spanish can interfere with learni ng the word formation, usage, pronunciation, conjugation, et c., in French. In the ﬁeld of psychology , the phenomenon that previous experience has a negative effect on learning new tasks is called negative transfer [1]. Similarly , in the tra nsfer learning area, if the target learner is negatively affected by the transferred knowledge, the phenomenon is also termed as negative transfer [2], [3]. Whether negative transfer wi ll occur may depend on several factors, such as the relevance between the source and the target domains and the learners capacity of ﬁnding the transferable and beneﬁcial part of th e knowledge across domains. In [3], a formal deﬁnition and some analyses of negative transfer are given. Roughly speaking, according to the discrepancy between domains, transfer learning can be further divided into two categories, i.e., homogeneous and heterogeneous transfer learning [4]. Homogeneous transfer learning approaches ar e developed and proposed for handling the situations where the domains are of the same feature space. In homogeneous transfer learning, some studies assume that domains differ only in marginal distributions. Therefore, they adapt the d o- mains by correcting the sample selection bias [5] or covaria te shift [6]. However , this assumption does not hold in many cases. For example, in sentiment classiﬁcation problem, a word may have different meaning tendencies in different domains. This phenomenon is also called context feature bias [7]. T o solve this problem, some studies further adapt the conditional distributions. Heterogeneous transfer le arn- ing refers to the knowledge transfer process in the situatio ns where the domains have different feature spaces. In additio n to distribution adaptation, heterogeneous transfer learn ing requires feature space adaptation [7], which makes it more complicated than homogeneous transfer learning. The survey aims to give readers a comprehensive un- derstanding about transfer learning from the perspectives of data and model. The mechanisms and the strategies of transfer learning approaches are introduced to allow readers grasp how the approaches work. And a number of the existing transfer learning researches are connected an d systematized. Speciﬁcally , over forty representative tra nsfer learning approaches are introduced. Besides, we conduct experiments to demonstrate on which dataset a transfer learning model performs well. In this survey , we focus more on homogeneous transfer learning. Some interesting transfer learning topics are no t covered in this survey , such as reinforcement transfer lear n- ing [8], lifelong transfer learning [9], and online transfe r learning [10]. The rest of this survey are organized into seven sections. Section 2 clariﬁes the difference between transfer learning and other related machine learning tech- niques. Section 3 introduces the notations used in this sur- vey and the deﬁnitions about transfer learning. Sections 4 and 5 interpret transfer learning approaches from the data and the model perspectives, respectively . Section 6 intro- duces some applications of transfer learning. Experiments are conducted and the results are provided in Section 7. The last section concludes this survey . The main contributions of this survey are summarized below .  Over forty representative transfer learning approaches are introduced and summarized, which can give read- ers a comprehensive overview about transfer learning.  Experiments are conducted to compare different trans- fer learning approaches. The performance of over twenty different approaches is displayed intuitively and then analyzed, which may be instructive for the readers to select the appropriate ones in practice. 2 R ELATED WORK Some areas related to transfer learning are introduced. The connections and difference between them and transfer learn - ing are clariﬁed. Semi-Supervised Learning [11]: Semi-supervised learning is a machine learning task and method that lies between supervised learning (with completely labeled instances) and unsupervised learning (without any labeled instances) . T ypically , a semi-supervised method utilizes abundant un- labeled instances combined with a limited number of la- beled instances to train a learner . Semi-supervised learn- ing relaxes the dependence on labeled instances, and thus reduces the expensive labeling costs. Note that, in semi- supervised learning, both the labeled and unlabeled in- stances are drawn from the same distribution. In contrast, i n transfer learning, the data distributions of the source and the target domains are usually different. Many transfer learn- ing approaches absorb the technology of semi-supervised learning. The key assumptions in semi-supervised learning , i.e., smoothness, cluster , and manifold assumptions, are a lso made use of in transfer learning. It is worth mentioning that semi-supervised transfer learning is a controversial term. The reason is that the concept of whether the label information is available in transfer learning is ambiguous because both the source and the target domains can be involved. Multi-View Learning [12]: Multi-view learning focuses on the machine learning problems with multi-view data. A view represents a distinct feature set. An intuitive exampl e about multiple views is that a video object can be described from two different viewpoints, i.e., the image signal and the audio signal. Brieﬂy , multi-view learning describes an object from multiple views, which results in abundant in- formation. By properly considering the information from al l views, the learner s performance can be improved. There are several strategies adopted in multi-view learning such as subspace learning, multi-kernel learning, and co-train ing [13], [14]. Multi-view techniques are also adopted in some transfer learning approaches. For example, Zhang et al. pro- posed a multi-view transfer learning framework, which im- poses the consistency among multiple views [15]. Y ang and Gao incorporated multi-view information across different domains for knowledge transfer [16]. The work by Feuz and Cook introduces a multi-view transfer learning approach for activity learning, which transfers activity knowledge between heterogeneous sensor platforms [17]. Multi-T ask Learning [18]: The thought of multi-task learn- ing is to jointly learn a group of related tasks. T o be more speciﬁc, multi-task learning reinforces each task by taking advantage of the interconnections between task, i.e ., considering both the inter-task relevance and the inter-ta sk 3 difference. In this way , the generalization of each task is e n- hanced. The main difference between transfer learning and multi-task learning is that the former transfer the knowl- edge contained in the related domains, while the latter transfer the knowledge via simultaneously learning some related tasks. In other words, multi-task learning pays equ al attention to each task, while transfer learning pays more attention to the target task than to the source task. There ar e some commons and associations between transfer learning and multi-task learning. Both of them aim to improve the performance of learners via knowledge transfer . Besides, they adopt some similar strategies for constructing models , such as feature transformation and parameter sharing. Note that some existing studies utilize both the transfer learn- ing and the multi-task learning technologies. For example, the work by Zhang et al. employs multi-task and transfer learning techniques for biological image analysis [19]. Th e work by Liu et al. proposes a framework for human action recognition based on multi-task learning and multi-source transfer learning [20]. 3 O VERVIEW In this section, the notations used in this survey are listed for convenience. Besides, some deﬁnitions and categorization s about transfer learning are introduced, and some related surveys are also provided. 3.1 Notation For convenience, a list of symbols and their deﬁnitions are shown in T able 1. Besides, we use to represent the norm and superscript T to denote the transpose of a vectormatrix. 3.2 Deﬁnition In this subsection, some deﬁnitions about transfer learnin g are given. Before giving the deﬁnition of transfer learning , let us review the deﬁnitions of a domain and a task. Deﬁnition 1. (Domain) A domain Dis composed of two parts, i.e., a feature space Xand a marginal distribution P (X). In other words, D {X, P (X)}. And the symbol X denotes an instance set, which is deﬁned as X  {xxi X , i  1,  , n }. Deﬁnition 2. (T ask) A task T consists of a label space Yand a decision function f, i.e., T  {Y, f }. The decision function f is an implicit one, which is expected to be learned from the sample data. Some machine learning models actually output the pre- dicted conditional distributions of instances. In this cas e, f(xj )  {P (ykxj )yk Y, k  1 ,  , Y}. In practice, a domain is often observed by a number of instances withwithout the label information. For ex- ample, a source domain DS corresponding to a source task TS is usually observed via the instance-label pairs, i.e., DS  {(x, y )xi  XS , y i  YS, i  1 ,  , n S}; an observation of the target domain usually consists of a number of unlabeled instances andor limited number of labeled instances. Deﬁnition 3. (T ransfer Learning) Given somean observation(s) corresponding to mS  N source domain(s) and task(s) T ABLE 1 Notations. Symbol Deﬁnition n Number of instances m Number of domains D Domain T T ask X Feature space Y Label space x Feature vector y Label X Instance set Y Label set corresponding to X S Source domain T T arget domain L Labeled instances U Unlabeled instances H Reproducing kernel Hilbert space θ MappingCoefﬁcient vector α W eighting coefﬁcient β W eighting coefﬁcient λ T radeoff parameter δ ParameterError b Bias B Boundary parameter N IterationKernel number f Decision function L Loss function η Scale parameter G Graph Φ Nonlinear mapping σ Monotonically increasing function Ω Structural risk κ Kernel function K Kernel matrix H Centering matrix C Covariance matrix d Document w W ord z Class variable z Noise D Discriminator G Generator S Function M Orthonormal bases Θ Model parameters P Probability E Expectation Q Matrix variable R Matrix variable W Mapping matrix (i.e., {(DSi , TSi )i  1 ,  , m S}), and somean observa- tion(s) about mT  N target domain(s) and task(s) (i.e., {(DTj , TTj )j  1 ,  , m T }), transfer learning utilizes the knowledge implied in the source domain(s) to improve the performance of the learned decision functions fTj (j  1,  , m T ) on the target domain(s). The above deﬁnition, which covers the situation of multi- source transfer learning, is an extension of the one present ed in the survey [2]. If mS equals 1, the scenario is called single- source transfer learning. Otherwise, it is called multi-so urce transfer learning. Besides, mT represents the number of the transfer learning tasks. A few studies focus on the setting that mT 2 [21]. The existing transfer learning studies pay more attention to the scenarios where mT  1 (especially where mS  mT  1 ). It is worth mentioning that the observation of a domain or a task is a concept with broad 4 Transfer Learning Problem Categorization Solution Categorization Homogeneous Transfer Learning Heterogeneous Transfer Learning Inductive Transfer Learning Transductive Transfer Learning Unsupervised Transfer Learning Instance-Based Approach Feature-Based Approach Parameter-Based Approach Relational-Based Approach Symmetric Transformation Asymmetric Transformation Label-Setting-Based Categorization Space-Setting-Based Categorization Fig. 2. Categorizations of transfer learning. sense, which is often cemented into a labeledunlabeled instance set or a pre-learned model. A common scenario is that we have abundant labeled instances or have a well- trained model on the source domain, and we only have limited labeled target-domain instances. In this case, the resources such as the instances and the model are actually the observations, and the goal of transfer learning is to lea rn a more accurate decision function on the target domain. Another term commonly used in the transfer learning area is domain adaptation. Domain adaptation refers to the process that adapting one or more source domains to trans- fer knowledge and improve the performance of the target learner [4]. T ransfer learning often relies on the domain adaptation process, which attempts to reduce the differenc e between domains. 3.3 Categorization of T ransfer Learning There are several categorization criteria of transfer lear ning. For example, transfer learning problems can be divided into three categories, i.e., transductive, inductive, and un- supervised transfer learning [2]. The complete deﬁnitions of these three categories are presented in [2]. These three categories can be interpreted from a label-setting aspect. Roughly speaking, transductive transfer learning refers t o the situations where the label information only comes from the source domain. If the label information of the target- domain instances is available, the scenario can be catego- rized into inductive transfer learning. If the label inform a- tion is unknown for both the source and the target domains, the situation is known as unsupervised transfer learning. Another categorization is based on the consistency between the source and the target feature spaces and label spaces. If XS  XT and YS  YT , the scenario is termed as homogeneous transfer learning. Otherwise, if XS  XT orand YS  YT , the scenario is referred to as heteroge- neous transfer learning. According to the survey [2], the transfer learning ap- proaches can be categorized into four groups: instance- based, feature-based, parameter-based, and relational-b ased approaches. Instance-based transfer learning approaches are mainly based on the instance weighting strategy . Feature- based approaches transform the original features to create a new feature representation; they can be further divided int o two subcategories, i.e., asymmetric and symmetric feature - based transfer learning. Asymmetric approaches transform the source features to match the target ones. In contrast, symmetric approaches attempt to ﬁnd a common latent feature space and then transform both the source and the target features into a new feature representation. The parameter-based transfer learning approaches transfer th e knowledge at the modelparameter level. Relational-based transfer learning approaches mainly focus on the problems in relational domains. Such approaches transfer the logica l relationship or rules learned in the source domain to the target domain. For better understanding, Fig. 2 shows the above-mentioned categorizations of transfer learning. Some surveys are provided for the readers who want to have a more complete understanding of this ﬁeld. The survey by Pan and Y ang [2], which is a pioneering work, cat- egorizes transfer learning and reviews the research progre ss before 2010. The survey by W eiss et al. introduces and summarizes a number of homogeneous and heterogeneous transfer learning approaches [4]. Heterogeneous transfer learning is specially reviewed in the survey by Day and Khoshgoftaar [7]. Some surveys review the literatures re- lated to speciﬁc themes such as reinforcement learning [8], computational intelligence [22], and deep learning [23], [ 24]. Besides, a number of surveys focus on speciﬁc application scenarios including activity recognition [25], visual cat ego- rization [26], collaborative recommendation [27], comput er vision [24], and sentiment analysis [28]. Note that the organization of this survey does not strictly follow the above-mentioned categorizations. In the next two sections, transfer learning approaches are interprete d from the data and the model perspectives. Roughly speak- ing, data-based interpretation covers the above-mentione d instance-based and feature-based transfer learning ap- proaches, but from a broader perspective. Model-based interpretation covers the above-mentioned parameter-bas ed approaches. Since there are relatively few studies concern - ing relational-based transfer learning and the representa tive approaches are well introduced in [2], [4], this survey does not focus on relational-based approaches. 5 Covariance ... Geometric Structure Cluster Structure ... Data-Based Interpretation Objective Measurement Type Distribution Adaptation Data Property PreservationAdjustment Marginal Distribution Adaptation Conditional Distribution Adaptation Kullback-Leibler Divergence Maximum Mean Discrepancy Jensen-Shannon Divergence ... Statistical Property Strategy Instance Weighting Feature Transformation Feature Clustering Feature Alignment Feature Augmentation Feature Reduction Joint Distribution Adaptation Feature Replication ... Feature Encoding Bregman Divergence Feature Stacking Mean Manifold Structure ... Feature Mapping Estimation Method ... Heuristic Method Space Adaptation Feature Space Adaptation Label Space Adaptation Spectral Feature Alignment Subspace Feature Alignment ... ... Statistic Feature Alignment Feature Selection Fig. 3. Strategies and the objectives of the transfer learni ng approaches from the data perspective. 4 D ATA-BASED INTERPRETAT ION Many transfer learning approaches, especially the data- based approaches, focus on transferring the knowledge via the adjustment and the transformation of data. Fig. 3 shows the strategies and the objectives of the approaches from the data perspective. As shown in Fig. 3, space adaptation is one of the objectives. This objective is required to be sat- isﬁed mostly in heterogeneous transfer learning scenarios . In this survey , we focus more on homogeneous transfer learning, and the main objective in this scenario is to reduc e the distribution difference between the source-domain and the target-domain instances. Furthermore, some advanced approaches may attempt to preserve the data properties in the adaptation process. There are generally two strategies for realizing the objectives from the data perspective, i.e ., in- stance weighting and feature transformation. In this secti on, some related transfer learning approaches are introduced i n proper order according to the strategies shown in Fig. 3. 4.1 Instance Weighting Strategy Let us ﬁrst consider a simple scenario in which a large number of labeled source-domain and a limited number of target-domain instances are available and domains diffe r only in marginal distributions (i.e., P S(X)  P T (X) and P S(Y X)  P T (Y X)). For example, suppose we need to build a model to diagnose cancer in a speciﬁc region where the elderly are the majority . Limited target-domain instances are given, and relevant data are available from another region where young people are the majority . Di- rectly transferring all the data from another region may be unsuccessful, since the marginal distribution differen ce exists, and the elderly have a higher risk of cancer than younger people. In this scenario, it is natural to consider adapting the marginal distributions. A simple idea is to assign weights to the source-domain instances in the loss function. The weighting strategy is based on the following equation [5]: E(x,y )P T [L(x, y ; f)]  E(x,y )P S [ P T (x, y ) P S(x, y ) L(x, y ; f) ]  E(x,y )P S [ P T (x) P S(x) L(x, y ; f) ] . Therefore, the general objective function of a learning tas k can be written as [5]: min f 1 nS nS  i1 βiL ( f(xS i ), y S i )  Ω( f), where βi (i  1 , 2,  , n S) is the weighting parameter . The theoretical value of βi is equal to P T (xi)P S(xi). However , this ratio is generally unknown and is difﬁcult to be obtained by using the traditional methods. Kernel Mean Matching (KMM) [5], which is proposed by Huang et al. , resolves the estimation problem of the above unknown ratios by matching the means between the source- domain and the target-domain instances in a Reproducing Kernel Hilbert Space (RKHS), i.e., arg min β i[0,B ]             1 nS nS  i1 βiΦ( xS i )  1 nT nT  j1 Φ( xT j )             2 H s.t. 1 nS nS  i1 βi 1 δ, where δ is a small parameter , and B is a parameter for con- straint. The above optimization problem can be converted 6 into a quadratic programming problem by expanding and using the kernel trick. This approach to estimating the rati os of distributions can be easily incorporated into many exist - ing algorithms. Once the weight βi is obtained, a learner can be trained on the weighted source-domain instances. There are some other studies attempting to estimate the weights. For example, Sugiyama et al. proposed an approach termed Kullback-Leibler Importance Estimation Procedure (KLIEP) [6]. KLIEP depends on the minimization of the Kullback-Leibler (KL) divergence and incorporates a built-in model selection procedure. Based on the studies of weight estimation, some instance-based transfer learni ng frameworks or algorithms are proposed. For example, Sun et al. proposed a multi-source framework termed 2-Stage W eighting Framework for Multi-Source Domain Adaptation (2SW-MDA) with the following two stages [29]. 1. Instance Weighting : The source-domain instances are as- signed with weights to reduce the marginal distribution difference, which is similar to KMM. 2. Domain Weighting : W eights are assigned to each source domain for reducing the conditional distribution differ- ence based on the smoothness assumption [30]. Then, the source-domain instances are reweighted based on the instance weights and the domain weights. These reweighted instances and the labeled target-domain in- stances are used to train the target classiﬁer . In addition to directly estimating the weighting param- eters, adjusting weights iteratively is also effective. Th e key is to design a mechanism to decrease the weights of the instances that have negative effects on the target learner . A representative work is T rAdaBoost [31], which is a framework proposed by Dai et al . This framework is an extension of AdaBoost [32]. AdaBoost is an effective boosting algorithm designed for traditional machine learn - ing tasks. In each iteration of AdaBoost, a learner is traine d on the instances with updated weights, which results in a weak classiﬁer . The weighting mechanism of instances ensures that the instances with incorrect classiﬁcation ar e given more attention. Finally , the resultant weak classiﬁe rs are combined to form a strong classiﬁer . T rAdaBoost ex- tends the AdaBoost to the transfer learning scenario; a new weighting mechanism is designed to reduce the impact of the distribution difference. Speciﬁcally , in T rAdaBoost, the labeled source-domain and labeled target-domain instance s are combined as a whole, i.e., a training set, to train the weak classiﬁer . The weighting operations are different for the source-domain and the target-domain instances. In each iteration, a temporary variable δ, which measures the classi- ﬁcation error rate on the labeled target-domain instances, is calculated. Then, the weights of the target-domain instanc es are updated based on δ and the individual classiﬁcation results, while the weights of the source-domain instances a re updated based on a designed constant and the individual classiﬁcation results. For better understanding, the form ulas used in the k-th iteration ( k  1 ,  , N ) for updating the weights are presented repeatedly as follows [31]: βS k,i  βS k1,i (1   2 ln nSN )fk (xS i )yS i (i  1 ,  , n S), βT k,j  βT k1,j (δk (1 δk))fk (xT j )yT j (j  1 ,  , n T ). Note that each iteration forms a new weak classiﬁer . The ﬁnal classiﬁer is constructed by combining and ensembling half the number of the newly resultant weak classiﬁers through voting scheme. Some studies further extend T rAdaBoost. The work by Y ao and Doretto [33] proposes a Multi-Source T rAdaBoost (MsT rAdaBoost) algorithm, which mainly has the following two steps in each iteration. 1. Candidate Classiﬁer Construction : A group of candi- date weak classiﬁers are respectively trained on the weighted instances in the pairs of each source domain and the target domain, i.e., DSi DT (i  1 ,  , m S). 2. Instance Weighting : A classiﬁer (denoted by j and trained on DSj DT ) which has the minimal classi- ﬁcation error rate δ on the target domain instances is selected, and then is used for updating the weights of the instances in DSj and DT . Finally , the selected classiﬁers from each iteration are co m- bined to form the ﬁnal classiﬁer . Another parameter-based algorithm, i.e., T askT rAdaBoost, is also proposed in the work [33], which is introduced in Section 5.3. Some approaches realize instance weighting strategy in a heuristic way . For example, Jiang and Zhai proposed a general weighting framework [34]. There are three terms in the frameworks objective function, which are designed to minimize the cross-entropy loss of three types of instances . The following types of instances are used to construct the target classiﬁer .  Labeled T arget-domain Instance : The classiﬁer should mini- mize the cross-entropy loss on them, which is actually a standard supervised learning task.  Unlabeled T arget-domain Instance : These instances true con- ditional distributions P (yxT,U i ) are unknown and should be estimated. A possible solution is to train an auxiliary classiﬁer on the labeled source-domain and target-domain instances to help estimate the conditional distributions o r assign pseudo labels to these instances.  Labeled Source-domain Instance : The authors deﬁne the weight of xS,L i as the product of two parts, i.e., α i and βi. The weight βi is ideally equal to P T (xi)P S(xi), which can be estimated by non-parametric methods such as KMM or can be set uniformly in the worst case. The weight α i is used to ﬁlter out the source-domain instances that differ greatly from the target domain. A heuristic method can be used to produce the value of α i, which contains the following three steps. 1. Auxiliary Classiﬁer Construction : An auxiliary classiﬁer trained on the labeled target-domain instances are used to classify the unlabeled source-domain instances. 2. Instance Ranking : The source-domain instances are ranked based on the probabilistic prediction results. 3. Heuristic Weighting ( βi): The weights of the top- k source-domain instances with wrong predictions are set to zero, and the weights of others are set to one. 4.2 Feature T ransformation Strategy Feature transformation strategy is often adopted in featur e- based approaches. For example, consider a cross-domain text classiﬁcation problem. The task is to construct a targe t 7 T ABLE 2 Metrics Adopted in T ransfer Learning. Measurement Related Algorithms Maximum Mean Discrepancy [35] [36] [37] [38] [39]    Kullback-Leibler Divergence [40] [41] [42] [43] [44]    Jensen-Shannon Divergence [45] [46] [47] [48] [49]    Bregman Divergence [50] [51] [52] [53] [54]    Hilbert-Schmidt Independence Criterion [55] [36] [56] [57] [58]   classiﬁer by using the labeled text data from a related domain. In this scenario, a feasible solution is to ﬁnd the common latent features (e.g., latent topics) through fea- ture transformation and use them as a bridge to transfer knowledge. Feature-based approaches transform each orig- inal feature into a new feature representation for knowl- edge transfer . The objectives of constructing a new feature representation include minimizing the marginal and the conditional distribution difference, preserving the prop er- ties or the potential structures of the data, and ﬁnding the correspondence between features. The operations of featur e transformation can be divided into three types, i.e., featu re augmentation, feature reduction, and feature alignment. B e- sides, feature reduction can be further divided into severa l types such as feature mapping, feature clustering, feature selection, and feature encoding. A complete feature trans- formation process designed in an algorithm may consist of several operations. 4.2.1 Distribution Difference Metric One primary objective of feature transformation is to reduc e the distribution difference of the source and the target do- main instances. Therefore, how to measure the distribution difference or the similarity between domains effectively i s an important issue. The measurement termed Maximum Mean Discrepancy (MMD) is widely used in the ﬁeld of transfer learning, which is formulated as follows [35]: MMD(XS, X T )              1 nS nS  i1 Φ( xS i )  1 nT nT  j1 Φ( xT j )             2 H . MMD can be easily computed by using kernel trick. Brieﬂy , MMD quantiﬁes the distribution difference by calculating the distance of the mean values of the instances in a RKHS. Note that the above-mentioned KMM actually produces the weights of instances by minimizing the MMD distance between domains. T able. 2 lists some commonly used metrics and the related algorithms. In addition to T able. 2, there are some other measurement criteria adopted in transfer learning, including W asserstein distance [59], [60], central moment discrepancy [61], etc. Some studies focus on optimizing and improving the existing measurements. T ake MMD as an example. Gretton et al. proposed a multi-kernel version of MMD, i.e., MK-MMD [62], which takes advantage of multiple kernels. Besides, Y an et al. proposed a weighted version of MMD [63], which attempts to address the issue of class weight bias. 4.2.2 Feature Augmentation Feature augmentation operations are widely used in fea- ture transformation, especially in symmetric feature-bas ed approaches. T o be more speciﬁc, there are several ways to realize feature augmentation such as feature replication a nd feature stacking. For better understanding, we start with a simple transfer learning approach which is established based on feature replication. The work by Daum  e proposes a simple domain adap- tation method, i.e., Feature Augmentation Method (F AM) [64]. This method transforms the original features by sim- ple feature replication. Speciﬁcally , in single-source tr ansfer learning scenario, the feature space is augmented to three times its original size. The new feature representation con - sists of general features, source-speciﬁc features, and ta rget- speciﬁc features. Note that, for the transformed source- domain instances, their target-speciﬁc features are set to zero. Similarly , for the transformed target-domain instan ces, their source-speciﬁc features are set to zero. The new featu re representation of F AM is presented as follows: Φ S(xS i )  xS i , xS i , 0, Φ T (xT j )  xT j , 0, xT j , where Φ S and Φ T denote the mappings to the new feature space from the source and the target domain, respectively . The ﬁnal classiﬁer is trained on the transformed labeled instances. It is worth mentioning that this augmentation method is actually redundant. In other words, augmenting the feature space in other ways (with fewer dimensions) may be able to produce competent performance. The supe- riority of F AM is that its feature expansion has an elegant form, which results in some good properties such as the generalization to multi-source scenarios. An extension of F AM is proposed in [65] by Daum  e et al. , which utilizes the unlabeled instances to further facilitate the knowledg e transfer process. However , F AM may not work well in handling het- erogeneous transfer learning tasks. The reason is that di- rectly replicating features and padding zero vectors are le ss effective when the source and the target domains have different feature representations. T o solve this problem, Li et al. proposed an approach termed Heterogeneous Feature Augmentation (HF A) [66], [67]. The feature representation of HF A is presented below: Φ S(xS i )  W SxS i , xS i , 0T , Φ T (xT j )  W T xT j , 0S, xT j , where W SxS i and W T xT j have the same dimension; 0S and 0T denote the zero vectors with the dimensions of xS and xT , respectively . HF A maps the original features into a common feature space, and then performs a feature stacking operation. The mapped features, original features, and zer o elements are stacked in a particular order to produce a new feature representation. 4.2.3 Feature Mapping In the ﬁeld of traditional machine learning, there are many feasible mapping-based methods of extracting fea- tures such as Principal Component Analysis (PCA) [68] and Kernelized-PCA (KPCA) [69]. However , these methods mainly focus on the data variance rather than the distribu- tion difference. In order to solve the distribution differe nce, 8 some feature extraction methods are proposed for transfer learning. Let us ﬁrst consider a simple scenario where there is little difference in the conditional distributions of th e do- mains. In this case, the following simple objective functio n can be used to ﬁnd a mapping for feature extraction: min Φ ( DIST(XS, X T ; Φ)  λΩ(Φ) )  ( V AR(XS XT ; Φ) ) , where Φ is a low-dimensional mapping function, DIST () represents a distribution difference metric, Ω(Φ) is a regular- izer controlling the complexity of Φ , and V AR () represents the variance of instances. This objective function aims to ﬁnd a mapping function Φ that minimizes the marginal distribution difference between domains and meanwhile makes the variance of the instances as large as possible. The objective corresponding to the denominator can be opti- mized in several ways. One possible way is to optimize the objective of the numerator with a variance constraint. For example, the scatter matrix of the mapped instances can be enforced as an identity matrix. Another way is to optimize the objective of the numerator in a high-dimensional featur e space at ﬁrst. Then, a dimension reduction algorithm such as PCA or KPCA can be performed to realize the objective of the denominator . Further , ﬁnding the explicit formulation of Φ( ) is non- trivial. T o solve this problem, some approaches adopt linea r mapping technique or turn to the kernel trick. In general, there are three main ideas to deal with the above optimiza- tion problems.  (Mapping Learning  Feature Extraction) A possible way is to ﬁnd a high-dimensional space at ﬁrst where the objec- tives are met by solving a kernel matrix learning problem or a transformation matrix ﬁnding problem. Then, the high-dimensional features are compacted to form a low- dimensional feature representation. For example, once the kernel matrix is learned, the principal components of the implicit high-dimensional features can be extracted to construct a new feature representation based on PCA.  (Mapping Construction  Mapping Learning) Another way is to map the original features to a constructed high- dimensional feature space, and then a low-dimensional mapping is learned to satisfy the objective function. For example, a kernel matrix can be constructed based on a selected kernel function at ﬁrst. Then, the transfor- mation matrix can be learned, which projects the high- dimensional features into a common latent subspace.  (Direct Low-dimensional Mapping Learning) It is usu- ally difﬁcult to ﬁnd a desired low-dimensional mapping directly . However , if the mapping is assumed to satisfy certain conditions, it may be solvable. For example, if the low-dimensional mapping is restricted to be a linear one, the optimization problem can be easily solved. Some approaches also attempt to match the conditional distributions and preserve the structures of the data. T o achieve this, the above simple objective function needs to incorporate new terms orand constraints. For example, the following general objective function is a possible choice: min Φ µDIST(XS, X T ; Φ)  λ1Ω GEO (Φ)  λ2Ω(Φ)  (1 µ)DIST(Y SXS, Y T XT ; Φ) , s.t. Φ( X)T HΦ( X)  I, with H  I (1n ) Rnn, where µ is a parameter balancing the marginal and the conditional distribution difference [70], Ω GEO(Φ) is a reg- ularizer controlling the geometric structure, Φ( X) is the matrix whose rows are the instances from both the source and the target domains with the extracted new feature representation, H is the centering matrix for constructing the scatter matrix, and the constraint is used to maximize the variance. The last term in the objective function denote s the measurement of the conditional distribution differenc e. Before the further discussion about the above objective function, it is worth mentioning that the label information of the target-domain instances is often limited or even unknown. The lack of the label information makes it difﬁcult to estimate the distribution difference. In order to solve this problem, some approaches resort to the pseudo-label strategy , i.e., assigning pseudo labels to the unlabeled ta rget- domain instances. A simple method of realizing this is to train a base classiﬁer to assign pseudo labels. By the way , there are some other methods of providing pseudo labels such as co-training [71], [72] and tri-training [73] , [74]. Once the pseudo-label information is complemented, the conditional distribution difference can be measured. F or example, MMD can be modiﬁed and extended to measure the conditional distribution difference. Speciﬁcally , fo r each label, the source-domain and the target-domain instances that belong to the same class are collected, and the estima- tion expression of the conditional distribution differenc e is given by [38]: Y k1             1 nS k nS k i1 Φ( xS i )  1 nT k nT k j1 Φ( xT j )             2 H , where nS k and nT k denote the numbers of the instances in the source and the target domains with the same label Yk, respectively . This estimation actually measures the class - conditional distribution (i.e., P (xy)) difference to approx- imate the conditional distribution (i.e., P (yx)) difference. Some studies improve the above estimation. For example, the work by W ang et al. uses a weighted method to ad- ditionally solve the class imbalance problem [70]. For bett er understanding, the transfer learning approaches that are t he special cases of the general objective function presented i n the previous paragraph are detailed as follows.  (µ  1 and λ1  0 ) The objective function of Maximum Mean Discrepancy Embedding (MMDE) is given by [75]: min K MMD(XS, X T ; Φ)  λ1 nS  nT  ij Φ( xi) Φ( xj )2 s.t. (xi k-NN(xj )) (xj k-NN(xi)), Φ( xi) Φ( xj )2  xi xj 2, (xi, xj XS XT ), where k-NN(x) represents the k nearest neighbors of the instance x. The authors design the above objective func- tion motivated by Maximum V ariance Unfolding (MVU) [76]. Instead of employing a scatter matrix constraint, the constraints and the second term of this objective function aim to maximize the distance between instances as well as preserve local geometry . The desired kernel matrix K can be learned by solving a Semi-Deﬁnite Programming (SDP) [77] problem. After obtaining the kernel matrix, 9 PCA is applied to it, and then the leading eigenvectors are selected to help construct a low-dimensional feature representation.  (µ  1 and λ1  0 ) The work by Pan et al. proposes an approach termed T ransfer Component Analysis (TCA) [36], [78]. TCA adopts MMD to measure the marginal dis- tribution difference and enforces the scatter matrix as the constraint. Different from MMDE that learns the kernel matrix and then further adopts PCA, TCA is a uniﬁed method that just needs to learn a linear mapping from an empirical kernel feature space to a low-dimensional fea- ture space. In this way , it avoids solving the SDP problem, which results in relatively low computational burden. The ﬁnal optimization problem can be easily solved via eigen- decomposition. TCA can also be extended to utilize the label information. In the extended version, the scatter matrix constraint is replaced by a new one that balances the label dependence (measured by HSIC) and the data variance. Besides, a graph Laplacian regularizer [30] is also added to preserve the geometry of the manifold. Sim- ilarly , the ﬁnal optimization problem can also be solved by eigen-decomposition.  (µ  0 . 5 and λ1  0 ) Long et al. proposed an ap- proach termed Joint Distribution Adaptation (JDA) [38]. JDA attempts to ﬁnd a transformation matrix that maps the instances to a low-dimensional space where both the marginal and the conditional distribution difference are minimized. T o realize it, the MMD metric and the pseudo- label strategy are adopted. The desired transformation matrix can be obtained by solving a trace optimization problem via eigen-decomposition. Further , it is obvious that the accuracy of the estimated pseudo labels affects the performance of JDA. In order to improve the labeling quality , the authors adopt the iterative reﬁnement oper- ations. Speciﬁcally , in each iteration, JDA is performed, and then a classiﬁer is trained on the instances with the extracted features. Next, the pseudo labels are updated based on the trained classiﬁer . After that, JDA is per- formed repeatedly with the updated pseudo labels. The iteration ends when convergence occurs. Note that JDA can be extended by utilizing the label and structure infor- mation [79], clustering information [80], various statist ical and geometrical information [81], etc.  (µ (0, 1) and λ1  0 ) The paper by W ang et al. proposes an approach termed Balanced Distribution Adaptation (BDA) [70], which is an extension of JDA. Different from JDA which assumes that the marginal and the conditional distributions have the same importance in adaptation, BDA attempts to balance the marginal and the condi- tional distribution adaptation. The operations of BDA are similar to JDA. In addition, the authors also proposed the W eighted BDA (WBDA). In WBDA, the conditional distribution difference is measured by a weighted version of MMD to solve the class imbalance problem. It is worth mentioning that some approaches transform the features into a new feature space (usually of a high dimension) and train an adaptive classiﬁer simultaneously . T o realize this, the mapping function of the features and the decision function of the classiﬁer need to be associated. On e possible way is to deﬁne the following decision function: f(x)  θ Φ( x)b, where θ denotes the classiﬁer parameter; b denotes the bias. In light of the representer theorem [82], the parameter θ can be deﬁned as θ   n i1 α iΦ( xi), and thus we have f(x)  n i1 α iΦ( xi) Φ( x)  b  n i1 α iκ(xi, x)  b, where κ denotes the kernel function. By using the kernel matrix as the bridge, the regularizers designed for the map- ping function can be incorporated into the classiﬁer s obje c- tive function. In this way , the ﬁnal optimization problem is usually about the parameter (e.g., α i) or the kernel function. For example, the paper by Long et al. proposes a general framework termed Adaptation Regularization Based T rans- fer Learning (ARTL) [39]. The goals of ARTL are to learn the adaptive classiﬁer , to minimize the structural risk, to jointly reduce the marginal and the conditional distributi on difference, and to maximize the manifold consistency be- tween the data structure and the predictive structure. The authors also proposed two speciﬁc algorithms under this framework based on different loss functions. In these two algorithms, the coefﬁcient matrix for computing MMD and the graph Laplacian matrix for manifold regularization are constructed at ﬁrst. Then, a kernel function is selected to construct the kernel matrix. After that, the classiﬁer lear ning problem is converted into a parameter (i.e., α i) solving problem, and the solution formula is also given in [39]. In ARTL, the choice of the kernel function affects the performance of the ﬁnal classiﬁer . In order to construct a robust classiﬁer , some studies turn to kernel learning. For example, the paper by Duan et al. proposes a uni- ﬁed framework termed Domain T ransfer Multiple Kernel Learning (DTMKL) [83]. In DTMKL, the kernel function is assumed to be a linear combination of a group of base kernels, i.e., κ(xi, xj )   N k1 βkκk(xi, xj ). DTMKL aims to minimize the distribution difference, the classiﬁcatio n error , etc., simultaneously . The general objective functi on of DTMKL can be written as follows: min β k,f σ ( MMD(XS, X T ; κ) )  λΩ L(βk, f ), where σ is any monotonically increasing function, f is the decision function with the same deﬁnition as the one in ARTL, and Ω L(βk, f ) is a general term representing a group of regularizers deﬁned on the labeled instances such as the ones for minimizing the classiﬁcation error and controllin g the complexity of the resultant model. The authors devel- oped an algorithm to learn the kernel and the decision function simultaneously by using the reduced gradient de- scent method [84]. In each iteration, the weight coefﬁcient s of base kernels are ﬁxed to update the decision function at ﬁrst. Then, the decision function is ﬁxed to update the weight coefﬁcients. Note that DTMKL can incorporate many existing kernel methods. The authors proposed two speciﬁc algorithms under this framework. The ﬁrst one implements the framework by using hinge loss and Support V ector Machine (SVM). The second one is an extension of the ﬁrst one with an additional regularizer utilizing pseudo- label information, and the pseudo labels of the unlabeled instances are generated by using base classiﬁers. 10 4.2.4 Feature Clustering Feature clustering aims to ﬁnd a more abstract feature representation of the original features. Although it can be regarded as a way of feature extraction, it is different from the above-mentioned mapping-based extraction. For example, some transfer learning approaches implic- itly reduce the features by using the co-clustering tech- nique, i.e., simultaneously clustering both the columns an d rows of (or say , co-cluster) a contingency table based on the information theory [85]. The paper by Dai et al. [41] proposes an algorithm termed Co-Clustering Based Clas- siﬁcation (CoCC), which is used for document classiﬁca- tion. In a document classiﬁcation problem, the transfer learning task is to classify the target-domain documents (represented by a document-to-word matrix) with the help of the labeled source document-to-word data. CoCC re- gards the co-clustering technique as a bridge to transfer the knowledge. In CoCC algorithm, both the source and the target document-to-word matrices are co-clustered. Th e source document-to-word matrix is co-clustered to generat e the word clusters based on the known label information, and these word clusters are used as constraints during the co-clustering process of the target-domain data. The co- clustering criterion is to minimize the loss in mutual infor - mation, and the clustering results are obtained by iteratio n. Each iteration contains the following two steps. 1. Document Clustering : Each row of the target document- to-word matrix is re-ordered based on the objective function for updating the document clusters. 2. Word Clustering : The word clusters are adjusted to min- imize the joint mutual-information loss of the source and the target document-word matrices. After several times of iterations, the algorithm converges , and the classiﬁcation results are obtained. Note that, in CoCC, the word clustering process implicitly extracts the word features to form uniﬁed word clusters. Dai et al. also proposed an unsupervised clustering ap- proach, which is termed as Self-T aught Clustering (STC) [42]. Similar to CoCC, this algorithm is also a co-clusterin g- based one. However , STC does not need the label infor- mation. STC aims to simultaneously co-cluster the source- domain and the target-domain instances with the assump- tion that these two domains share the same feature clusters in their common feature space. Therefore, two co-clusterin g tasks are separately performed at the same time to ﬁnd the shared feature clusters. Each iteration of STC has the following steps. 1. Instance Clustering : The clustering results of the source- domain and the target domain instances are updated to minimize their respective loss in mutual information. 2. Feature Clustering : The feature clusters are updated to minimize the joint loss in mutual information. When the algorithm converges, the clustering results of the target-domain instances are obtained. Different from the above-mentioned co-clustering-based ones, some approaches extract the original features into co n- cepts (or topics). In the document classiﬁcation problem, t he concepts represent the high-level abstractness of the word s (e.g., word clusters). In order to introduce the concept-ba sed transfer learning approaches easily , let us brieﬂy review t he Latent Semantic Analysis (LSA) [86], the Probabilistic LSA (PLSA) [87], and the Dual-PLSA [88].  LSA: LSA is an approach to mapping the document-to- word matrix to a low-dimensional space (i.e., a latent se- mantic space) based on the SVD technique. In short, LSA attempts to ﬁnd the true meanings of the words. T o realize this, SVD technique is used to reduce the dimensionality , which can remove the irrelevant information and ﬁlter out the noise information from the raw data.  PLSA: PLSA is developed based on a statistical view of LSA. PLSA assumes that there is a latent class variable z, which reﬂects the concept, associating the document d and the word w. Besides, d and w are independently con- ditioned on the concept z. The diagram of this graphical model is presented as follows: d P (dizk)  P (zk )  z P (wj zk) w, where the subscripts i, j and k represent the indexes of the document, the word, and the concept, respectively . PLSA constructs a Bayesian network, and the parameters are estimated by using the Expectation-Maximization (EM) algorithm [89].  Dual-PLSA: The Dual-PLSA is an extension of PLSA. This approach assumes there are two latent variables zd and zw associating the documents and the words. Speciﬁcally , the variables zd and zw reﬂect the concepts behind the documents and the words, respectively . The diagram of the Dual-PLSA is provided below: d P (dizd k1 ) zd P (zd k1 ,z w k2 ) zw P (wj zw k2 ) w. The parameters of the Dual-PLSA can also be obtained based on the EM algorithm. Some concept-based transfer learning approaches are established based on PLSA. For example, the paper by Xue et al. proposes a cross-domain text classiﬁcation approach termed T opic-Bridged Probabilistic Latent Semantic Analy - sis (TPLSA) [90]. TPLSA, which is an extension of PLSA, assumes that the source-domain and the target-domain instances share the same mixing concepts of the words. Instead of performing two PLSAs for the source domain and the target domain separately , the authors merge those two PLSAs as an integrated one by using the mixing concept z as a bridge, i.e., each concept has some probabilities to produ ce the source-domain and the target-domain documents. The diagram of TPLSA is provided below:    dS dT տ ւ P (dS i zk)        P (dT i zk) z P (zkwj ) w. Note that PLSA does not require the label information. In order to exploit the label information, the authors add the concept constraints, which include must-link and cannot- link constraints, as the penalty terms in the objective function of TPLSA. Finally , the objective function is iter- atively optimized to obtain the classiﬁcation results (i.e ., arg maxzP (zdT i )) by using the EM algorithm. The work by Zhuang et al. proposes an approach termed Collaborative Dual-PLSA (CD-PLSA) for multi-domain text classiﬁcation ( mS source domains and mT target domains) 11 [91], [92]. CD-PLSA is an extension of Dual-PLSA. Its dia- gram is shown below: P (Dk0 )  D  P (dizd k1 , Dk0 )  d zd P (zd k1 ,z w k2 ) zw  P (wj zw k2 , Dk0 )  w ց ր , where 1  k0  mS  mT denotes the domain index. The domain Dconnects both the variables d and w, but is independent of the variables zd and zw. The label in- formation of the source-domain instances is utilized by initializing the value P (dizd k1 , Dk0 ) (k0  1 ,  , m S). Due to the lack of the target-domain label information, the value P (dizd k1 , Dk0 ) (k0  mS  1 ,  , m S  mT ) can be initialized based on any supervised classiﬁer . Similarl y , the authors adopt the EM algorithm to ﬁnd the param- eters. Through the iterations, all the parameters in the Bayesian network are obtained. Thus, the class label of the i-th document in a target domain (denoted by Dk) can be predicted by computing the posterior probabilities, i.e ., arg maxzd P (zddi, Dk). Zhuang et al. further proposed a general framework that is termed as Homogeneous-Identical-Distinct-Concep t Model (HIDC) [93]. This framework is also an extension of Dual-PLSA. HIDC is composed of three generative models, i.e., identical-concept, homogeneous-concept, and disti nct- concept models. These three graphical models are presented below: Identical-Concept Model: D d zd ց ր zw IC w, Homogeneous-Concept Model: ր ց D d zd ց ր zw HC w, Distinct-Concept Model: ր ց ց D d zd ց ր zw DC w . The original word concept zw is divided into three types, i.e., zw IC, zw HC , and zw DC. In the identical-concept model, the word distributions only rely on the word concepts, and the word concepts are independent of the domains. However , in the homogeneous-concept model, the word distributions also depend on the domains. The difference between the identical and the homogeneous concepts is that zw IC is di- rectly transferable, while zw HC is the domain-speciﬁc transfer- able one that may have different effects on the word distri- butions for different domains. In the distinct-concept mod el, zw DC is actually the nontransferable domain-speciﬁc one, which may only appear in a speciﬁc domain. The above- mentioned three models are combined as an integrated one, i.e., HIDC. Similar to other PLSA-related algorithms, HIDC also uses EM algorithm to get the parameters. 4.2.5 Feature Selection Feature selection is another kind of operation for feature reduction, which is used to extract the pivot features. The pivot features are the ones that behave in the same way in different domains. Due to the stability of these features , they can be used as the bridge to transfer the knowledge. For example, Blitzer et al. proposed an approach termed Structural Correspondence Learning (SCL) [94]. Brieﬂy , SC L consists of the following steps to construct a new feature representation. 1. Feature Selection : SCL ﬁrst performs feature selection operations to obtain the pivot features. 2. Mapping Learning : The pivot features are utilized to ﬁnd a low-dimensional common latent feature space by using the structural learning technique [95]. 3. Feature Stacking : A new feature representation is con- structed by feature augmentation, i.e., stacking the original features with the obtained low-dimensional features. T ake the part-of-speech tagging problem as an example. The selected pivot features should occur frequently in source and target domains. Therefore, determiners can be included in pivot features. Once all the pivot features are deﬁned and selected, a number of binary linear classiﬁers whose function is to predict the occurrence of each pivot feature a re constructed. Without losing generality , the decision func tion of the i-th classiﬁer , which is used to predict the i-th pivot feature, can be formulated as fi(x)  sign(θi x), where x is assumed to be a binary feature input. And the i-th classiﬁer is trained on all the instances excluding the features deriv ed from the i-th pivot feature. The following formula can be used to estimate the i-th classiﬁer s parameters, i.e., θi  arg min θ 1 n n j1 L(θ xj , Rowi(xj ))  λθ2, where Row i(xj ) denotes the true value of the unlabeled instance xj in terms of the i-th pivot feature. By stacking the obtained parameter vectors as column elements, a matrix W is obtained. Next, based on singular value decomposition (SVD), the top- k left singular vectors, which are the prin- cipal components of the matrix W , are taken to construct the transformation matrix W . At last, the ﬁnal classiﬁer is trained on the labeled instances in an augmented feature space, i.e., ([xL i ; W T xL i ]T , y L i ). 4.2.6 Feature Encoding In addition to feature extraction and selection, feature en - coding is also an effective tool. For example, autoencoders , which are often adopted in deep learning area, can be used for feature encoding. An autoencoder consists of an encoder and a decoder . The encoder tries to produce a more abstract representation of the input, while the decoder aims to map back that representation and to minimize the reconstructio n error . Autoencoders can be stacked to build a deep learning architecture. Once an autoencoder completes the training process, another autoencoder can be stacked at the top of it. The newly added autoencoder is then trained by using the encoded output of the upper-level autoencoder as its input. In this way , deep learning architectures can thus be constructed. Some transfer learning approaches are developed based on autoencoders. For example, the paper by Glorot et al. proposes an approach termed Stacked Denoising Autoen- coder (SDA) [96]. The denoising autoencoder , which can enhance the robustness, is an extension of the basic one [97] . This kind of autoencoder contains a randomly corrupting mechanism that adds noise to the input before mapping. For 12 example, an input can be corrupted or partially destroyed by adding a masking noise or Gaussian noise. The denoising autoencoder is then trained to minimize the denoising re- construction error between the original clean input and the output. The SDA algorithm proposed in the paper mainly encompasses the following steps. 1. Autoencoder T raining : The source-domain and target- domain instances are used to train a stack of denoising autoencoders in a greedy layer-by-layer way . 2. Feature Encoding  Stacking: A new feature representa- tion is constructed by stacking the encoding output of intermediate layers, and the features of the instances are transformed into the obtained new representation. 3. Learner T raining : The target classiﬁer is trained on the transformed labeled instances. Although the SDA algorithm has excellent performance for feature extraction, it still has some drawbacks such as high computational and parameter-estimation cost. In orde r to shorten the training time and to speed up traditional SDA algorithms, Chen et al. proposed a modiﬁed version of SDA, i.e., Marginalized Stacked Linear Denoising Au- toencoder (mSLDA) [98], [99]. This algorithm adopts linear autoencoders and marginalizes the randomly corrupting step in a closed form. It may seem that linear autoencoders are too simple to learn complex features. However , the authors observe that linear autoencoders are often sufﬁcie nt to achieve competent performance when encountering high dimensional data. The basic architecture of mSLDA is a single-layer linear autoencoder . The corresponding singl e- layer mapping matrix W (augmented with a bias column for convenience) should minimize the expected squared reconstruction loss function, i.e., W  arg min W 1 2n n i1 EP (xix) [ xi W xi2] , where xi denotes the corrupted version of the input xi. The solution of W is given by [98], [99]: W  ( n i1 xiE[xi]T )( n i1 E [ xi xT i ] ) 1 . When the corruption strategy is determined, the above for- mulas can be further expanded and simpliﬁed into a speciﬁc form. Note that, in order to insert nonlinearity , a nonlinea r function is used to squash the output of each autoencoder after we obtain the matrix W in a closed form. Then, the next linear autoencoder is stacked to the current one in a similar way to SDA. In order to deal with high dimensional data, the authors also put forward an extension approach to further reduce the computational complexity . 4.2.7 Feature Alignment Note that feature augmentation and feature reduction mainly focus on the explicit features in a feature space. In contrast, in addition to the explicit features, feature alignment also focuses on some implicit features such as the statistic features and the spectral features. Therefor e, feature alignment can play various roles in the feature transformation process. For example, the explicit feature s can be aligned to generate a new feature representation, or the implicit features can be aligned to construct a satisﬁed feature transformation. There are several kinds of features that can be aligned, which includes subspace features, spectral features, and statistic features. T ake the subspace feature alignment as an example. A typical approach mainly has the following steps. 1. Subspace Generation : In this step, the instances are used to generate the respective subspaces for the source and the target domains. The orthonormal bases of the source and the target domain subspaces are then obtained, which are denoted by MS and MT , respectively . These bases are used to learn the shift between the subspaces. 2. Subspace Alignment : In the second step, a mapping, which aligns the bases MS and MT of the subspaces, is learned. And the features of the instances are pro- jected to the aligned subspaces to generate new feature representation. 3. Learner T raining: Finally , the target learner is trained on the transformed instances. For example, the work by Fernando et al. proposes an approach termed Subspace Alignment (SA) [100]. In SA, the subspaces are generated by performing PCA; the bases MS and MT are obtained by selecting the leading eigenvectors. Then, a transformation matrix W is learned to align the subspaces, which is given by [100]: W  arg min W MSW MT 2 F  MT SMT , where  F denotes the Frobenius norm. Note that the matrix W aligns MS with MT , or say , transforms the source subspace coordinate system into the target subspace coor- dinate system. The transformed low-dimensional source- domain and target-domain instances are given by XSMSW and XT MT , respectively . Finally , a learner can be trained on the resultant transformed instances. In light of SA, a number of transfer learning approaches are established. For example, the paper by Sun and Saenko proposes an approach that aligns both the subspace bases and the distributions [101], which is termed as Subspace Distribution Alignment between T wo Subspaces (SDA-TS). In SDA-TS, the transformation matrix W is formulated as W  MT SMT Q, where Q is a matrix used to align the distribution difference. The transformation matrix W in SA is a special case of the one in SDA-TS by setting Q to an identity matrix. Note that SA is a symmetrical feature-base d approach, while SDA-TS is an asymmetrical one. In SDA- TS, the labeled source-domain instances are projected to th e source subspace, then mapped to the target subspace, and ﬁnally mapped back to the target domain. The transformed source-domain instances are formulated as XSMSW M T T . Another representative subspace feature alignment ap- proach is Geodesic Flow Kernel (GFK) [102], which is pro- posed by Gong et al . GFK is closely related to a previous ap- proach termed Geodesic Flow Subspaces (GFS) [103]. Before introducing GFK, let us review the steps of GFS at ﬁrst. GFS is inspired by incremental learning. Intuitively , utilizi ng the information conveyed by the potential path between two domains may be beneﬁcial to the domain adaptation. GFS generally takes the following steps to align features. 13 1. Subspace Generation : GFS ﬁrst generates two subspaces of the source and the target domains by performing PCA, respectively . 2. Subspace Interpolation : The two obtained subspaces can be viewed as two points on the Grassmann manifold [104]. A ﬁnite number of the interpolated subspaces are generated between these two subspaces based on the geometric properties of the manifold. 3. Feature Projection  Stacking: The original features are transformed by stacking the corresponding projections from all the obtained subspaces. Despite the usefulness and superiority of GFS, there is a problem about how to determine the number of the interpo- lated subspaces. GFK resolves this problem by integrating inﬁnite number of the subspaces located on the geodesic curve from the source subspace to the target one. The key of GFK is to construct an inﬁnite-dimensional feature space that incorporating the information of all the subspaces lyi ng on the geodesic ﬂow . In order to compute the inner product in the resultant inﬁnite-dimensional space, the geodesic- ﬂow kernel is deﬁned and derived. In addition, a subspace- disagreement measure is proposed to select the optimal dimensionality of the subspaces; a rank-of-domain metric is also proposed to select the optimal source domain when multi-source domains are available. Statistic feature alignment is another kind of feature alignment. For example, Sun et al. proposed an approach termed Co-Relation Alignment (CORAL) [105]. CORAL constructs the transformation matrix of the source feature s by aligning the second-order statistic features, i.e., the co- variance matrices. The transformation matrix W is given by [105]: W  arg min W W T CS W CT 2 F , where C denotes the covariance matrix. Note that, com- pared to the above subspace-based approaches, CORAL avoids subspace generation as well as projection and is very easy to implement. Some transfer learning approaches are established based on spectral feature alignment. In traditional machine lear n- ing area, spectral clustering is a clustering technique bas ed on graph theory . The key of this technique is to utilize the spectrum, i.e., eigenvalues, of the similarity matrix t o reduce the dimension of the features before clustering. The similarity matrix is constructed to quantitatively assess the relative similarity of each pair of datavertices. On the basis of spectral clustering and feature alignment, Spectr al Feature Alignment (SF A) [106] is proposed by Pan et al . SF A is an algorithm for sentiment classiﬁcation. This algorith m tries to identify the domain-speciﬁc words and domain- independent words in different domains, and then aligns these domain-speciﬁc word features to construct a low- dimensional feature representation. SF A generally contai ns the following ﬁve steps. 1. Feature Selection : In this step, feature selection operations are performed to select the domain- independentpivot features. The paper presents three strategies to select domain-independent features. These strategies are based on the occurrence frequency of words, the mutual information between features and labels [107], and the mutual information between fea- tures and domains, respectively . 2. Similarity Matrix Construction : Once the domain-speciﬁc and the domain-independent features are identiﬁed, a bipartite graph is constructed. Each edge of this bipar- tite graph is assigned with a weight that measures the co-occurrence relationship between a domain-speciﬁc word and a domain-independent word. Based on the bipartite graph, a similarity matrix is then constructed. 3. Spectral Feature Alignment : In this step, a spectral clus- tering algorithm is adapted and performed to align domain-speciﬁc features [108], [109]. Speciﬁcally , based on the eigenvectors of the graph Laplacian, a feature alignment mapping is constructed, and the domain- speciﬁc features are mapped into a low-dimensional feature space. 4. Feature Stacking : The original features and the low- dimensional features are stacked to produce the ﬁnal feature representation. 5. Learner T raining : The target learner is trained on the labeled instances with the ﬁnal feature representation. There are some other spectral transfer learning ap- proaches. For example, the work by Ling et al. proposes an approach termed Cross-Domain Spectral Classiﬁer (CDSC) [110]. The general ideas and steps of this approach are presented as follows. 1. Similarity Matrix Construction : In the ﬁrst step, two similarity matrices are constructed corresponding to the whole instances and the target-domain instances, respectively . 2. Spectral Feature Alignment : An objective function is de- signed with respect to a graph-partition indicator vec- tor; a constraint matrix is constructed, which contains pair-wise must-link information. Instead of seeking the discrete solution of the indicator vector , the solution is relaxed to be continuous, and the eigen-system problem corresponding to the objective function is solved to construct the aligned spectral features [111]. 3. Learner T raining: A traditional classiﬁer is trained on the transformed instances. T o be more speciﬁc, the objective function has a form of the generalized Rayleigh quotient, which aims to ﬁnd the optimal graph partition that respects the label informatio n with small cut-size [112], to maximize the separation of the target-domain instances, and to ﬁt the constraints of the pair-wise property . After eigen-decomposition, the la st eigenvectors are selected and combined as a matrix, and then the matrix is normalized. Each row of the normalized matrix represents a transformed instance. 5 M ODEL -BASED INTERPRETATIO N T ransfer learning approaches can also be interpreted from the model perspective. Fig. 4 shows the corresponding strategies and the objectives. The main objective of a trans fer learning model is to make accurate prediction results on the target domain, e.g., classiﬁcation or clustering results. Note that a transfer learning model may consist of a few sub- modules such as classiﬁers, extractors, or encoders. These sub-modules may play different roles, e.g., feature adapta - tion or pseudo label generation. In this section, some relat ed 14 Model-Based Interpretation Objective Domain Adaptation ... Strategy Model Ensemble Model Selection Parameter Sharing Prediction Making Parameter Control Deep Learning Technique Parameter Restriction Voting Strategy Weighting Strategy ... ... Traditional Deep Learning Adversarial Deep Learning ... Model Control Consensus Regularizer Domain-Dependent Regularizer ... Pseudo Label Generation Fig. 4. Strategies and objectives of the transfer learning a pproaches from the model perspective. transfer learning approaches are introduced in proper orde r according to the strategies shown in Fig. 4. 5.1 Model Control Strategy From the perspective of model, a natural thought is to directly add the model-level regularizers to the learner s objective function. In this way , the knowledge contained in the pre-obtained source models can be transferred into the target model during the training process. For example, the paper by Duan et al. proposes a general framework termed Domain Adaptation Machine (DAM) [113], [114], which is designed for multi-source transfer learning. The goal of DAM is to construct a robust classiﬁer for the target domain with the help of some pre-obtained base classiﬁers that are respectively trained on multiple source domains. The objective function is given by: min fT LT,L (fT )  λ1Ω D (fT )  λ2Ω( fT ), where the ﬁrst term represents the loss function used to min- imize the classiﬁcation error of the labeled target-domain in- stances, the second term denotes different regularizers, a nd the third term is used to control the complexity of the ﬁnal decision function fT . Different types of the loss functions can be adopted in LT,L (fT ) such as the square error or the cross-entropy loss. Some transfer learning approaches can be regarded as the special cases of this framework to some extent.  (Consensus Regularizer) The work by Luo et al. proposes a framework termed Consensus Regularization Frame- work (CRF) [115], [116]. CRF is designed for multi-source transfer learning with no labeled target-domain instances . The framework constructs mS classiﬁers corresponding to each source domain, and these classiﬁers are required to reach mutual consensuses on the target domain. The objective function of each source classiﬁer , denoted by fS k (with k  1 ,  , m S), is similar to that of DAM, which is presented below: min fS k  nSk  i1 log P (ySk i xSk i ; fS k )  λ2Ω( fS k ) λ1 nT,U  i1  yj Y S ( 1 mS mS  k01 P (yj xT,U i ; fS k0 ) ) , where fS k denotes the decision function corresponding to the k-th source domain, and S(x)  x log x. The ﬁrst term is used to quantify the classiﬁcation error of the k-th classiﬁer on the k-th source domain, and the last term is the consensus regularizer in the form of cross- entropy . The consensus regularizer can not only enhance the agreement of all the classiﬁers, but also reduce the uncertainty of the predictions on the target domain. The authors implement this framework based on the logistic regression. A difference between DAM and CRF is that DAM explicitly constructs the target classiﬁer , while CRF makes the target predictions based on the reached consen- sus from the source classiﬁers.  (Domain-dependent Regularizer) Fast-DAM is a speciﬁc algorithm of DAM [113]. In light of the manifold assump- tion [30] and the graph-based regularizer [117], [118], Fast-DAM designs a domain-dependent regularizer . The objective function is given by: min fT nT,L  j1 ( fT (xT,L j ) yT,L j ) 2  λ2Ω( fT ) λ1 mS  k1 βk nT,U  i1 ( fT (xT,U i ) fS k (xT,U i ) ) 2 , where fS k (k  1 , 2,  , m S) denotes the pre-obtained source decision function for the k-th source domain and βk represents the weighting parameter that is determined by the relevance between the target domain and the k- th source domain and can be measured based on the MMD metric. The third term is the domain-dependent regularizer , which transfers the knowledge contained in 15 the source classiﬁer motivated by domain dependence. In [113], the authors also introduce and add a new term to the above objective function based on ε-insensitive loss function [119], which makes the resultant model have high computational efﬁciency .  (Domain-dependent Regularizer  Universum Regular- izer) Univer-DAM is an extension of the Fast-DAM [114]. Its objective function contains an additional regularizer , i.e., Universum regularizer . This regularizer usually uti - lizes an additional dataset termed Universum where the instances do not belong to either the positive or the negative class [120]. The authors treat the source-domain instances as the Universum for the target domain, and the objective function of Univer-DAM is presented as follows: min fT nT,L  j1 ( fT (xT,L j ) yT,L j ) 2  λ2 nS  j1 ( fT (xS j ) ) 2 λ1 mS  k1 βk nT,U  i1 ( fT (xT,U i ) fS k (xT,U i ) ) 2  λ3Ω( fT ). Similar to Fast-DAM, the ε-insensitive loss function can also be utilized [114]. 5.2 Parameter Control Strategy The parameter control strategy focuses on the parameters of models. For example, in the application of object categoriz a- tion, the knowledge from known source categories can be transferred into target categories via object attributes s uch as shape and color [121]. The attribute priors, i.e., probabil istic distribution parameters of the image features correspondi ng to each attribute, can be learned from the source domain and then used to facilitate learning the target classiﬁer . The parameters of a model actually reﬂect the knowledge learned by the model. Therefore, it is possible to transfer t he knowledge at the parametric level. 5.2.1 Parameter Sharing An intuitive way of controlling the parameters is to directl y share the parameters of the source learner to the target learner . Parameter sharing is widely employed especially in the network-based approaches. For example, if we have a neural network for the source task, we can freeze (or say , share) most of its layers and only ﬁnetune the last few layers to produce a target network. The network-based approaches are introduced in Section 5.4. In addition to network-based parameter sharing, matrix- factorization-based parameter sharing is also workable. F or example, Zhuang et al. proposed an approach for text clas- siﬁcation, which is referred to as Matrix T ri-Factorizatio n Based Classiﬁcation Framework (MT rick) [122]. The au- thors observe that, in different domains, different words o r phrases sometimes express the same or similar connotative meaning. Thus, it is more effective to use the concepts be- hind the words rather than the words themselves as a bridge to transfer the knowledge in source domains. Different from PLSA-based transfer learning approaches that utilize the concepts by constructing Bayesian networks, MT rick attempts to ﬁnd the connections between the document classes and the concepts conveyed by the word clusters through matrix tri-factorization. These connections are c on- sidered to be the stable knowledge that is supposed to be transferred. The main idea is to decompose a document-to- word matrix into three matrices, i.e., document-to-cluste r , connection, and cluster-to-word matrices. Speciﬁcally , b y performing the matrix tri-factorization operations on the source and the target document-to-word matrices respec- tively , a joint optimization problem is constructed, which is given by min Q,R,W XS QSRW S2  λ1XT QT RW T 2 λ2QS QS2 s.t. Normalization Constraints , where X denotes the document-to-word matrix, Q denotes the document-to-cluster matrix, R represents the transfor- mation matrix from document clusters to word clusters, W denotes the cluster-to-word matrix, nd denotes the number of the documents, and QS represents the label matrix. The matrix QS is constructed based on the class information of the source-domain documents. If the i-th document belongs to the k-th class, QS [i,k ]  1 . In the above objective function, the matrix R is actually the shared parameter . The ﬁrst term aims to tri-factorize the source document-to-word matrix, and the second term decomposes the target document-to- word matrix. The last term incorporates the source-domain label information. The optimization problem is solved base d on the alternating iteration method. Once the solution of QT is obtained, the class index of the k-th target-domain instance is the one with the maximum value in the k-th row of QT . Further , Zhuang et al. extended MT rick and proposed an approach termed T riplex T ransfer Learning (T riTL) [123] . MT rick assumes that the domains share the similar con- cepts behind their word clusters. In contrast, T riTL as- sumes that the concepts of these domains can be further divided into three types, i.e., domain-independent, trans fer- able domain-speciﬁc, and nontransferable domain-speciﬁc concepts, which is similar to HIDC. This idea is motivated by Dual T ransfer Learning (DTL), where the concepts are assumed to be composed of the domain-independent ones and the transferable domain-speciﬁc ones [124]. The objec- tive function of T riTL is provided as follows: min Q,R,W mS mT  k1 Xk Qk [ RDI RTD RND k ]   W DI W TD k W ND k  2 s.t. Normalization Constraints , where the deﬁnitions of the symbols are similar to those of MT rick and the subscript k denotes the index of the domains with the assumption that the ﬁrst mS domains are the source domains and the last mT domains are the target do- mains. The authors proposed an iterative algorithm to solve the optimization problem. And in the initialization phase, W DI and W TD k are initialized based on the clustering results of the PLSA algorithm, while W UT k is randomly initialized; the PLSA algorithm is performed on the combination of the instances from all the domains. There are some other approaches developed based on matrix factorization. W ang et al. proposed a transfer learn- ing framework for image classiﬁcation [125]. W ang et al. 16 proposed a softly associative approach that integrates two matrix tri-factorizations into a joint framework [126]. Do et al. utilized matrix tri-factorization to discover both the implicit and the explicit similarities for cross-domain re c- ommendation [127]. 5.2.2 Parameter Restriction Another parameter-control-type strategy is to restrict th e parameters. Different from the parameter sharing strategy that enforces the models share some parameters, parame- ter restriction strategy only requires the parameters of th e source and the target models to be similar . T ake the approaches to category learning as examples. The category-learning problem is to learn a new decision function for predicting a new category (denoted by the (k  1) -th category) with only limited target-domain in- stances and k pre-obtained binary decision functions. The function of these pre-obtained decision functions is to pre - dict which of the k categories an instance belongs to. In order to solve the category-learning problem, T ommasi et al. proposed an approach termed Single-Model Knowledge T ransfer (SMKL) [128]. SMKL is based on Least-Squares SVM (LS-SVM). The advantage of LS-SVM is that LS-SVM transforms inequality constraints to equality constraint s and has high computational efﬁciency; its optimization is equi v- alent to solving a linear equation system problem instead of a quadratic programming problem. SMKL selects one of the pre-obtained binary decision functions, and transfers the knowledge contained in its parameters. The objective function is given by min f 1 2      θ β θ       2  λ 2 nT,L  j1 ηj ( f(xT,L j ) yT,L j ) 2 , where f(x)  θ Φ( x)  b, β is the weighting parameter controlling the transfer degree, θ is the parameter of a selected pre-obtained model, and ηj is the coefﬁcient for resolving the label imbalance problem. The kernel param- eter and the tradeoff parameter are chosen based on cross- validation. In order to ﬁnd the optimal weighting parameter , the authors refer to an earlier work [129]. In [129], Cawley proposed a model selection mechanism for LS-SVM, which is based on the leave-one-out cross-validation method. The superiority of this method is that the leave-one-out error for each instance can be obtained in a closed form without performing the real cross-validation experiment. Motivat ed by Cawleys work, the generalization error can be easily estimated to guide the parameter setting in SMKL. T ommasi et al. further extended SMKL by utilizing all the pre-obtained decision functions. In [130], an approach tha t is referred to as Multi-Model Knowledge T ransfer (MMKL) is proposed. Its objective function is presented as follows : min f 1 2          θ  k i1 βiθi           2  λ 2 nT,L  j1 ηj ( f(xT,L j ) yT,L j ) 2 , where θi and βi are the model parameter and the weighting parameter of the i-th pre-obtained decision function, respec- tively . The leave-one-out error can also be obtained in a closed form, and the optimal value of βi (i  1 , 2,  , k ) is the one that maximizes the generalization performance. 5.3 Model Ensemble Strategy In sentiment analysis applications related to product re- views, data or models from multiple product domains are available and can be used as the source domains [131]. Com- bining data or models directly into a single domain may not be successful because the distributions of these domain s are different from each other . Model ensemble is another commonly used strategy . This strategy aims to combine a number of weak classiﬁers to make the ﬁnal predictions. Some previously mentioned transfer learning approaches already adopted this strategy . For example, T rAdaBoost and MsT rAdaBoost ensemble the weak classiﬁers via voting and weighting, respectively . In this subsection, several typi cal ensemble-based transfer learning approaches are introduc ed to help readers better understand the function and the appliance of this strategy . As mentioned in Section 4.1, T askT rAdaBoost, which is an extension of T rAdaBoost for handling multi-source scenarios, is proposed in the paper [33]. T askT rAdaBoost mainly has the following two stages. 1. Candidate Classiﬁer Construction : In the ﬁrst stage, a group of candidate classiﬁers are constructed by per- forming AdaBoost on each source domain. Note that, for each source domain, each iteration of AdaBoost re- sults in a new weak classiﬁer . In order to avoid the over- ﬁtting problem, the authors introduced a threshold to pick the suitable classiﬁers into the candidate group. 2. Classiﬁer Selection and Ensemble : In the second stage, a revised version of AdaBoost is performed on the target- domain instances to construct the ﬁnal classiﬁer . In each iteration, an optimal candidate classiﬁer which has the lowest classiﬁcation error on the labeled target-domain instances is picked out and assigned with a weight based on the classiﬁcation error . Then, the weight of each target-domain instance is updated based on the performance of the selected classiﬁer on the target do- main. After the iteration process, the selected classiﬁers are ensembled to produce the ﬁnal predictions. The difference between the original AdaBoost and the sec- ond stage of T askT rAdaBoost is that, in each iteration, the former constructs a new candidate classiﬁer on the weighted target-domain instances, while the latter selects one pre- obtained candidate classiﬁer which has the minimal clas- siﬁcation error on the weighted target-domain instances. The paper by Gao et al. proposes another ensemble- based framework that is referred to as Locally W eighted En- semble (L WE) [132]. L WE focuses on the ensemble process of various learners; these learners could be constructed on different source domains, or be built by performing differe nt learning algorithms on a single source domain. Different from T askT rAdaBoost that learns the global weight of each learner , the authors adopted the local-weight strategy , i. e., assigning adaptive weights to the learners based on the loca l manifold structure of the target-domain test set. In L WE, a learner is usually assigned with different weights when classifying different target-domain instances. Speciﬁca lly , the authors adopt a graph-based approach to estimate the weights. The steps for weighting are outlined below . 1. Graph Construction : For the i-th source learner , a graph GT Si is constructed by using the learner to classify the 17 target-domain instances in the test set; if two instances are classiﬁed into the same class, they are connected in the graph. Another graph GT is constructed for the target-domain instances as well by performing a clustering algorithm. 2. Learner Weighting : The weight of the i-th learner for the j-th target-domain instance xT j is proportional to the similarity between the instances local structures in GT Si and GT . And the similarity can be measured by the percentage of the common neighbors of xT j in these two graphs. Note that this weighting scheme is based on the clustering- manifold assumption, i.e., if two instances are close to eac h other in a high-density region, they often have similar labels. In order to check the validity of this assumption for the task, the target task is tested on the source-domain training set(s). Speciﬁcally , the clustering quality of th e training set(s) is quantiﬁed and checked by using a metric such as purity or entropy . If the clustering quality is not satisfactory , uniform weights are assigned to the learners instead. Besides, it is intuitive that if the measured struc ture similarity is particularly low for every learner , weightin g and combining these learners seems unwise. Therefore, the authors introduce a threshold and compare it to the average similarity . If the similarity is lower than the threshold, t he label of xT j is determined by the voting scheme among its reliable neighbors, where the reliable neighbors are the on es whose label predictions are made by the combined classiﬁer . The above-mentioned T askT rAdaBoost and L WE ap- proaches mainly focus on the ensemble process. In con- trast, some studies focus more on the construction of weak learners. For example, Ensemble Framework of Anchor Adapters (ENCHOR) [133] is a weighting ensemble frame- work proposed by Zhuang et al . An anchor is a speciﬁc instance. Different from T rAdaBoost which adjusts weights of instances to train and produce a new learner iteratively , ENCHOR constructs a group of weak learners via using dif- ferent representations of the instances produced by anchor s. The thought is that the higher similarity between a certain instance and an anchor , the more likely the feature of that in - stance remains unchanged relative to the anchor , where the similarity can be measured by using the cosine or Gaussian distance function. ENCHOR contains the following steps. 1. Anchor Selection : In this step, a group of anchors are selected. These anchors can be selected based on some rules or even randomly . In order to improve the ﬁ- nal performance of ENCHOR, the authors proposed a method of selecting high-quality anchors [133]. 2. Anchor-based Representation Generation : For each anchor and each instance, the feature vector of an instance is directly multiplied by a coefﬁcient that measures the distance from the instance to the anchor . In this way , each anchor produces a new pair of anchor-adapted source and target instance sets. 3. Learner T raining and Ensemble : The obtained pairs of instance sets can be respectively used to train learners. Then, the resultant learners are weighted and combined to make the ﬁnal predictions. The framework ENCHOR is easy to be realized in a parallel manner in that the operations performed on each anchor are independent. 5.4 Deep Learning T echnique Deep learning methods are particularly popular in the ﬁeld of machine learning. Many researchers utilize the deep learning techniques to construct transfer learning models . For example, the SDA and the mSLDA approaches men- tioned in Section 4.2.6 utilize the deep learning technique s. In this subsection, we speciﬁcally discuss the deep-learni ng- related transfer learning models. The deep learning ap- proaches introduced are divided into two types, i.e., non- adversarial (or say , traditional) ones and adversarial one s. 5.4.1 T raditional Deep Learning As said earlier , autoencoders are often used in deep learnin g area. In addition to SDA and mSLDA, there are some other reconstruction-based transfer learning approaches. For e x- ample, the paper by Zhuang et al. proposes an approach termed T ransfer Learning with Deep Autoencoders (TLDA) [44], [134]. TLDA adopts two autoencoders for the source and the target domains, respectively . These two autoen- coders share the same parameters. The encoder and the decoder both have two layers with activation functions. The diagram of the two autoencoders is presented as follows: XS (W1,b 1) QS (W2,b 2)  Softmax Regression RS ( ˆW2, ˆb2) QS ( ˆW1, ˆb1) XS,  KL Divergence  XT (W1,b 1) QT (W2,b 2)  Softmax Regression RT ( ˆW2, ˆb2) QT ( ˆW1, ˆb1) XT . There are several objectives of TLDA, which are listed as follows. 1. Reconstruction Error Minimization : The output of the de- coder should be extremely close to the input of encoder . In other words, the distance between XS and XS as well as the distance between XT and XT should be minimized. 2. Distribution Adaptation : The distribution difference be- tween QS and QT should be minimized. 3. Regression Error Minimization : The output of the encoder on the labeled source-domain instances, i.e., RS, should be consistent with the corresponding label information Y S. Therefore, the objective function of TLDA is given by min Θ LREC (X, X)  λ1KL(QSQT )  λ2Ω( W, b, ˆW , ˆb) λ3LREG (RS, Y S), where the ﬁrst term represents the reconstruction error , KL() represents the KL divergence, the third term controls the complexity , and the last term represents the regression error . TLDA is trained by using a gradient descent method. The ﬁnal predictions can be made in two different ways. The ﬁrst way is to directly use the output of the encoder to make predictions. And the second way is to treat the autoencoder as a feature extractor , and then train the target classiﬁer on the labeled instances with the feature representation produced by the encoder s ﬁrst-layer output. 18 In addition to the reconstruction-based domain adapta- tion, discrepancy-based domain adaptation is also a popula r direction. In earlier research, the shallow neural network s are tried to learn the domain-independent feature repre- sentation [135]. It is found that the shallow architectures often make it difﬁcult for the resultant models to achieve excellent performance. Therefore, many studies turn to uti - lize deep neural networks. Tzeng et al. [136] added a single adaptation layer and a discrepancy loss to the deep neural network, which improves the performance. Further , Long et al. performed multi-layer adaptation and utilized multi- kernel technique, and they proposed an architecture termed Deep Adaptation Networks (DAN) [137]. For better understanding, let us review DAN in detail. DAN is based on AlexNet [138] and its architecture is presented below [137]. full  6th RS 6 full  7th RS 7 full  8th RS 8 ( f(XS) ) XS XT conv  1st QS 1 QT 1 conv   QS 5 QT 5   ր ց  MK-MMD   MK-MMD   MK-MMD  Five Convolutional Layers full  6th RT 6 full  7th RT 7 full  8th RT 8 ( f(XT ) )    Three Fully Connected Layers In the above network, the features are ﬁrst extracted by ﬁve convolutional layers in a general-to-speciﬁc manner . Next , the extracted features are fed into one of the two fully connected networks switched by their original domains. These two networks both consist of three fully connected layers that are specialized for the source and the target domains. DAN has the following objectives. 1. Classiﬁcation Error Minimization : The classiﬁcation error of the labeled instances should be minimized. The cross-entropy loss function is adopted to measure the prediction error of the labeled instances. 2. Distribution Adaptation : Multiple layers, which include the representation layers and the output layer , can be jointly adapted in a layer-wise manner . Instead of using the single-kernel MMD to measure the distribution difference, the authors turn to MK-MMD. The authors adopt the linear-time unbiased estimation of MK-MMD to avoid numerous inner product operations [62]. 3. Kernel Parameter Optimization : The weighting parame- ters of the multiple kernels in MK-MMD should be optimized to maximize the test power [62]. The objective function of the DAN network is given by: min Θ max κ nL  i1 L ( f(xL i ), y L i )  λ 8 l6 MK-MMD(RS l , R T l ; κ), where l denotes the index of the layer . The above opti- mization is actually a minimax optimization problem. The maximization of the objective function with respect to the kernel function κ aims to maximize the test power . After this step, the subtle difference between the source and the targe t domains are magniﬁed. This train of thought is similar to the Generative Adversarial Network (GAN) [139]. In the training process, the DAN network is initialized by a pre- trained AlexNet [138]. There are two categories of param- eters that should be learned, i.e., the network parameters and the weighting parameters of the multiple kernels. Given that the ﬁrst three convolutional layers output the general features and are transferable, the authors freeze them and ﬁne-turn the last two convolutional layers and the two fully connected layers [140]. The last fully connected layer (or s ay , the classiﬁer layer) is trained from scratch. Long et al. further extended the above DAN approach and proposed the DAN framework [141]. The new charac- teristics are summarized as follows. 1. Regularizer Adding : The framework introduces an ad- ditional regularizer to minimize the uncertainty of the predicted labels of the unlabeled target-domain in- stances, which is motivated by entropy minimization criterion [142]. 2. Architecture Generalizing : The DAN framework can be applied to many other architectures such as GoogLeNet [143] and ResNet [144]. 3. Measurement Generalizing : The distribution difference can be estimated by other metrics. For example, in addition to MK-MMD, the authors also present the Mean Embedding test for distribution adaptation [145]. The objective function of the DAN framework is given by: min Θ max κ nL  i1 L ( f(xL i ), y L i )  λ1 lend llstrt DIST(RS l , R T l )  λ2 nT,U  i1  yj Y S ( P (yj f(xT,U i )) ) , where lstrt and lend denote the boundary indexes of the fully connected layers for adapting the distributions. There are some other impressive works. For example, Long et al. constructed residual transfer networks for do- main adaptation, which is motivated by deep residual learn- ing [146]. Besides, another work by Long et al. proposes the Joint Adaptation Network (JAN) [147], which adapts the joint distribution difference of multiple layers. Sun a nd Saenko extended CORAL for deep domain adaptation and proposed an approach termed Deep CORAL (DCORAL), in which the CORAL loss is added to minimize the feature covariance [148]. Chen et al. realized that the instances with the same label should be close to each other in the feature space, and they not only add the CORAL loss but also add an instance-based class-level discrepancy loss [149]. Pan et al. constructed three prototypical networks (corresponding to DS, DT and DS DT ) and incorporated the thought of multi-model consensus. They also adopt pseudo-label strategy and adapt both the instance-level and class-level discrepancy [150]. Kang et al. proposed the Contrastive Adaptation Network (CAN), which is based on the dis- crepancy metric termed contrastive domain discrepancy [151]. Zhu et al. aimed to adapt the extracted multiple fea- ture representations and proposed the Multi-Representati on Adaptation Network (MRAN) [152]. Deep learning technique can also be used for multi- source transfer learning. For example, the work by Zhu et al. proposes a framework that is referred to as Multiple Feature Spaces Adaptation Network (MFSAN) [153]. The architec- ture of MFSAN consists of a common-feature extractor , mS domain-speciﬁc feature extractors, and mS domain-speciﬁc 19 classiﬁers. The corresponding schematic diagram is shown below . XS 1  XS k  XS mS XT Common  Extractor QS 1 QS k QS mS QT Domain-Speciﬁc   Extractors RS 1 RS k  RS mS RT 1 RT k  RT mS Domain-Speciﬁc  Classiﬁers ˆY S 1  ˆY S k  ˆY S mS ˆY T 1  ˆY T k  ˆY T mS In each iteration, MFSAN has the following steps. 1. Common Feature Extraction : For each source domain (denoted by DSk with k  1 ,  , m S), the source- domain instances (denoted by XS k ) are separately input to the common-feature extractor to produce instances in a common latent feature space (denoted by QS k ). Similar operations are also performed on the target-domain instances (denoted by XT ), which produces QT . 2. Speciﬁc Feature Extraction : For each source domain, the extracted common features QS k is fed to the k- th domain-speciﬁc feature extractor . Meanwhile, QT is fed to all the domain-speciﬁc feature extractors, which results in RT k with k  1 ,  , m S. 3. Data Classiﬁcation : The output of the k-th domain- speciﬁc feature extractor is input to the k-th classiﬁer . In this way , mS pairs of the classiﬁcation results are predicted in the form of probability . 4. Parameter Updating : The parameters of the network are updated to optimize the objective function. There are three objectives in MFSAN, i.e., classiﬁcation error minimization, distribution adaptation, and consens us regularization. The objective function is given by: min Θ mS  i1 L( ˆY S i , Y S i )  λ1 mS  i1 MMD(RS i , R T i )  λ2 mS  ij   ˆY T i ˆY T j   , where the ﬁrst term represents the classiﬁcation error of th e labeled source-domain instances, the second term measures the distribution difference, and the third term measures the discrepancy of the predictions on the target-domain instances. 5.4.2 Adversarial Deep Learning The thought of adversarial learning can be integrated into deep-learning-based transfer learning approaches. As men - tioned above, in the DAN framework, the network Θ and the kernel κ play a minimax game, which reﬂects the thought of adversarial learning. However , the DAN frame- work is a little different from the traditional GAN-based methods in terms of the adversarial matching. In the DAN framework, there is only a few parameters to be optimized in the max game, which makes the optimization easier to achieve equilibrium. Before introducing the adversaria l transfer learning approaches, let us brieﬂy review the orig i- nal GAN framework and the related work. The original GAN [139], which is inspired by the two- player game, is composed of two models, a generator G and a discriminator D . The generator produces the counterfeits of the true data for the purpose of confusing the discrimina- tor and making the discriminator produce wrong detection. The discriminator is fed with the mixture of the true data and the counterfeits, and it aims to detect whether a data is the true one or the fake one. These two models actually play a two-player minimax game, and the objective function is as follows: min G max D ExPtrue [log D (x)]  EzPz [log (1 D (G (z)))], where z represents the noise instances (sampled from a certain noise distribution) used as the input of the generat or for producing the counterfeits. The entire GAN can be trained by using the back-propagation algorithm. When the two-player game achieves equilibrium, the generator can produce almost true-looking instances. Motivated by GAN, many transfer learning approaches are established based on the assumption that a good feature representation contains almost no discriminative informa - tion about the instances original domains. For example, th e work by Ganin et al. proposes a deep architecture termed Domain-Adversarial Neural Network (DANN) for domain adaptation [154], [155]. DANN assumes that there is no labeled target-domain instance to work with. Its architec- ture consists of a feature extractor , a label predictor , and a domain classiﬁer . The corresponding diagram is as follows. Label  Predictor ˆY S,L ˆY T,U XS,L XT,U Feature  Extractor  QS,L QT,U } Domain  Classiﬁer ˆS ˆT (Domain Label ) The feature extractor acts like the generator , which aims to produce the domain-independent feature representation fo r confusing the domain classiﬁer . The domain classiﬁer plays the role like the discriminator , which attempts to detect whether the extracted features come from the source domain or the target domain. Besides, the label predictor produces the label prediction of the instances, which is trained on th e extracted features of the labeled source-domain instances , i.e., QS,L . DANN can be trained by inserting a special gra- dient reversal layer (GRL). After the training of the whole system, the feature extractor learns the deep feature of the instances, and the output ˆY T,U is the predicted labels of the unlabeled target-domain instances. There are some other related impressive works. The work by Tzeng et al. proposes a uniﬁed adversarial domain adaptation framework [156]. The work by Shen et al. adopts W asserstein distance for domain adaptation [59]. Hoffman et al. adopted cycle-consistency loss to ensure the structural and semantic consistency [157]. Long et al. proposed the Conditional Domain Adversarial Network (CDAN), which utilizes a conditional domain discriminator to assist adve r- sarial adaptation [158]. Zhang et al. adopted a symmetric design for the source and the target classiﬁers [159]. Zhao et al. utilized domain adversarial networks to solve the multi- source transfer learning problem [160]. Y u et al. proposed a dynamic adversarial adaptation network [161]. Some approaches are designed for some special scenar- ios. T ake the partial transfer learning as an example. The partial transfer learning approaches are designed for the s ce- nario that the target-domain classes are less than the sourc e- domain classes, i.e., YS  YT . In this case, the source- domain instances with different labels may have different 20 importance for domain adaptation. T o be more speciﬁc, the source-domain and the target-domain instances with the same label are more likely to be potentially associated. How - ever , since the target-domain instances are unlabeled, how to identify and partially transfer the important informati on from the labeled source-domain instances is a critical issu e. The paper by Zhang et al. proposes an approach for partial domain adaptation, which is called Impor- tance W eighted Adversarial Nets-Based Domain Adaptation (IW ANDA) [162]. The architecture of IW ANDA is different from that of DANN. DANN adopts one common feature extractor based on the assumption that there exists a com- mon feature space where QS,L and QT,U have the similar distribution. However , IW ANDA uses two domain-speciﬁc feature extractors for the source and the target domains, respectively . Speciﬁcally , IW ANDA consists of two feature extractors, two domain classiﬁers, and one label predictor . The diagram of IW ANDA is presented below . Label  Predictor ˆY S,L ˆY T,U XS,L Source Feature  Extractor  QS,L XT,U T arget Feature  Extractor QT,U  } β S   ˆY T,U 2nd Domain  Classiﬁer ˆS2 ˆT2 1st Domain  Classiﬁer ˆS1 ˆT1 W eight    Function β S Before training, the source feature extractor and the label predictor are pre-trained on the labeled source-domain in- stances. These two components are frozen in the training process, which means that only the target feature extractor and the domain classiﬁers should be optimized. In each iteration, the above network is optimized by taking the following steps. 1. Instance Weighting : In order to solve the partial transfer issue, the source-domain instances are assigned with weights based on the output of the ﬁrst domain clas- siﬁer . The ﬁrst domain classiﬁer is fed with QS,L and QT,U , and then outputs the probabilistic predictions of their domains. If a source domain instance is predicted with a high probability of belonging to the target do- main, this instance is highly likely to associate with the target domain. Thus, this instance is assigned with a larger weight and vice versa. 2. Prediction Making : The label predictor outputs the label predictions of the instances. The second classiﬁer pre- dicts which domain an instance belongs to. 3. Parameter Updating : The ﬁrst classiﬁer is optimized to minimize the domain classiﬁcation error . The second classiﬁer plays a minmax game with the target fea- ture extractor . This classiﬁer aims to detect whether a instance is the instance from the target domain or the weighted instance from the source domain, and to reduce the uncertainty of the label prediction ˆY T,U . The target feature extractor aims to confuse the second classiﬁer . These components can be optimized in a similar way to GAN or by inserting a GRL. In addition to IW ANDA, the work by Cao et al. con- structs the selective adversarial network for partial tran sfer learning [163]. There are some other studies related to transfer learning. For example, the work by W ang et al. proposes a minimax-based approach to select high-quality source-domain data [164]. Chen et al. investigated the trans- ferability and the discriminability in the adversarial dom ain adaptation, and proposed a spectral penalization approach to boost the existing adversarial transfer learning method s [165]. 6 A PPLICATIO N In previous sections, a number of representative trans- fer learning approaches are introduced, which have been applied to solving a variety of text-relatedimage-relate d problems in their original papers. For example, MT rick [122 ] and T riTL [123] utilize the matrix factorization technique to solve cross-domain text classiﬁcation problems; the deep- learning-based approaches such as DAN [137], DCORAL [148], and DANN [154], [155] are applied to solving image classiﬁcation problems. Instead of focusing on the general text-related or image-related applications, in this secti on, we mainly focus on the transfer learning applications in speci ﬁc areas such as medicine, bioinformatics, transportation, a nd recommender systems. 6.1 Medical Application Medical imaging plays an important role in the medical area, which is a powerful tool for diagnosis. With the de- velopment of computer technology such as machine learn- ing, computer-aided diagnosis has become a popular and promising direction. Note that medical images are gener- ated by special medical equipment, and their labeling often relies on experienced doctors. Therefore, in many cases, it is expensive and hard to collect sufﬁcient training data. T ran s- fer learning technology can be utilized for medical imaging analysis. A commonly used transfer learning approach is to pre-train a neural network on the source domain (e.g., ImageNet, which is an image database containing more than fourteen million annotated images with more than twenty thousand categories [166]) and then ﬁnetune it based on the instances from the target domain. For example, Maqsood et al. ﬁnetuned the AlexNet [138] for the detection of Alzheimer s disease [167]. Their ap- proach has the following four steps. First, the MRI images from the target domain are pre-processed by performing contrast stretching operations. Second, the AlexNet archi tec- ture [138] is pre-trained over ImageNet [166] (i.e., the sou rce domain) as a starting point to learn the new task. Third, the convolutional layers of AlexNet are ﬁxed, and the last three fully connected layers are replaced by the new ones including one softmax layer , one fully connected layer , and one output layer . Finally , the modiﬁed AlexNet is ﬁnetuned by training on the Alzheimer s dataset [168] (i.e., the targ et domain). The experimental results show that the proposed approach achieves the highest accuracy for the multi-class classiﬁcation problem (i.e, Alzheimer s stage detection) . Similarly , Shin et al. ﬁnetuned the pre-trained deep neural network for solving the computer-aided detection problems [169]. Byra et al. utilized the transfer learning tech- nology to help assess knee osteoarthritis [170]. In additio n to imaging analysis, transfer learning has some other applica - tions in the medical area. For example, the work by T ang et 21 al. combines the active learning and the domain adaptation technologies for the classiﬁcation of various medical data [171]. Zeng et al. utilized transfer learning for automatically encoding ICD-9 codes that are used to describe a patients diagnosis [172]. 6.2 Bioinformatics Application Biological sequence analysis is an important task in the bioinformatics area. Since the understanding of some or- ganisms can be transferred to other organisms, transfer learning can be applied to facilitate the biological sequen ce analysis. The distribution difference problem exists sign iﬁ- cantly in this application. For example, the function of som e biological substances may remain unchanged but with the composition changed between two organisms, which may result in the marginal distribution difference. Besides, i f two organisms have a common ancestor but with long evo- lutionary distance, the conditional distribution differe nce would be signiﬁcant. The work by Schweikert et al. uses the mRNA splice site prediction problem as the example to analyze the effectiveness of transfer learning approach es [173]. In their experiments, the source domain contains the sequence instances from a well-studied model organism, i.e ., C. elegans , and the target organisms include two additional nematodes (i.e., C. remanei and P . paciﬁcus ), D. melanogaster , and the plant A. thaliana . A number of transfer learning approaches, e.g., F AM [64] and the variant of KMM [5], are compared with each other . The experimental results show that transfer learning can help improve the classiﬁcation performance. Another widely encountered task in the bioinformatics area is gene expression analysis, e.g., predicting associa tions between genes and phenotypes. In this application, one of the main challenges is the data sparsity problem, since there is usually very little data of the known associations. T ransfer learning can be used to leverage this problem by providing additional information and knowledge. For example, Petegrosso et al. [174] proposed a transfer learn- ing approach to analyze and predict the gene-phenotype associations based on the Label Propagation Algorithm (LP A) [175]. LP A utilizes the Protein-Protein Interaction (PPI) network and the initial labeling to predict the target associations based on the assumption that the genes that are connected in the PPI network should have the similar labels. The authors extended LP A by incorporating multi-task and transfer-learning technologies. First, Human Phenotype O n- tology (HPO), which provides a standardized vocabulary of phenotypic features of human diseases, is utilized to form the auxiliary task. In this way , the associations can be predicted by utilizing phenotype paths and both the linkage knowledge in HPO and in the PPI network; the interacted genes in PPI are more likely to be associated with the same phenotype and the connected phenotypes in HPO are more likely to be associated with the same gene. Second, Gene Ontology (GO), which contains the association information between gene functions and genes, is used as the source domain. Additional regularizers are designed, and the PPI network and the common genes are used as the bridge for knowledge transfer . The gene-GO term and gene-HPO phenotype associations are constructed simultaneously fo r all the genes in the PPI network. By transferring additional knowledge, the predicted gene-phenotype associations can be more reliable. T ransfer learning can also be applied to solving the PPI prediction problems. Xu et al. [176] proposed an approach to transfer the linkage knowledge from the source PPI network to the target one. The proposed approach is based on the collective matrix factorization technique [177], wh ere a factor matrix is shared across domains. 6.3 T ransportation Application One application of transfer learning in the transportation area is to understand the trafﬁc scene images. In this applic a- tion, a challenge problem is that the images taken from a cer- tain location often suffer from variations because of diffe rent weather and light conditions. In order to solve this problem , Di et al. proposed an approach that attempts to transfer the information of the images that were taken from the same location in different conditions [178]. In the ﬁrst step, a p re- trained network is ﬁnetuned to extract the feature represen - tations of images. In the second step, the feature transfor- mation strategy is adopted to construct a new feature rep- resentation. Speciﬁcally , the dimension reduction algori thm (i.e., partial least squares regression [179]) is performe d on the extracted features to generate low-dimension features . Then, a transformation matrix is learned to minimize the domain discrepancy of the dimension-reduced data. Next, the subspace alignment operations are adopted to further reduce the domain discrepancy . Note that, although images under different conditions often have different appearanc es, they often have the similar layout structure. Therefore, in the ﬁnal step, the cross-domain dense correspondences are established between the test image and the retrieved best matching image at ﬁrst, and then the annotations of the best matching image are transferred to the test image via the Markov random ﬁeld model [180], [181]. T ransfer learning can also be applied to the task of driver behavior modeling. In this task, sufﬁcient personalized da ta of each individual driver are usually unavailable. In such situations, transferring the knowledge contained in the hi s- torical data for the newly-involved driver is a promising alternative. For example, Lu et al. proposed an approach to driver model adaptation in lane-changing scenarios [182]. The source domain contains the sufﬁcient data describing the behavior of the source drivers, while the target domain has a few numbers of data about the target driver . In the ﬁrst step, the data from both domains are pre-processed by performing PCA to generate low-dimension features. The authors assume that the source and the target data are from two manifolds. Therefore, in the second step, a manifold alignment approach is adopted for domain adap- tation. Speciﬁcally , the dynamic time warping algorithm [183] is applied to measuring similarity and ﬁnding the corresponding source-domain data point of each target- domain data point. Then, local Procrustes analysis [184] is adopted to align the two manifolds based on the obtained correspondences between data points. In this way , the data from the source domain can be transferred to the target domain. And in the ﬁnal step, a stochastic modeling method (e.g., Gaussian mixture regression [185]) is used to model 22 the behavior of the target driver . The experimental results demonstrate that the transfer learning approach can help the target driver even when few target-domain data are available. Besides, the results also show that when the number of target instances are very small or very large, the superiority of their approach is not obvious. This may because the relationship across domains cannot be found exactly with few target-domain instances, and in the case of sufﬁcient target-domain instances, the necessity of trans fer learning is reduced. Besides, there are some other applications of transfer learning in the transportation area. For example, Liu et al. applied transfer learning to driver poses recognition [186 ]. W ang et al. adopted the regularization technique in transfer learning for vehicle type recognition [187]. T ransfer lear ning can also be utilized for anomalous activity detection [188] , [189], trafﬁc sign recognition [190], etc. 6.4 Recommender-System Application Due to the rapid increase of the amount of information, how to effectively recommend the personalized content for individual users is an important issue. In the ﬁeld of recommender systems, some traditional recommendation methods, e.g., factorization-based collaborative ﬁlteri ng, of- ten rely on the factorization of the user-item interaction matrix to obtain the predictive function. These methods often require a large amount of training data to make accurate recommendations. However , the necessary trainin g data, e.g., the historical interaction data, are often spar se in real-world scenarios. Besides, for new registered users or new items, traditional methods are often hard to make effective recommendations, which is also known as the cold- start problem. Recognizing the above-mentioned problems in recom- mender systems, kinds of transfer learning approaches, e.g ., instance-based and feature-based approaches, have been proposed. These approaches attempt to make use of the data from other recommender systems (i.e., the source domains) to help construct the recommender system in the target domain. Instance-based approaches mainly focus on transferring different types of instances, e.g., rating s, feedbacks, and examinations, from the source domain to the target domain. The work by Pan et al. [191] leverages the uncertain ratings (represented as rating distribution s) of the source domain for knowledge transfer . Speciﬁcally , the source-domain uncertain ratings are used as constraints to help complete the rating matrix factorization task on the target domain. Hu et al. [192] proposed an approach termed transfer meeting hybrid, which extracts the knowledge from unstructured text by using an attentive memory network and selectively transfer the useful information. Feature-based approaches often leverage and transfer the information from a latent feature space. For example, Pan et al. proposed an approach termed Coordinate System T ransfer (CST) [193] to leverage both the user-side and the item-side latent features. The source-domain instances co me from another recommender system, sharing common users and items with the target domain. CST is developed based on the assumption that the principle coordinates, which reﬂect the tastes of users or the factors of items, character - ize the domain-independent structure and are transferable across domains. CST ﬁrst constructs two principle coordi- nate systems, which are actually the latent features of user s and items, by applying sparse matrix tri-factorization on the source-domain data, and then transfer the coordinate systems to the target domain by setting them as constraints. The experimental results show that CST signiﬁcantly out- performs the non-transfer baselines (i.e., average ﬁlling model and latent factorization model) in all data sparsity levels [193]. There are some other studies about cross-domain rec- ommendation [194], [195], [196], [197]. For example, He et al. proposed a transfer learning framework based on the Bayesian neural network [198]. Zhu et al. [199] proposed a deep framework, which ﬁrst generates the user and item feature representations based on the matrix factorization technique, and then employs a deep neural network to learn the mapping of features across domains. Y uan et al. [200] proposed a deep domain adaptation approach based on autoencoders and a modiﬁed DANN [154], [155] to extract and transfer the instances from rating matrices. 6.5 Other Applications Communication Application : In addition to WiFi localiza- tion tasks [2], [36], transfer learning has also been employ ed in wireless-network applications. For example, Bastug et al. proposed a caching mechanism [201]; the knowledge contained in contextual information, which is extracted fr om the interactions between devices, is transferred to the tar get domain. Besides, some studies focus on the energy saving problems. The work by Li et al. proposes an energy saving scheme for cellular radio access networks, which utilizes the transfer-learning expertise [202]. The work by Zhao and Grace applies transfer learning to topology management for reducing energy consumption [203]. Urban-Computing Application : With a large amount of data related to our cities, urban-computing is a promis- ing researching track in directions of trafﬁc monitoring, health care, social security , etc. T ransfer learning has be en applied to alleviate the data scarcity problem in many urban computing applications. For example, Guo et al. [204] proposed an approach for chain store site recommendation, which leverages the knowledge from semantically-relevant domains (e.g., other cities with the same store and other chain stores in the target city) to the target city . W ei et al. [205] proposed a ﬂexible multi-modal transfer learning approach that transfers knowledge from a city that have sufﬁcient multi-model data and labels to the target city to alleviate the data sparsity problem. T ransfer learning has been applied to some recognition tasks such as hand gesture recognition [206], face recogni- tion [207], activity recognition [208], and speech emotion recognition [209]. Besides, transfer-learning expertise has also been incorporated into some other areas such as sen- timent analysis [28], [96], [210], fraud detection [211], s ocial network [212], and hyperspectral image analysis [54], [213 ]. 7 E XPERIMENT T ransfer learning techniques have been successfully appli ed in many real-world applications. In this section, we perfor m 23 experiments to evaluate the performance of some represen- tative transfer learning models 1 [214] of different categories on two mainstream research areas, i.e., object recognition and text classiﬁcation. The datasets are introduced at ﬁrst . Then, the experimental results and further analyses are provided. 7.1 Dataset and Preprocessing Three datasets are studied in the experiments, i.e., Ofﬁce- 31, Reuters-21578, and Amazon Reviews. For simplicity , we focus on the classiﬁcation tasks. The statistical informat ion of the preprocessed datasets is listed in T able 3.  Amazon Reviews 2 [107] is a multi-domain sentiment dataset which contains product reviews taken from Ama- zon.com of four domains (Books, Kitchen, Electronics and DVDs). Each review in the four domains has a text and a rating from zero to ﬁve. In the experiments, the ratios that are less than three are deﬁned as the negative ones, while others are deﬁned as the positive ones. The frequency of each word in all reviews is calculated. Then, the ﬁve thousand words with the highest frequency are selected as the attributes of each review . In this way , we ﬁnally have 1000 positive instances, 1000 negative instances, and about 5000 unlabeled instances in each domain. In the experiments, every two of the four domains are selected to generate twelve tasks.  Reuters-215783 is a dataset for text categorization, which has a hierarchical structure. The dataset contains 5 top categories (Exchanges, Orgs, People, Places, T opics). In out experiment, we use the top three big category Orgs, People and Places to generate three classiﬁcation tasks (Orgs vs People, Orgs vs Places and People vs Places). In each task, the subcategories in the corresponding two categories are separately divided into two parts. Then, the resultant four parts are used as the components to form two domains. Each domain has about 1000 instances, and each instance has about 4500 features. Speciﬁcally , taking the task Orgs vs People as an example, one part from Orgs and one part from People and combined to form the source domain; similarly , the rest two parts form the target domain. Note that the instances in the three categories are all labeled. In order to generate the unlabeled instances, the labeled instances are selected from the dataset, and their labels are ignored.  Ofﬁce-31 [215] is an object recognition dataset which contains thirty-one categories and three domains, i.e., Amazon, W ebcam, and DSLR. These three domains have 2817, 498, and 795 instances, respectively . The images in Amazon are the online e-commerce pictures taken from Amazon.com. The images in W ebcam are the low- resolution pictures taken by web cameras. And the im- ages in DSLR are the high-resolution pictures taken by DSLR cameras. In the experiments, every two of the three domains (with the order considered) are selected as the source and the target domains, which results in six tasks. 1. https:github.comFuzhenZhuangT ransfer-Learning-T oolkit 2. http:www .cs.jhu.edu mdredzedatasetssentiment 3. https:archive.ics.uci.edumldatasetsReuters- 21578T extCategorizationCollection Model .ĺ .ĺ .ĺ( ĺ. ĺ ĺ( ĺ. ĺ ĺ( (ĺ. (ĺ (ĺ HIDC 0.88 0.875 0.88 0.7925 0.81 0.8025 0.7925 0.8175 0.8075 0.8075 0.87 0.87 濃濁濋濆濆濋 TriTL 0.715 0.725 0.6775 0.5725 0.525 0.5775 0.615 0.6125 0. 6 0.625 0.61 0.615 濃濁濉濅濅濈 CD-PLSA 0.7475 0.7225 0.72 0.6075 0.6175 0.6075 0.575 0.61 0 .6425 0.7225 0.745 0.7 濃濁濉濉濋濄 MTrick 0.82 0.835 0.8125 0.7725 0.7475 0.7275 0.755 0.745 0.7 8 0.79 0.7975 0.81 濃濁濊濋濅濊 SFA 0.8525 0.8575 0.8675 0.7825 0.805 0.775 0.7925 0.785 0.7 775 0.84 0.8525 0.84 濃濁濋濄濌濃 mSLDA 0.7975 0.7825 0.7925 0.635 0.645 0.6325 0.6525 0.6675 0.6625 0.7225 0.715 0.7125 濃濁濊濃濄濈 SDA 0.8425 0.7925 0.8025 0.745 0.76 0.765 0.7625 0.7475 0.74 25 0.8175 0.805 0.81 濃濁濊濋濅濊 GFK 0.62 0.6275 0.6325 0.62 0.61 0.6225 0.58 0.565 0.5725 0.6 575 0.65 0.6325 濃濁濉濄濈濋 SCL 0.8575 0.8625 0.8725 0.78 0.785 0.7825 0.7925 0.7925 0.7 825 0.8425 0.8525 0.845 濃濁濋濅濃濉 TCA 0.755 0.755 0.755 0.6475 0.6475 0.65 0.58 0.5825 0.585 0. 7175 0.715 0.7125 濃濁濉濊濈濅 Baseline 0.727 0.709 0.827 0.74 0.728 0.73 0.745 0.772 0.708 0 .84 0.706 0.707 濃濁濊濇濇濌 .ĺ .ĺ .ĺ( ĺ. ĺ ĺ( ĺ. ĺ ĺ( (ĺ. (ĺ (ĺ HIDC TriTL CD-PLSA MTrick SFA mSLDA SDA GFK SCL TCA Baseline Fig. 5. Comparison results on Amazon Reviews. 7.2 Experiment Setting Experiments are conducted to compare some representative transfer learning models. Speciﬁcally , eight algorithms a re performed on the dataset Ofﬁce-31 for solving the object recognition problem. Besides, fourteen algorithms are per - formed and evaluated on the dataset Reuters-21578 for solving the text classiﬁcation problem. In the sentiment classiﬁcation problem, eleven algorithms are performed on Amazon Reviews. The classiﬁcation results are evaluated by accuracy , which is deﬁned as follows: accuracy  {xxi Dtest f(xi)  yi} Dtest  where Dtest denotes the test data and y denotes the truth classiﬁcation label; f(x) represents the predicted classiﬁca- tion result. Note that some algorithms need the base classi- ﬁer . In these cases, an SVM with a linear kernel is adopted as the base classiﬁer in the experiments. Besides, the sourc e- domain instances are all labeled. And for the performed al- gorithms (except T rAdaBoost), the target-domain instance s are unlabeled. Each algorithm was executed three times, and the average results are adopted as our experimental results . The evaluated transfer learning models include: HIDC [93], T riTL [123], CD-PLSA [91], [92], MT rick [122], SF A [106], mSLDA [98], [99], SDA [96], GFK [102], SCL [94], TCA [36], [78], CoCC [41], JDA [38], T rAdaBoost [31], DAN [137], DCORAL [148], MRAN [152], CDAN [158], DANN [154], [155], JAN [147], and CAN [151]. 7.3 Experiment Result In this subsection, we compare over twenty algorithms on three datasets in total. The parameters of all algorithms are set to the default values or the recommended values mentioned in the original papers. The experimental results are presented in T ables 4, 5, and 6 corresponding to Amazon Reviews, Reuters-21578, and Ofﬁce-31, respectively . In or der to allow readers to understand the experimental results more intuitively , three radar maps, i.e., Figs. 5, 6, and 7, a re provided, which visualize the experimental results. In the radar maps, each direction represents a task. The general performance of an algorithm is demonstrated by a polygon 24 T ABLE 3 Statistical information of the preprocessed datasets. Area Dataset Domain Attribute T otal Instances T asks Sentiment Classiﬁcation Amazon Reviews 4 5000 27677 12 T ext Classiﬁcation Reuters-21578 3 4772 6570 3 Object Recognition Ofﬁce-31 3 800 4110 6 Orgs vs Places People vs Places Orgs vs People HIDC 0.7698 0.6945 0.8375 GFK 0.622 0.5417 0.6446 CD-PLSA 0.5624 0.5749 0.7826 MTrick 0.7494 0.6457 0.793 CoCC 0.6704 0.8264 0.7644 SFA 0.7468 0.6768 0.7906 mSLDA 0.5645 0.6064 0.5289 Orgs vs Places People vs Places Orgs vs People HIDC GFK CD-PLSA MTrick CoCC SFA mSLDA Orgs vs Places People vs Places Orgs vs People SDA 0.6603 0.5556 0.5992 濃濁濉濃濈濃 TriTL 0.7338 0.5517 0.7505 濃濁濉濊濋濊 SCL 0.6794 0.5046 0.6694 濃濁濉濄濊濋 TCA 0.7368 0.6065 0.7562 濃濁濉濌濌濋 JDA 0.5694 0.6296 0.7424 濃濁濉濇濊濄 TrAdaBoost 0.7336 0.7052 0.7879 濃濁濊濇濅濅 Baseline 0.6683 0.5198 0.6696 HIDC 0.7698 0.6945 0.8375 濃濁濊濉濊濆 GFK 0.622 0.5417 0.6446 濃濁濉濃濅濋 CD-PLSA 0.5624 0.5749 0.7826 濃濁濉濇濃濃 MTrick 0.7494 0.6457 0.793 濃濁濊濅濌濇 CoCC 0.6704 0.8264 0.7644 濃濁濊濈濆濊 SFA 0.7468 0.6768 0.7906 濃濁濊濆濋濄 mSLDA 0.5645 0.6064 0.5289 濃濁濈濉濉濉 Orgs vs Places People vs Places Orgs vs People SDA TriTL SCL TCA JDA TrAdaBoost Baseline Fig. 6. Comparison results on Reuters-21578. T ABLE 4 Accuracy performance on the Amazon Reviews of four domains: Kitchen (K), Electronics (E), D VDs (D) and Books (B). Model K  D K  B K  E D  K D  B D  E B  K B  D B  E E  K E  D E  B A verage HIDC 0.8800 0.8750 0.8800 0.7925 0.8100 0.8025 0.7925 0.817 5 0.8075 0.8075 0.8700 0.8700 0.8338 T riTL 0.7150 0.7250 0.6775 0.5725 0.5250 0.5775 0.6150 0.61 25 0.6000 0.6250 0.6100 0.6150 0.6225 CD-PLSA 0.7475 0.7225 0.7200 0.6075 0.6175 0.6075 0.5750 0.6 100 0.6425 0.7225 0.7450 0.7000 0.6681 MT rick 0.8200 0.8350 0.8125 0.7725 0.7475 0.7275 0.7550 0.7 450 0.7800 0.7900 0.7975 0.8100 0.7827 SF A 0.8525 0.8575 0.8675 0.7825 0.8050 0.7750 0.7925 0.7850 0 .7775 0.8400 0.8525 0.8400 0.8190 mSLDA 0.7975 0.7825 0.7925 0.6350 0.6450 0.6325 0.6525 0.667 5 0.6625 0.7225 0.7150 0.7125 0.7015 SDA 0.8425 0.7925 0.8025 0.7450 0.7600 0.7650 0.7625 0.7475 0 .7425 0.8175 0.8050 0.8100 0.7827 GFK 0.6200 0.6275 0.6325 0.6200 0.6100 0.6225 0.5800 0.5650 0.5725 0.6575 0.6500 0.6325 0.6158 SCL 0.8575 0.8625 0.8725 0.7800 0.7850 0.7825 0.7925 0.7925 0 .7825 0.8425 0.8525 0.8450 0.8206 TCA 0.7550 0.7550 0.7550 0.6475 0.6475 0.6500 0.5800 0.5825 0.5850 0.7175 0.7150 0.7125 0.6752 Baseline 0.7270 0.7090 0.8270 0.7400 0.7280 0.7300 0.7450 0 .7720 0.7080 0.8400 0.7060 0.7070 0.7449 whose vertices representing the accuracy of the algorithm for dealing with different tasks. T able 4 shows the experimental results on Amazon Re- views. The baseline is a linear classiﬁer trained only on the source domain (here we directly use the results from the paper [107]). Fig. 5 visualizes the results. As shown in Fig. 5, most algorithms are relatively well-performed when the source domain is electronics or kitchen, which indicate s that these two domains may contains more transferable information than the other two domains. In addition, it can be observed that HIDC, SCL, SF A, MT rick and SDA perform well and relatively stable in all the twelve tasks. Meanwhil e, other algorithms, especially mSLDA, CD-PLSA, and T riTL, are relatively unstable; the performance of them ﬂuctuates in a range about twenty percent. T riTL has a relatively high accuracy on the tasks where the source domain is kitchen, but has a relatively low accuracy on other tasks. The algorithms TCA, mSLDA, and CD-PLSA have similar performance on all the tasks with an accuracy about seventy percent on average. Among the well-performed algorithms, HIDC and MT rick are based on feature reduction (feature clustering), while the others are based on feature encoding (SDA), feature alignment (SF A), and feature selection (SCL ). Those strategies are currently the mainstreams of feature- based transfer learning. T able 5 presents the comparison results on Reuter-21578 (here we directly use the results of the baseline and CoCC from papers [78] and [41]). The baseline is a regularized lea st square regression model trained only on the labeled target domain instances [78]. Fig. 6, which has the same structure of Fig. 5, visualizes the performance. For clarity , thirtee n algorithms are divided into two parts that correspond to the two subﬁgures in Fig. 6. It can be observed that most algorithms are relatively well-performed for Orgs vs Place s 25 Method ĺ: ĺ: :ĺ ĺ ĺ :ĺ DAN 0.826 0.977 0.831 0.668 0.666 0.828 DCORAL 0.79 0.98 0.827 0.653 0.645 0.816 MRAN 0.914 0.969 0.998 0.864 0.683 0.709 0.856 CDAN 0.931 0.982 0.898 0.701 0.68 0.865 DANN 0.826 0.978 0.833 0.668 0.661 0.828 JAN 0.854 0.974 0.998 0.847 0.686 0.7 0.843 CAN 0.945 0.991 0.998 0.95 0.78 0.77 0.906 Baseline 0.616 0.954 0.99 0.638 0.511 0.498 0.701166667 ĺ: ĺ: :ĺ ĺ ĺ :ĺ DAN DCORAL MRAN CDAN DANN JAN CAN Baseline Fig. 7. Comparison results on Ofﬁce-31. T ABLE 5 Accuracy performance on the Reuters-21578 of three domains : Orgs, People, and Places. Model Orgs vs Places People vs Places Orgs vs People A verage HIDC 0.7698 0.6945 0.8375 0.7673 T riTL 0.7338 0.5517 0.7505 0.6787 CD-PLSA 0.5624 0.5749 0.7826 0.6400 MT rick 0.7494 0.6457 0.7930 0.7294 CoCC 0.6704 0.8264 0.7644 0.7537 SF A 0.7468 0.6768 0.7906 0.7381 mSLDA 0.5645 0.6064 0.5289 0.5666 SDA 0.6603 0.5556 0.5992 0.6050 GFK 0.6220 0.5417 0.6446 0.6028 SCL 0.6794 0.5046 0.6694 0.6178 TCA 0.7368 0.6065 0.7562 0.6998 JDA 0.5694 0.6296 0.7424 0.6471 T rAdaBoost 0.7336 0.7052 0.7879 0.7422 Baseline 0.6683 0.5198 0.6696 0.6192 T ABLE 6 Accuracy performance on Ofﬁce-31 of three domains: Amazon ( A), Webcam (W), and DSLR (D). Model A  W D  W W  D A  D D  A W  A A verage DAN 0.826 0.977 1.00 0.831 0.668 0.666 0.828 DCORAL 0.790 0.980 1.00 0.827 0.653 0.645 0.816 MRAN 0.914 0.969 0.998 0.864 0.683 0.709 0.856 CDAN 0.931 0.982 1.00 0.898 0.701 0.680 0.865 DANN 0.826 0.978 1.00 0.833 0.668 0.661 0.828 JAN 0.854 0.974 0.998 0.847 0.686 0.700 0.843 CAN 0.945 0.991 0.998 0.950 0.780 0.770 0.906 Baseline 0.616 0.954 0.990 0.638 0.511 0.498 0.701 and Orgs vs People, but poor for People vs Places. This phe- nomenon indicates that the discrepancy between People and Places may be relatively large. T rAdaBoost has a relatively good performance in this experiment because it uses the labels of the instances in the target domain to reduce the impact of the distribution difference. Besides, the algori thms HIDC, SF A, and MT rick have relatively consistent perfor- mance in the three tasks. These algorithms are also well- performed in the previous experiment on Amazon Reviews. In addition, the top two well-performed algorithms in terms of People vs Places are CoCC and T rAdaBoost. In the third experiment, seven deep-learning-based transfer learning models (i.e., DAN, DCORAL, MRAN, CDAN, DANN, JAN, and CAN) and the baseline (i.e., the Alexnet [138], [140] pre-trained on ImageNet [166] and then directly trained on the target domain) are performed on the dataset Ofﬁce-31 (here we directly use the results of CDAN, JAN, CAN, and the baseline from the original papers [137], [147], [151], [158]). The ResNet-50 [144] is used as the back - bone network for all these three models. The experimental results are provided in T able 6 and the average performance is visualized in Fig. 7. As shown in Fig. 7, all of these seven algorithms have excellent performance, especially o n the tasks D W and W D, whose accuracy is very close to one hundred percent. This phenomenon reﬂects the superiority of the deep-learning based approaches, and is consistent with the fact that the difference between W ebcam and DSLR is smaller than that between W ebcamDSLR and Amazon. Clearly , CAN outperforms the other six al- gorithms. In all the six tasks, the performance of DANN is similar to that of DAN, and is better than that of DCORAL, which indicates the effectiveness and the practicability o f incorporating adversarial learning. It is worth mentioning that, in the above experiments, the performance of some algorithms is not ideal. One reason is that we use the default parameter settings provided in the algorithms original papers, which may not be suitable for the dataset we selected. For example, GFK was originally designed for object recognition, and we directly adopt it into text classiﬁcation in the ﬁrst experiment, which turns out to produce an unsatisfactory result (having about sixty - two percent accuracy on average). The above experimental results are just for reference. These results demonstrate t hat some algorithms may not be suitable for the datasets of certain domains. Therefore, it is important to choose the appropriate algorithms as the baselines in the process of re - search. Besides, in practical applications, it is also nece ssary to ﬁnd a suitable algorithm. 8 C ONCLUSION AND FUTURE DIRECTION In this survey paper , we have summarized the mechanisms and the strategies of transfer learning from the perspectiv es of data and model. The survey gives the clear deﬁnitions about transfer learning and manages to use a uniﬁed sym- bol system to describe a large number of representative transfer learning approaches and related works. W e have basically introduced the objectives and strategies in tran sfer learning based on data-based interpretation and model- based interpretation. Data-based interpretation introdu ces the objectives, the strategies, and some transfer learning 26 approaches from the data perspective. Similarly , model- based interpretation introduces the mechanisms and the strategies of transfer learning but from the model level. The applications of transfer learning have also been intro- duced. At last, experiments have been conducted to evaluate the performance of representative transfer learning model s on two mainstream area, i.e., object recognition and text categorization. The comparisons of the models have also been given, which reﬂects that the selection of the transfer learning model is an important research topic as well as a complex issue in practical applications. Several directions are available for future research in the transfer learning area. First, transfer learning techn iques can be further explored and applied to a wider range of applications. And new approaches are needed to solve the knowledge transfer problems in more complex scenarios. For example, in real-world scenarios, sometimes the user- relevant source-domain data comes from another company . In this case, how to transfer the knowledge contained in the source domain while protecting user privacy is an important issue. Second, how to measure the transferability across do - mains and avoid negative transfer is also an important issue . Although there have been some studies on negative transfer , negative transfer still needs further systematic analyses [3]. Third, the interpretability of transfer learning also need s to be investigated further [216]. Finally , theoretical stu dies can be further conducted to provide theoretical support for the effectiveness and applicability of transfer learning. As a popular and promising area in machine learning, transfer learning shows some advantages over traditional machine learning such as less data dependency and less label depen- dency . W e hope our work can help readers have a better understanding of the research status and the research ideas . ACKNOWLEDGME NT S The research work is supported by the National Key Re- search and Development Program of China under Grant No. 2018YFB1004300, the National Natural Science Foundation of China under Grant No. U1836206, U1811461, 61773361, 61836013, and the Project of Y outh Innovation Promotion Association CAS under Grant No. 2017146. REFERENCES [1] D.N. Perkins and G. Salomon, T ransfer of Learning. Oxford, England: Pergamon, 1992. [2] S.J. Pan and Q. Y ang, A survey on transfer learning, IEEE T rans. Knowl. Data Eng. , vol. 22, no. 10, pp. 13451359, Oct. 2010. [3] Z. W ang, Z. Dai, B. Poczos, and J. Carbonell, Characterizing and avoiding negative transfer , in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Long Beach, Jun. 2019, pp. 11293 11302. [4] K. W eiss, T .M. Khoshgoftaar , and D. W ang, A survey of tra nsfer learning, J. Big Data , vol. 3, no. 1, Dec. 2016. [5] J. Huang, A.J. Smola, A. Gretton, K.M. Borgwardt, and B. Sch  olkopf, Correcting sample selection bias by unlabeled data, in Proc. 20th Annual Conference on Neural Information Processing Sy stems, V ancouver , Dec. 2006, pp. 601608. [6] M. Sugiyama, T . Suzuki, S. Nakajima, H. Kashima, P . Bnau, and M. Kawanabe, Direct importance estimation for covariate s hift adaptation, Ann. Inst. Stat. Math. , vol. 60, no. 4, pp. 699746, Dec. 2008. [7] O. Day and T .M. Khoshgoftaar , A survey on heterogeneous trans- fer learning, J. Big Data , vol. 4, no. 1, Dec. 2017. [8] M.E. T aylor and P . Stone, T ransfer learning for reinforc ement learning domains: A survey , J. Mach. Learn. Res. , vol. 10, pp. 1633 1685, Sep. 2009. [9] H.B. Ammar , E. Eaton, J.M. Luna, and P . Ruvolo, Autonomo us cross-domain knowledge transfer in lifelong policy gradie nt rein- forcement learning, in Proc. 24th International Joint Conference on Artiﬁcial Intelligence , Buenos Aires, Jul. 2015, pp. 33453351. [10] P . Zhao and S.C.H. Hoi, OTL: A framework of online transf er learning, in Proc. 27th International Conference on Machine Learning , Haifa, Jun. 2010, pp. 12311238. [11] O. Chapelle, B. Schlkopf, and A. Zien, Semi-supervised Learning . Cambridge: MIT Press, 2010. [12] S. Sun, A survey of multi-view machine learning, Neural Com- put. Appl. , vol. 23, no. 78, pp. 20312038, Dec. 2013. [13] C. Xu, D. T ao, and C. Xu, A survey on multi-view learning , 2013, arXiv:1304.5634v1. [14] J. Zhao, X. Xie, X. Xu, and S. Sun, Multi-view learning ove rview: Recent progress and new challenges, Inf. Fusion , vol. 38, pp. 4354, Nov . 2017. [15] D. Zhang, J. He, Y . Liu, L. Si, and R. Lawrence, Multi-vie w transfer learning with a large margin approach, in Proc. 17th ACM SIGKDD International Conference on Knowledge Discovery an d Data Mining, San Diego, Aug. 2011, pp. 12081216. [16] P . Y ang and W . Gao, Multi-view discriminant transfer l earning, in Proc. 23rd International Joint Conference on Artiﬁcial Int elligence, Beijing, Aug. 2013, pp. 18481854. [17] K.D. Feuz and D.J. Cook, Collegial activity learning b etween heterogeneous sensors, Knowl. Inf. Syst. , vol. 53, pp. 337364, Mar . 2017. [18] Y . Zhang and Q. Y ang, An overview of multi-task learnin g, Natl. Sci. Rev . , vol. 5, no. 1, pp. 3043, Jan. 2018. [19] W . Zhang, R. Li, T . Zeng, Q. Sun, S. Kumar , J. Y e, and S. Ji, De ep model based transfer and multi-task learning for biologica l image analysis, in Proc. 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Sydney , Aug. 2015, pp. 1475 1484. [20] A. Liu, N. Xu, W . Nie, Y . Su, and Y . Zhang, Multi-domain an d multi-task learning for human action recognition, IEEE T rans. Image Process. , vol. 28, no. 2, pp. 853867, Feb. 2019. [21] X. Peng, Z. Huang, X. Sun, and K. Saenko, Domain agnostic learning with disentangled representations, in Proc. 36th Interna- tional Conference on Machine Learning , Long Beach, Jun. 2019, pp. 51025112. [22] J. Lu, V . Behbood, P . Hao, H. Zuo, S. Xue, and G. Zhang, T ra nsfer learning using computational intelligence: A survey , Knowledge- Based Syst. , vol. 80, pp. 1423, May 2015. [23] C. T an, F . Sun, T . Kong, W . Zhang, C. Y ang, and C. Liu, A Surv ey on deep transfer learning, in Proc. 27th International Conference on Artiﬁcial Neural Networks , Rhodes, Oct. 2018, pp. 270279. [24] M. W ang and W . Deng, Deep visual domain adaptation: A survey , Neurocomputing, vol. 312, pp. 135153, Oct. 2018. [25] D. Cook, K.D. Feuz, and N.C. Krishnan, T ransfer learni ng for activity recognition: A survey , Knowl. Inf. Syst. , vol. 36, no. 3, pp. 537556, Sep. 2013. [26] L. Shao, F . Zhu, and X. Li, T ransfer learning for visual c ategoriza- tion: A survey , IEEE T rans. Neural Netw . Learn. Syst. , vol. 26, no. 5, pp. 10191034, May 2015. [27] W . Pan, A survey of transfer learning for collaborativ e recom- mendation with auxiliary data, Neurocomputing, vol. 177, pp. 447 453, Feb. 2016. [28] R. Liu, Y . Shi, C. Ji, and M. Jia, A Survey of sentiment anal ysis based on transfer learning, IEEE Access , vol. 7, pp. 8540185412, Jun. 2019. [29] Q. Sun, R. Chattopadhyay , S. Panchanathan, and J. Y e, A tw o- stage weighting framework for multi-source domain adaptat ion, in Proc. 25th Annual Conference on Neural Information Process ing Systems, Granada, Dec. 2011, pp. 505513. [30] M. Belkin, P . Niyogi, and V . Sindhwani, Manifold regula rization: A geometric framework for learning from labeled and unlabel ed examples, J. Mach. Learn. Res. , vol. 7, pp. 23992434, Nov . 2006. [31] W . Dai, Q. Y ang, G. Xue, and Y . Y u, Boosting for transfer learning, in Proc. 24th International Conference on Machine Learning , Corvalis, Jun. 2007, pp. 193200. [32] Y . Freund and R.E. Schapire, A decision-theoretic gene ralization of on-line learning and an application to boosting, J. Comput. Syst. Sci., vol. 55, no. 1, pp. 119139, Aug. 1997. 27 [33] Y . Y ao and G. Doretto, Boosting for transfer learning w ith multi- ple sources, in Proc. IEEE Conference on Computer V ision and Pattern Recognition, San Francisco, Jun. 2010, pp. 18551862. [34] J. Jiang and C. Zhai, Instance weighting for domain ada ptation in NLP , in Proc. 45th Annual Meeting of the Association of Computation al Linguistics, Prague, Jun. 2007, pp. 264271. [35] K.M. Borgwardt, A. Gretton, M.J. Rasch, H.-P . Kriegel, B. Scholkopf, and A.J. Smola, Integrating structured biologic al data by kernel maximum mean discrepancy , Bioinformatics, vol. 22, no. 14, pp. 4957, Jul. 2006. [36] S.J. Pan, I.W . T sang, J.T . Kwok, and Q. Y ang, Domain adap tation via transfer component analysis, IEEE T rans. Neural Netw . , vol. 22, no. 2, pp. 199210, Feb. 2011. [37] M. Ghifary , W .B. Kleijn, and M. Zhang, Domain adaptive neural networks for object recognition, in Proc. Paciﬁc Rim International Conference on Artiﬁcial Intelligence , Gold Coast, Dec. 2014, pp. 898 904. [38] M. Long, J. W ang, G. Ding, J. Sun, and P .S. Y u, T ransfer fea ture learning with joint distribution adaptation,in Proc. IEEE Interna- tional Conference on Computer V ision , Sydney , Dec. 2013, pp. 2200 2207. [39] M. Long, J. W ang, G. Ding, S.J. Pan, and P .S. Y u, Adaptatio n regularization: A general framework for transfer learning , IEEE T rans. Knowl. Data Eng. , vol. 26, no. 5, pp. 1076-1089, May 2014. [40] S. Kullback and R.A. Leibler , On information and sufﬁci ency , Ann. Math. Statist. , vol. 22, no. 1, pp. 7986, 1951. [41] W . Dai, G.-R. Xue, Q. Y ang, and Y . Y u, Co-clustering bas ed classiﬁcation for out-of-domain documents, in Proc. 13th ACM SIGKDD International Conference on Knowledge Discovery an d Data Mining, San Jose, Aug. 2007, pp. 210219. [42] W . Dai, Q. Y ang, G. Xue, and Y . Y u, Self-taught clusterin g, in Proc. 25th International Conference of Machine Learning , Helsinki, Jul. 2008, pp. 200207. [43] J. Davis and P . Domingos, Deep transfer via second-ord er Markov logic, in Proc. 26th International Conference on Machine Learning, Montreal, Jun. 2009, pp. 217224. [44] F . Zhuang, X. Cheng, P . Luo, S.J. Pan, and Q. He, Supervise d rep- resentation learning: T ransfer learning with deep autoenc oders, in Proc. 24th International Joint Conference on Artiﬁcial Int elligence, Buenos Aires, Jul. 2015, pp. 41194125. [45] I. Dagan, L. Lee, and F . Pereira, Similarity-based meth ods for word sense disambiguation, in Proc. 35th Annual Meeting of the Association of Computational Linguistics and 8th Conferen ce of the European Chapter of the Association for Computational Ling uistics (ACLEACL), Madrid, Jul. 1997, pp. 5663. [46] B. Chen, W . Lam, I. T sang, and T . W ong, Location and scat ter matching for dataset shift in text mining, in Proc. 10th IEEE International Conference on Data Mining , Sydney , Dec. 2010, pp. 773 778. [47] S. Dey , S. Madikeri, and P . Motlicek, Information theore tic clus- tering for unsupervised domain-adaptation, in Proc. IEEE Interna- tional Conference on Acoustics, Speech and Signal Processi ng, Shanghai, Mar . 2016, pp. 55805584. [48] W .-H. Chen, P .-C. Cho, and Y .-L. Jiang, Activity recog nition using transfer learning, Sens. Mater . , vol. 29, no. 7, pp. 897904, Jul. 2017. [49] J. Giles, K.K. Ang, L.S. Mihaylova, and M. Arvaneh, A sub ject- to-subject transfer learning framework based on Jensen-Sha nnon divergence for improving brain-computer interface, in Proc. IEEE International Conference on Acoustics, Speech and Signal P rocessing, Brighton, May 2019, pp. 30873091. [50] L.M. Bregman, The relaxation method of ﬁnding the comm on point of convex sets and its application to the solution of pr oblems in convex programming, USSR Comput. Math. Math. Phys. , vol. 7, no. 3, pp. 200217, 1967. [51] S. Si, D. T ao, and B. Geng, Bregman divergence-based regu lariza- tion for transfer subspace learning, IEEE T rans. Knowl. Data Eng. , vol. 22, no. 7, pp. 929942, Jul. 2010. [52] H. Sun, S. Liu, S. Zhou, and H. Zou, Unsupervised cross-vie w semantic transfer for remote sensing image classiﬁcation,  IEEE Geosci. Remote Sens. Lett. , vol. 13, no. 1, pp. 1317, Jan. 2016. [53] H. Sun, S. Liu, and S. Zhou, Discriminative subspace align ment for unsupervised visual domain adaptation, Neural Process. Lett. , vol. 44, no. 3, pp. 779793, Dec. 2016. [54] Q. Shi, Y . Zhang, X. Liu, and K. Zhao, Regularised transf er learning for hyperspectral image classiﬁcation, IET Comput. V is. , vol. 13, no. 2, pp. 188193, Feb. 2019. [55] A. Gretton, O. Bousquet, A.J. Smola, and B. Schlkopf, Mea suring statistical dependence with Hilbert-Schmidt norms, in Proc. 18th International Conference on Algorithmic Learning Theory , Singapore, Oct. 2005, pp. 6377. [56] H. W ang and Q. Y ang, T ransfer learning by structural an alogy , in Proc. 25th AAAI Conference on Artiﬁcial Intelligence , San Francisco, Aug. 2011, pp. 513518. [57] M. Xiao and Y . Guo, Feature space independent semi-sup ervised domain adaptation via kernel matching, IEEE T rans. Pattern Anal. Mach. Intell. , vol. 37, no. 1, pp. 5466, Jan. 2015. [58] K. Y an, L. Kou, and D. Zhang, Learning domain-invarian t sub- space using domain features and independence maximization , IEEE T . Cybern. , vol. 48, no. 1, pp. 288299, Jan. 2018. [59] J. Shen, Y . Qu, W . Zhang, and Y . Y u, W asserstein distance guided representation learning for domain adaptation, in Proc. 32nd AAAI Conference on Artiﬁcial Intelligence , New Orleans, Feb. 2018, pp. 40584065. [60] C.-Y . Lee, T . Batra, M.H. Baig, and D. Ulbricht, Sliced W asserstein discrepancy for unsupervised domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Long Beach, Jun. 2019, pp. 1028510295. [61] W . Zellinger , T . Grubinger , E. Lughofer , T . Natschlger , and S. Saminger-Platz, Central moment discrepancy (CMD) for doma in- invariant representation learning, in Proc. 5th International Confer- ence on Learning Representations , T oulon, Apr . 2017, pp. 113. [62] A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan , M. Pon- til, K. Fukumizu, and B.K. Sriperumbudur , Optimal kernel ch oice for large-scale two-sample tests, in Proc. 26th Annual Conference on Neural Information Processing Systems , Lake T ahoe, Dec. 2012, pp. 12051213. [63] H. Y an, Y . Ding, P . Li, Q. W ang, Y . Xu, and W . Zuo, Mind the class weight bias: W eighted maximum mean discrepancy for un su- pervised domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Honolulu, Jul. 2017, pp. 22722281. [64] H. Daum  e III, Frustratingly easy domain adaptation,  in Proc. 45th Annual Meeting of the Association for Computational Li nguistics, Prague, Jun. 2007, pp. 256263. [65] H. Daum  e III, A. Kumar , and A. Saha, Co-regularization based semi-supervised domain adaptation, in Proc. 24th Annual Confer- ence on Neural Information Processing Systems , V ancouver , Dec. 2010, pp. 478486. [66] L. Duan, D. Xu, and I.W . T sang, Learning with augmented features for heterogeneous domain adaptation, in Proc. 29th Inter- national Conference on Machine Learning , Edinburgh, Jun. 2012, pp. 18. [67] W . Li, L. Duan, D. Xu, and I.W . T sang, Learning with augm ented features for supervised and semi-supervised heterogeneou s do- main adaptation, IEEE T rans. Pattern Anal. Mach. Intell. , vol. 36, no. 6, pp. 11341148, Jun. 2014. [68] K.I. Diamantaras and S.Y . Kung, Principal Component Neural Net- works. New Y ork: Wiley , 1996. [69] B. Schlkopf, A. Smola, and K. Mller , Nonlinear component anal- ysis as a kernel eigenvalue problem, Neural Comput. , vol. 10, no. 5, pp. 12991319, Jul. 1998. [70] J. W ang, Y . Chen, S. Hao, W . Feng, and Z. Shen, Balanced distribution adaptation for transfer learning, in Proc. 17th IEEE International Conference on Data Mining , New Orleans, Nov . 2017, pp. 11291134. [71] A. Blum and T . Mitchell, Combining labeled and unlabel ed data with co-training, in Proc. 11th Annual Conference on Computational Learning Theory , Madison, Jul. 1998, pp. 92100. [72] M. Chen, K.Q. W einberger , and J.C. Blitzer , Co-traini ng for domain adaptation, in Proc. 25th Annual Conference on Neural Information Processing Systems , Granada, Dec. 2011, pp. 24562464. [73] Z.-H. Zhou and M. Li, T ri-training: Exploiting unlabe led data using three classiﬁers, IEEE T rans. Knowl. Data Eng. , vol. 17, no. 11, pp. 15291541, Nov . 2005. [74] K. Saito, Y . Ushiku, and T . Harada, Asymmetric tri-trai ning for unsupervised domain adaptation, in Proc. 34th International Conference on Machine Learning , Sydney , Aug. 2017, pp. 29882997. [75] S.J. Pan, J.T . Kwok, and Q. Y ang, T ransfer learning via d imen- sionality reduction, in Proc. 23rd AAAI Conference on Artiﬁcial Intelligence, Chicago, Jul. 2008, pp. 677682. [76] K.Q. W einberger , F . Sha, and L.K. Saul, Learning a kernel matrix for nonlinear dimensionality reduction, in Proc. 21st International Conference on Machine Learning , Banff, Jul. 2004, pp. 106113. 28 [77] L. V andenberghe and S. Boyd, Semideﬁnite programming, SIAM Rev . , vol. 38, no. 1, pp. 4995, Mar . 1996. [78] S.J. Pan, I.W . T sang, J.T . Kwok, and Q. Y ang, Domain adap tation via transfer component analysis, in Proc. 21st International Joint Conference on Artiﬁcial Intelligence , Pasadena, Jul. 2009, pp. 1187 1192. [79] C. Hou, Y .H. T sai, Y . Y eh, and Y .F . W ang, Unsupervised d omain adaptation with label and structural consistency , IEEE T rans. Image Process., vol. 25, no. 12, pp. 55525562, Dec. 2016. [80] J. T ahmoresnezhad and S. Hashemi, V isual domain adapta tion via transfer feature learning, Knowl. Inf. Syst. , vol. 50, no. 2, pp. 585605, Feb. 2017. [81] J. Zhang, W . Li, and P . Ogunbona, Joint geometrical and statistical alignment for visual domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Honolulu, Jul. 2017, pp. 51505158. [82] B. Schlkopf, R. Herbrich, and A.J. Smola, A generalized r epre- senter theorem, in Proc. International Conference on Computational Learning Theory , Amsterdam, Jul. 2001, pp. 416426. [83] L. Duan, I.W . T sang, and D. Xu, Domain transfer multipl e kernel learning, IEEE T rans. Pattern Anal. Mach. Intell. , vol. 34, no. 3, pp. 465479, Mar . 2012. [84] A. Rakotomamonjy , F .R. Bach, S. Canu, and Y . Grandvalet, Sim- pleMKL, J. Mach. Learn. Res. , vol. 9, pp. 2491-2521, Nov . 2008. [85] I.S. Dhillon, S. Mallela, and D.S. Modha, Information-th eoretic co-clustering, in Proc. 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , W ashington, Aug. 2003, pp. 8998. [86] S. Deerwester , S.T . Dumais, G.W . Furnas, T .K. Landauer , a nd R. Harshman, Indexing by latent semantic analysis, J. Am. Soc. Inf. Sci., vol. 41, pp. 391407, Sep. 1990. [87] T . Hofmann, Probabilistic latent semantic analysis,  in Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence , Stockholm, Jul. 1999, pp. 289296. [88] J. Y oo and S. Choi, Probabilistic matrix tri-factoriza tion, in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, T aipei, Apr . 2009, pp. 15531556. [89] A. Dempster , N. Laird, and D. Rubin, Maximum likelihoo d from incomplete data via the EM algorithm, J. R. Stat. Soc. - Ser . B , vol. 39, no. 1, pp. 138, 1977. [90] G.-R. Xue, W . Dai, Q. Y ang, and Y . Y u, T opic-bridged PLSA for cross-domain text classiﬁcation, in Proc. 31st Annual International ACM SIGIR Conference on Research and Development in Informa tion Retrieval, Singapore, Jul. 2008, pp. 627634. [91] F . Zhuang, P . Luo, Z. Shen, Q. He, Y . Xiong, Z. Shi, and H. Xio ng, Collaborative Dual-PLSA: Mining distinction and commonal ity across multiple domains for text classiﬁcation, in Proc. 19th ACM International Conference on Information and Knowledge Man agement, T oronto, Oct. 2010, pp. 359368. [92] F . Zhuang, P . Luo, Z. Shen, Q. He, Y . Xiong, Z. Shi, and H. Xio ng, Mining distinction and commonality across multiple domai ns using generative model for text classiﬁcation, IEEE T rans. Knowl. Data Eng. , vol. 24, no. 11, pp. 20252039, Nov . 2012. [93] F . Zhuang, P . Luo, P . Y in, Q. He, and Z. Shi, Concept learn - ing for cross-domain text classiﬁcation: A general probabi listic framework, in Proc. 23rd International Joint Conference on Artiﬁcial Intelligence, Beijing, Aug. 2013, pp. 19601966. [94] J. Blitzer , R. McDonald, and F . Pereira, Domain adapta tion with structural correspondence learning, in Proc. Conference on Empirical Methods in Natural Language Processing , Sydney , Jul. 2006, pp. 120 128. [95] R.K. Ando and T . Zhang, A framework for learning predic tive structures from multiple tasks and unlabeled data, J. Mach. Learn. Res., vol. 6, pp. 18171853, Dec. 2005. [96] X. Glorot, A. Bordes, and Y . Bengio, Domain adaptation for large-scale sentiment classiﬁcation: A deep learning appr oach, in Proc. 28th International Conference on Machine Learning , Bellevue, Jun. 2011, pp. 513520. [97] P . V incent, H. Larochelle, Y . Bengio, and P .-A. Manzago l, Extract- ing and composing robust features with denoising autoencod ers, in Proc. 25th International Conference on Machine Learning , Helsinki, Jul. 2008, pp. 10961103. [98] M. Chen, Z. Xu, K. W einberger , and F . Sha, Marginalized d enois- ing autoencoders for domain adaptation, in Proc. 29th International Conference on Machine Learning , Edinburgh, Jun. 2012, pp. 767774. [99] M. Chen, K.Q. W einberger , Z. Xu, and F . Sha, Marginalizi ng stacked linear denoising autoencoders, J. Mach. Learn. Res. , vol. 16, no. 1, pp. 38493875, Jan. 2015. [100] B. Fernando, A. Habrard, M. Sebban, and T . T uytelaars,  Unsu- pervised visual domain adaptation using subspace alignmen t, in Proc. IEEE International Conference on Computer V ision , Sydney , Dec. 2013, pp. 29602967. [101] B. Sun and K. Saenko, Subspace distribution alignment fo r unsupervised domain adaptation, in Proc. British Machine V ision Conference, Swansea, Sep. 2015, pp. 24.124.10. [102] B. Gong, Y . Shi, F . Sha, and K. Grauman, Geodesic ﬂow kern el for unsupervised domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Providence, Jun. 2012, pp. 20662073. [103] R. Gopalan, Ruonan Li, and R. Chellappa, Domain adapt ation for object recognition: An unsupervised approach, in Proc. IEEE International Conference on Computer V ision , Barcelona, Jun. 2011, pp. 999-1006. [104] M.I. Zelikin, Control Theory and Optimization I in Encyclopaedia of Mathematical Sciences, vol. 86, Berlin: Springer , 2000. [105] B. Sun, J. Feng, and K. Saenko, Return of frustratingly e asy domain adaptation, in Proc. 30th AAAI Conference on Artiﬁcial Intelligence, Phoenix, Feb. 2016, pp. 20582065. [106] S.J. Pan, X. Ni, J.-T . Sun, Q. Y ang, and Z. Chen, Cross-do main sentiment classiﬁcation via spectral feature alignment, in Proc. 19th International Conference on World Wide Web , Raleigh, Apr . 2010, pp. 751760. [107] J. Blitzer , M. Dredze, and F . Pereira, Biographies, b ollywood, boom-boxes and blenders: Domain adaptation for sentiment c lassi- ﬁcation, in Proc. 45th Annual Meeting of the Association of Computa- tional Linguistics , Prague, Jun. 2007, pp. 440447. [108] F .R.K. Chung, Spectral Graph Theory . Providence: American Math- ematical Society , 1997. [109] A.Y . Ng, M.I. Jordan, and Y . W eiss, On spectral cluste ring: Analysis and an algorithm, in Proc. 15th Annual Conference on Neural Information Processing Systems , V ancouver , Dec. 2001, pp. 849- 856. [110] X. Ling, W . Dai, G.-R. Xue, Q. Y ang, and Y . Y u, Spectral d omain- transfer learning, in Proc. 14th ACM SIGKDD International Confer- ence on Knowledge Discovery and Data Mining , Las V egas, Aug. 2008, pp. 488496. [111] S.D. Kamvar , D. Klein, and C.D. Manning, Spectral learn ing, in Proc. 18th International Joint Conference on Artiﬁcial Int elligence, Acapulco, Aug. 2003, pp. 561566. [112] J. Shi and J. Malik, Normalized cuts and image segmenta tion, IEEE T rans. Pattern Anal. Mach. Intell. , vol .22, no. 8, pp. 888905, Aug. 2000. [113] L. Duan, I.W . T sang, D. Xu, and T .-S. Chua, Domain adapt ation from multiple sources via auxiliary classiﬁers, in Proc. 26th In- ternational Conference on Machine Learning , Montreal, Jun. 2009, pp. 289296. [114] L. Duan, D. Xu, and I.W . T sang, Domain adaptation from multiple sources: A domain-dependent regularization appr oach, IEEE T rans. Neural Netw . Learn. Syst. , vol. 23, no. 3, pp. 504518, Mar . 2012. [115] P . Luo, F . Zhuang, H. Xiong, Y . Xiong, and Q. He, T ransf er learn- ing from multiple source domains via consensus regularizat ion, in Proc. 17th ACM Conference on Information and Knowledge Mana gement, Napa V alley , Oct. 2008, pp. 103112. [116] F . Zhuang, P . Luo, H. Xiong, Y . Xiong, Q. He, and Z. Shi, C ross- domain learning from multiple sources: A consensus regular ization perspective, IEEE T rans. Knowl. Data Eng. , vol. 22, no. 12, pp. 1664 1678, Dec. 2010. [117] T . Evgeniou, C.A. Micchelli, and M. Pontil, Learning multiple tasks with kernel methods, J. Mach. Learn. Res. , vol. 6, pp. 615-637, Apr . 2005. [118] T . Kato, H. Kashima, M. Sugiyama, and K. Asai, Multi-ta sk learning via conic programming, in Proc. 21st Annual Conference on Neural Information Processing Systems , V ancouver , Dec. 2007, pp. 737744. [119] A.J. Smola and B. Schlkopf, A tutorial on support vector regres- sion, Stat. Comput. , vol. 14, no. 3, pp. 199222, Aug. 2004. [120] J. W eston, R. Collobert, F . Sinz, L. Bottou, and V . V apni k, Infer- ence with the universum, in Proc. 23rd International Conference on Machine Learning , Pittsburgh, Jun. 2006, pp. 10091016. [121] X. Y u and Y . Aloimonos, Attribute-based transfer lea rning for object categorization with zeroone training example, in Proc. 29 European Conference on Computer V ision , Heraklion, Sep. 2010, pp. 127140. [122] F . Zhuang, P . Luo, H. Xiong, Q. He, Y . Xiong, and Z. Shi, E x- ploiting associations between word clusters and document c lasses for cross-domain text categorization, Stat. Anal. Data Min. , vol. 4, no. 1, pp. 100114, Feb. 2011. [123] F . Zhuang, P . Luo, C. Du, Q. He, Z. Shi, and H. Xiong, T rip lex transfer learning: Exploiting both shared and distinct con cepts for text classiﬁcation, IEEE T . Cybern. , vol. 44, no. 7, pp. 11911203, Jul. 2014. [124] M. Long, J. W ang, G. Ding, W . Cheng, X. Zhang, and W . W ang , Dual transfer learning, in Proc. 12th SIAM International Conference on Data Mining , Anaheim, Apr . 2012, pp. 540551. [125] H. W ang, F . Nie, H. Huang, and C. Ding, Dyadic transfer learn- ing for cross-domain image classiﬁcation, in Proc. International Conference on Computer V ision , Barcelona, Nov . 2011, pp. 551556. [126] D. W ang, C. Lu, J. Wu, H. Liu, W . Zhang, F . Zhuang, and H. Zhang, Softly associative transfer learning for cros s- domain classiﬁcation, IEEE T . Cybern. , to be published. doi: 10.1109TCYB.2019.2891577. [127] Q. Do, W . Liu, J. Fan, and D. T ao, Unveiling hidden impl icit similarities for cross-domain recommendation, IEEE T rans. Knowl. Data Eng. , to be published. doi: 10.1109TKDE.2019.2923904. [128] T . T ommasi and B. Caputo, The more you know , the less you learn: from knowledge transfer to one-shot learning of o bject categories in Proc. British Machine V ision Conference , London, Sep. 2009, pp. 80.180.11. [129] G.C. Cawley , Leave-one-out cross-validation based model selec- tion criteria for weighted LS-SVMs, in Proc. IEEE International Joint Conference on Neural Network , V ancouver , Jul. 2006, pp. 16611668. [130] T . T ommasi, F . Orabona, and B. Caputo, Safety in number s: Learning categories from few examples with multi model know l- edge transfer , in Proc. IEEE Conference on Computer V ision and Pattern Recognition , San Francisco, Jun. 2010, pp. 30813088. [131] C.-K. Lin, Y .-Y . Lee, C.-H. Y u, and H.-H. Chen, Explor ing ensemble of models in taxonomy-based cross-domain sentime nt classiﬁcation, in Proc. 23rd ACM International on Conference on Information and Knowledge Management , Shanghai, Nov . 2014, pp. 12791288. [132] J. Gao, W . Fan, J. Jiang, and J. Han, Knowledge transfe r via mul- tiple model local structure mapping, in Proc. 14th ACM SIGKDD International Conference on Knowledge Discovery and Data M ining, Las V egas, Aug. 2008, pp. 283291. [133] F . Zhuang, P . Luo, S.J. Pan, H. Xiong, and Q. He. Ensembl e of anchor adapters for transfer learning, in Proc. 25th ACM In- ternational on Conference on Information and Knowledge Man agement, Indianapolis, Oct. 2016, pp. 23352340. [134] F . Zhuang, X. Cheng, P . Luo, S.J. Pan, and Q. He, Supervis ed representation learning with double encoding-layer autoe ncoder for transfer learning, ACM T rans. Intell. Syst. T echnol. , vol. 9, no. 2, pp. 117, Jan. 2018. [135] M. Ghifary , W .B. Kleijn, and M. Zhang, Domain adaptiv e neural networks for object recognition, in Proc. 13th Paciﬁc Rim Interna- tional Conference on Artiﬁcial Intelligence , Gold Coast, Dec. 2014, pp. 898904. [136] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T . Darrell , Deep domain confusion: Maximizing for domain invariance,  2014, arXiv:1412.3474v1. [137] M. Long, Y . Cao, J. W ang, and M.I. Jordan, Learning tra nsferable features with deep adaptation networks, in Proc. 32nd International Conference on Machine Learning , Lille, Jul. 2015, pp. 97105. [138] A. Krizhevsky , I. Sutskever , and G.E. Hinton, Imagene t classi- ﬁcation with deep convolutional neural networks, in Proc. 26th Annual Conference on Neural Information Processing System s, Lake T ahoe, Dec. 2012, pp. 10971105. [139] I. Goodfellow , J. Pouget-Abadie, M. Mirza, B. Xu, D. W a rde- Farley , S. Ozair , A. Courville, and Y . Bengio, Generative ad ver- sarial nets, in Proc. 28th Annual Conference on Neural Information Processing Systems , Montreal, Dec. 2014, pp. 26722680. [140] J. Y osinski, J. Clune, Y . Bengio, and H. Lipson, How tr ansferable are features in deep neural networks? in Proc. 28th Annual Confer- ence on Neural Information Processing Systems , Montreal, Dec. 2014, pp. 33203328. [141] M. Long, Y . Cao, Z. Cao, J. W ang, and M.I. Jordan, T rans - ferable representation learning with deep adaptation netw orks, IEEE T rans. Pattern Anal. Mach. Intell. , to be published. doi: 10.1109TP AMI.2018.2868685. [142] Y . Grandvalet and Y . Bengio, Semi-supervised learnin g by en- tropy minimization, in Proc. 18th Annual Conference on Neural Information Processing Systems , V ancouver , Dec. 2004, pp. 529536. [143] C. Szegedy , W . Liu, Y . Jia, P . Sermanet, S. Reed, D. Anguelo v , D. Erhan, V . V anhoucke, and A. Rabinovich, Going deeper wit h convolutions, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Boston, Jun. 2015, pp. 19. [144] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learnin g for image recognition, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Las V egas, Jun. 2016, pp. 770778. [145] K.P . Chwialkowski, A. Ramdas, D. Sejdinovic, and A. Gre tton, Fast two-sample testing with analytic representations of probabil- ity measures, in Proc. 29th Annual Conference on Neural Information Processing Systems , Montreal, Dec. 2015, pp. 19811989. [146] M. Long, H. Zhu, J. W ang, and M.I. Jordan, Unsupervise d do- main adaptation with residual transfer networks, in Proc. 30th An- nual Conference on Neural Information Processing Systems , Barcelona, Dec. 2016, pp. 136144. [147] M. Long, H. Zhu, J. W ang, and M.I. Jordan, Deep transfe r learning with joint adaptation networks, in Proc. 34th International Conference on Machine Learning , Sydney , Aug. 2017, pp. 22082217. [148] B. Sun and K. Saenko, Deep CORAL: Correlation alignment for deep domain adaptation, in Proc. European Conference on Computer V ision Workshops , Amsterdam, Oct. 2016, pp. 443450. [149] C. Chen, Z. Chen, B. Jiang, and X. Jin, Joint domain ali gnment and discriminative feature learning for unsupervised deep domain adaptation, in Proc. 33rd AAAI Conference on Artiﬁcial Intelligence , Honolulu, Jan. 2019, pp. 32963303. [150] Y . Pan, T . Y ao, Y . Li, Y . W ang, C.-W . Ngo, and T . Mei, T ra ns- ferrable prototypical networks for unsupervised domain ad ap- tation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition, Long Beach, Jun. 2019, pp. 22392247. [151] G. Kang, L. Jiang, Y . Y ang, and A.G. Hauptmann, Contra stive adaptation network for unsupervised domain adaptation, i n Proc. IEEE Conference on Computer V ision and Pattern Recognition , Long Beach, Jun. 2019, pp. 48934902. [152] Y . Zhu, F . Zhuang, J. W ang, J. Chen, Z. Shi, W . Wu, and Q. He , Multi-representation adaptation network for cross-doma in image classiﬁcation, Neural Netw . , vol. 119. pp. 214221, Nov . 2019. [153] Y . Zhu, F . Zhuang, and D. W ang, Aligning domain-speci ﬁc dis- tribution and classiﬁer for cross-domain classiﬁcation fr om multiple sources, in Proc. 33rd AAAI Conference on Artiﬁcial Intelligence , Honolulu, Jan. 2019, pp. 59895996. [154] Y . Ganin and V . Lempitsky , Unsupervised domain adapt ation by backpropagation, in Proc. 32nd International Conference on Machine Learning, Lille, Jul. 2015, pp. 11801189. [155] Y . Ganin, E. Ustinova, H. Ajakan, P . Germain, H. Laroch elle, F .Laviolette, M. Marchand, and V . Lempitsky , Domain-adve rsarial training of neural networks, J. Mach. Learn. Res. , vol. 17, pp. 135, Apr . 2016. [156] E. Tzeng, J. Hoffman, K. Saenko, and T . Darrell, Advers arial discriminative domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Honolulu, Jul. 2017, pp. 29622971. [157] J. Hoffman, E. Tzeng, T . Park, J.-Y . Zhu, P . Isola, K. Sae nko, A.A. Efros, and T . Darrell, CyCADA: Cycle-consistent adve rsarial domain adaptation, in Proc. 35th International Conference on Machine Learning, Stockholm, Jul. 2018, pp. 19942003. [158] M. Long, Z. Cao, J. W ang, and M.I. Jordan, Conditional adver- sarial domain adaptation, in Proc. 32nd Annual Conference on Neural Information Processing Systems , Montreal, Dec. 2018, pp. 16401650. [159] Y . Zhang, H. T ang, K. Jia, and M. T an, Domain-symmetri c net- works for adversarial domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Long Beach, Jun. 2019, pp. 50315040. [160] H. Zhao, S. Zhang, G. Wu, J.M.F . Moura, J.P . Costeira, an d G.J. Gordon, Adversarial multiple source domain adaptation, in Proc. 32nd Annual Conference on Neural Information Processing Sy stems, Montreal, Dec. 2018, pp. 85598570. [161] C. Y u, J. W ang, Y . Chen, and M. Huang, T ransfer learnin g with dynamic adversarial adaptation network, in Proc. 19th IEEE International Conference on Data Mining , Beijing, Nov . 2019, pp. 19. [162] J. Zhang, Z. Ding, W . Li, and P . Ogunbona, Importance w eighted adversarial nets for partial domain adaptation, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Salt Lake City , Jun. 2018, pp. 81568163. 30 [163] Z. Cao, M. Long, J. W ang, and M.I. Jordan, Partial tran sfer learn- ing with selective adversarial networks, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Salt Lake City , Jun. 2018, pp. 27242732. [164] B. W ang, M. Qiu, X. W ang, Y . Li, Y . Gong, X. Zeng, J. Huang , B. Zheng, D. Cai, and J. Zhou, A minimax game for instance based selective transfer learning, in Proc. 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Anchorage, Aug. 2019, pp. 3443. [165] X. Chen, S. W ang, M. Long, and J. W ang, T ransferability vs. discriminability: Batch spectral penalization for advers arial domain adaptation, in Proc. 36th International Conference on Machine Learn- ing, Long Beach, Jun. 2019, pp. 10811090. [166] J. Deng, W . Dong, R. Socher , L.-J. Li, K. Li, and L. Fei-Fe i, ImageNet: A large-scale hierarchical image database, in Proc. IEEE Conference on Computer V ision and Pattern Recognition , Miami, Jun. 2009, pp. 248255. [167] M. Maqsood, F . Nazir , U. Khan, F . Aadil, H. Jamal, I. Meh mood, and O. Song, T ransfer learning assisted classiﬁcation and d etection of Alzheimer s disease stages using 3D MRI scans, Sensors, vol. 19, no. 11, pp. 119, Jun. 2019. [168] D.S. Marcus, A.F . Fotenos, J.G. Csernansky , J.C. Morri s, and R.L. Buckner , Open access series of imaging studies: Longitudi nal MRI data in nondemented and demented older adults, J. Cogn. Neurosci. , vol. 22, no. 12, pp. 26772684, Dec. 2010. [169] H.-C. Shin, H.R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Y ao, D. Mollura, and R.M. Summers, Deep convolutional neura l networks for computer-aided detection: CNN architectures , dataset characteristics and transfer Learning, IEEE T rans. Med. Imaging , vol. 35, no. 5, pp. 12851298, May 2016. [170] M. Byra, M. Wu, X. Zhang, H. Jang, Y .-J. Ma, E.Y . Chang, S. Shah, and Jiang Du, Knee menisci segmentation and relaxomet ry of 3D ultrashort echo time cones MR imaging using attention UNet with transfer learning, Magn. Reson. Med. , Sep. 2019, doi: 10.1002mrm.27969. [171] X. T ang, B. Du, J. Huang, Z. W ang, and L. Zhang, On combi ning active and transfer learning for medical data classiﬁcatio n, IET Comput. V is. , vol. 13, no. 2, pp. 194205, Feb. 2019. [172] M. Zeng, M. Li, Z. Fei, Y . Y u, Y . Pan, and J. W ang, Automa tic ICD-9 coding via deep transfer learning, Neurocomputing, vol. 324, pp. 4350, Jan. 2019. [173] G. Schweikert, G. Ratsch, C. Widmer , and B. Scholkopf, A n empirical analysis of domain adaptation algorithms for gen omic sequence analysis, in Proc. 22nd Annual Conference on Neural Infor- mation Processing Systems , V ancouver , Dec. 2008, pp. 14331440. [174] R. Petegrosso, S. Park, T .H. Hwang, and R. Kuang, T rans fer learning across ontologies for phenome-genome associatio n predic- tion, Bioinformatics, vol. 33, no. 4, pp. 529536, Feb. 2017. [175] T . Hwang and R. Kuang, A heterogeneous label propagat ion al- gorithm for disease gene discovery , in Proc. 10th SIAM International Conference on Data Mining , Columbus, Apr . 2010, pp. 583594. [176] Q. Xu, E.W . Xiang, and Q. Y ang, Protein-protein inter action prediction via collective matrix factorization, in Proc. IEEE Interna- tional Conference on Bioinformatics and Biomedicine , Hong Kong, Dec. 2010, pp. 6267. [177] A.P . Singh and G.J. Gordon, Relational learning via co llective matrix factorization, in Proc. 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Las V egas, Aug. 2008, pp. 650658. [178] S. Di, H. Zhang, C. Li, X. Mei, D. Prokhorov , and H. Ling,  Cross- domain trafﬁc scene understanding: A dense correspondence -based transfer learning approach, IEEE T rans. Intell. T ransp. Syst. , vol. 19, no. 3, pp. 745757, Mar . 2018. [179] H. Abdi, Partial least squares regression and projec tion on latent structure regression (PLS Regression), Wiley Interdiscip. Rev .- Comput. Statist. , vol. 2, no. 1, pp. 97106, Jan. 2010. [180] S.D. Pietra, V .D. Pietra, and J. Lafferty , Inducing fe atures of random ﬁelds, IEEE T rans. Pattern Anal. Mach. Intell. , vol. 19, no. 4, pp. 380393, Apr . 1997. [181] C. Liu, J. Y uen, and A. T orralba, Nonparametric scene parsing via label transfer , IEEE T rans. Pattern Anal. Mach. Intell. , vol. 33, no. 12, pp. 23682382, Dec. 2011. [182] C. Lu, F . Hu, D. Cao, J. Gong, Y . Xing, and Z. Li, T ransfe r learning for driver model adaptation in lane-changing scen arios using manifold alignment, IEEE T rans. Intell. T ransp. Syst. , to be published. doi: 10.1109TITS.2019.2925510. [183] D.J. Berndt and J. Clifford, Using dynamic time warpi ng to ﬁnd patterns in time series, in Proc. Knowledge Discovery in Databases Workshop, Seattle, Jul. 1994, pp. 359370. [184] N. Makondo, M. Hiratsuka, B. Rosman, and O. Hasegawa,  A non-linear manifold alignment approach to robot learning f rom demonstrations, J. Robot. Mechatron. , vol. 30, no. 2, pp. 265281, Apr . 2018. [185] P . Angkititrakul, C. Miyajima, and K. T akeda, Modeli ng and adaptation of stochastic driver-behavior model with appli cation to car following, in Proc. IEEE Intelligent V ehicles Symposium (IV) , Baden-Baden, Jun. 2011, pp. 814819. [186] Y . Liu, P . Lasang, S. Pranata, S. Shen, and W . Zhang, Drive r pose estimation using recurrent lightweight network and virtua l data augmented transfer learning, IEEE T rans. Intell. T ransp. Syst. , vol. 20, no. 10, pp. 38183831, Oct. 2019. [187] J. W ang, H. Zheng, Y . Huang, and X. Ding, V ehicle type r ecog- nition in surveillance images from labeled web-nature data using deep transfer learning, IEEE T rans. Intell. T ransp. Syst. , vol. 19, no. 9, pp. 29132922, Sep. 2018. [188] K. Gopalakrishnan, S.K. Khaitan, A. Choudhary , and A. A grawal, Deep convolutional neural networks with transfer learnin g for computer vision-based data-driven pavement distress dete ction, Constr . Build. Mater . , vol. 157, pp. 322330, Dec. 2017. [189] S. Bansod and A. Nandedkar , T ransfer learning for vide o anomaly detection, J. Intell. Fuzzy Syst. , vol. 36, no. 3, pp. 1967 1975, Mar . 2019. [190] G. Rosario, T . Sonderman, and X. Zhu, Deep transfer lea rning for trafﬁc sign recognition, in Proc. IEEE International Conference on Information Reuse and Integration , Salt Lake City , Jul. 2018, pp. 178185. [191] W . Pan, E.W . Xiang, and Q. Y ang, T ransfer learning in c ollabora- tive ﬁltering with uncertain ratings, in Proc. 26th AAAI Conference on Artiﬁcial Intelligence , T oronto, Jul. 2012, pp. 662668. [192] G. Hu, Y . Zhang, and Q. Y ang, T ransfer meets hybrid: A synthetic approach for cross-domain collaborative ﬁlteri ng with text, in Proc. 28th International Conference on World Wide Web , San Francisco, May 2019, pp. 28222829. [193] W . Pan, E.W . Xiang, N.N. Liu, and Q. Y ang, T ransfer lea rning in collaborative ﬁltering for sparsity reduction, in Proc. 24th AAAI Conference on Artiﬁcial Intelligence , Atlanta, Jul. 2010, pp. 230235. [194] W . Pan and Q. Y ang, T ransfer learning in heterogeneou s col- laborative ﬁltering domains, Artif. Intell. , vol. 197, pp. 3955, Apr . 2013. [195] F . Zhuang, Y . Zhou, F . Zhang, X. Ao, X. Xie, and Q. He, Seq uen- tial transfer learning: Cross-domain novelty seeking trai t mining for recommendation, in Proc. 26th International Conference on World Wide Web Companion , Perth, Apr . 2017, pp. 881882. [196] J. Zheng, F . Zhuang, and C. Shi, Local ensemble across m ultiple sources for collaborative ﬁltering, in Proc. 26th ACM International on Conference on Information and Knowledge Management , Singapore, Nov . 2017, pp. 24312434. [197] F . Zhuang, J. Zheng, J. Chen, X. Zhang, C. Shi, and Q. He, T ransfer collaborative ﬁltering from multiple sources vi a consen- sus regularization, Neural Netw . , vol. 108, pp. 287295, Dec. 2018. [198] J. He, R. Liu, F . Zhuang, F . Lin, C. Niu, and Q. He, A gene ral cross-domain recommendation framework via Bayesian neura l net- work, in Proc. 18th IEEE International Conference on Data Mining , Singapore, Nov . 2018, pp. 10011006. [199] F . Zhu, Y . W ang, C. Chen, G. Liu, M.A. Orgun, and J. Wu, A deep framework for cross-domain and cross-system recommendati ons, in Proc. 27th International Joint Conference on Artiﬁcial Int elligence, Stockholm, Jul. 2018, pp. 37113717. [200] F . Y uan, L. Y ao, and B. Benatallah, DARec: Deep domain adap- tation for cross-domain recommendation via transferring r ating patterns, in Proc. 29th International Joint Conference on Artiﬁcial Intelligence, Macao, Aug. 2019, pp. 42274233. [201] E. Bastug, M. Bennis, and M. Debbah, A transfer learni ng approach for cache-enabled wireless networks, in Proc. 13th Inter- national Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks , Mumbai, May 2015, pp. 161166. [202] R. Li, Z. Zhao, X. Chen, J. Palicot, and H. Zhang, T ACT: A transfer actor-critic learning framework for energy savin g in cel- lular radio access networks, IEEE T rans. Wirel. Commun. , vol. 13, no. 4, pp. 20002011, Apr . 2014. [203] Q. Zhao and D. Grace, T ransfer learning for QoS aware t opology management in energy efﬁcient 5G cognitive radio networks,  in 31 Proc. 1st International Conference on 5G for Ubiquitous Con nectivity, Akaslompolo, Nov . 2014, pp. 152157. [204] B. Guo, J. Li, V .W . Zheng, Z. W ang, and Z. Y u, Citytrans fer: T ransferring inter- and intra-city knowledge for chain sto re site recommendation based on multi-source urban data, in Proc. ACM on Interactive, Mobile, Wearable and Ubiquitous T echnolog ies, Jan. 2018, pp. 123. [205] Y . W ei, Y . Zheng, and Q. Y ang, T ransfer knowledge betw een cities, in Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , San Francisco, Aug. 2016, pp. 19051914. [206] U. Cote-Allard, C.L. Fall, A. Drouin, A. Campeau-Leco urs, C. Gosselin, K. Glette, F . Laviolette, and B. Gosselin, Deep l earn- ing for electromyographic hand gesture signal classiﬁcati on using transfer learning, IEEE T rans. Neural Syst. Rehabil. Eng. , vol. 27, no. 4, pp. 760771, Apr . 2019. [207] C. Ren, D. Dai, K. Huang, and Z. Lai, T ransfer learning of structured representation for face recognition, IEEE T rans. Image Process., vol. 23, no. 12, pp. 54405454, Dec. 2014. [208] J. W ang, Y . Chen, L. Hu, X. Peng, and P .S. Y u, Stratiﬁed tr ans- fer learning for cross-domain activity recognition, in Proc. IEEE International Conference on Pervasive Computing and Commu nications, Athens, Mar . 2018, pp. 110. [209] J. Deng, Z. Zhang, E. Marchi, and B. Schuller , Sparse autoencoder-based feature transfer learning for speech em otion recognition, in Proc. Humaine Association Conference on Affective Computing and Intelligent Interaction , Geneva, Sep. 2013, pp. 511 516. [210] D. Xi, F . Zhuang, G. Zhou, X. Cheng, F . Lin, and Q. He, Do main adaptation with category attention network for deep sentim ent analysis, in Proc. The Web Conference , T aipei, Apr . 2020, pp. 3133 3139. [211] Y . Zhu, D. Xi, B. Song, F . Zhuang, S. Chen, X. Gu, and Q. He, Modeling users behavior sequences with hierarchical exp lainable network for cross-domain fraud detection, in Proc. The Web Con- ference, T aipei, Apr . 2020, pp. 928938. [212] J. T ang, T . Lou, J. Kleinberg, and S. Wu, T ransfer learn ing to infer social ties across heterogeneous networks, ACM T rans. Inf. Syst., vol. 34, no. 2, pp. 143, Apr . 2016. [213] L. Zhang, L. Zhang, D. T ao, and X. Huang, Sparse transfe r man- ifold embedding for hyperspectral target detection, IEEE T rans. Geosci. Remote Sensing , vol. 52, no. 2, pp. 10301043, Feb. 2014. [214] F . Zhuang, K. Duan, T . Guo, Y . Zhu, D. Xi, Z. Qi, and Q. He, T ransfer learning toolkit: Primers and benchmarks, 2 019, arXiv:1911.08967v1. [215] K. Saenko, B. Kulis, M. Fritz, and T . Darrell, Adapting visual category models to new domains, in Proc. 11th European Conference on Computer V ision , Heraklion, Sep. 2010, pp. 213226. [216] Z.C. Lipton, The mythos of model interpretability , ACM Que. , vol. 16, no. 3, May 2018, pp. 127.",
    "page_start": null,
    "page_end": null,
    "word_count": 31499,
    "created_at": "2025-08-18T07:03:32",
    "updated_at": "2025-08-18T07:03:32"
  },
  {
    "id": "92ff6f7dab9348c5bfa9fb44793e1ee5",
    "doc_id": "cb772f87bc504c24a67a56eb069f1224",
    "doc_name": "Transfer_learning_in_DL_2.pdf",
    "heading": "Document",
    "content": "See discussions, stats, and author profiles for this publication at: https:www.researchgate.netpublication367432163 Transfer Learning in Deep Learning Models for Building Load Forecasting: Case of Limited Data Preprint  January 2023 DOI: 10.48550arXiv.2301.10663 CITATIONS 0 READS 123 4 authors, including: Menna Nawar Alexandria Higher Institute of Engineering and Technology 2 PUBLICATIONS 11 CITATIONS SEE PROFILE Moustafa Shomer Valify Solutions 2 PUBLICATIONS 11 CITATIONS SEE PROFILE All content following this page was uploaded by Moustafa Shomer on 01 February 2023. The user has requested enhancement of the downloaded file. Authors manuscript version accepted for publication. The final published version is copyrighted by IEEE and will be available as: Menna Nawar, Moustafa Shomer, Samy Faddel, Huangjie Gong, Transfer Learning in Deep Learning Models for Building Load Forecasting: Case of Limited Data , in IEEE SoutheastCon, 2023. 2023 IEEE Copyright Notice. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprintingrepublishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Transfer Learning in Deep Learning Models for Building Load Forecasting: Case of Limited Data AbstractPrecise load forecasting in buildings could increase the bill savings potential and facilitate optimized strategies for power generation planning. With the rapid evolution of computer science, data-driven techniques, in particular the Deep Learning models, have become a promising solution for the load forecasting problem. These models have showed accurate forecasting results; however, they need abundance amount of historical data to maintain the performance. Considering the new buildings and buildings with low resolution measuring equipment, it is difficult to get enough historical data from them, leading to poor forecasting performance. In order to adapt Deep Learning models for buildings with limited and scarce data, this paper proposes a Building-to-Building Transfer Learning framework to overcome the problem and enhance the performance of Deep Learning models. The transfer learning approach was applied to a new technique known as Transformer model due to its efficacy in capturing data trends. The performa nce of the algorithm was tested on a large commercial building with limited data. The result showed that the proposed approach improved the forecasting accuracy by 56.8 compared to the case of conventional deep learning where training from scratch is used . The paper also compared the proposed Transformer model to other sequential deep learning models such as Long-short Term Memory (LSTM) and Recurrent Neural Network (RNN). The accuracy of the transformer model outperformed other models by reducing the root mean square error to 0.009, compared to LSTM with 0.011 and RNN with 0.051. Keywords Deep Learning , Transfer Learning , Load Forecasting, Transformer, Sequential Models I. INTRODUCTION As a result of population growth and ongoing technological and economic developments, the number of commercial buildings has increased by 6 from 2012 to 2018 [1]. Newer buildings seem to be bigger in size and have modern electronic devices and more sophisticated equipment, which led to more power consumpti on and energy bills. In 2018, the U.S. has consumed more energy than ever before recording 101.3 quadrillion Btu, up 4 from 2017 [2]. In the following year 2019, U.S. has represented 17 of the energy consumption out of the whole world consumption [3]. Therefore, it is essential for respective parties involved in energy management such as governments and companies to improve the power consumption efficiency. Load-forecasting of commercial buil dings plays a crucial role in increasing energy schedule efficiency, and has become a serious topic that promoted a substantial degree of research. The principal obstacles arise from the fact that there is not a specific factor that controls the energy consumption in commercial buildings, instead there are multiple factors affecting the change of the consumed power. Those factors could affect the forecasting either in a direct way or indirect way, including weather conditions, building size, building type, electronic equipment and human activities, etc. [4]. Since forecasting is a key in improving the energy efficiency issue in commercial buildings, it is necessary to understand different types of load-forecasting. According to the spectrum of time intervals, load forecasting problems are classified into three types: short -term load forecasting (STLF) [5], medium-term load forecasting (MTLF) and long-term load forecasting (LTLF) [6]. This paper focuses on MTLF given the adopted time interval of load forecasting . MTLF has been an active research area which is highly discussed in the literature since it delive rs valuable information for both planning and operation [7]-[10]. MTLF can be assumed from one week to several months and has a goal of making a systematically effective operational plan and scheduling energy for both power generation plants and distribution utilities. In the past decades, it has been observed that improving the accuracy of energy forecasting has become an active issue. It was discussed in many studies using conventional statistical techniques and Artificial Intelligence (AI) base d approaches. Traditional statistical techniques usually involve auto-regressive integrated moving average (ARIMA), linear regression, and exponential smoothing [11], [1 2]. Statistical methods are fast and easy to apply because they rely on linear function s to process the relationships between the historical and forecasted data. As the time-series load forecasting is a non-linear problem, these models are not always forecast ing the load satisfactorily. This issue motivated the development of AI -based models as they are non-linear models which apply non-linear functions to forecast the load demand. Driven by the suitability and efficiency of AI techniques, they have become common in the literature to solve arduous load forecasting problems [1 3], [1 4]. AI -models can be categorized as Machine Learning and Deep Learning models. In terms of Machine Learning models, since these models have advantages such as simplicity and high computation speed, they are highly discussed in the literature [15], [16]. However, Deep Learning models , have proved to be more accurate in energy forecasting tasks [17]. AI-models depend on a large amount of historical data to predict power consumption [18], [19]. In [20], the authors used Deep Learning to predict humidity, air temperatures and energy behavior of buildings. Menna Nawar1, Moustafa Shomer1, Samy Faddel2, and Huangjie Gong2 1Alexandria Higher Institute of Engineering and Technology 2US Research Center, ABB Inc. Corresponding author: samy.faddel-mohamedus.abb.com The algorithm was trained on a large amount of data. Relying on a large-scale data to predict energy consumption was also used in [21], where the authors used a hybrid Deep Learning model that consists of Lon g Short -Term Memory (LSTM) and Recurrent Neural Network (RNN) in LTLF and trained their model on six years of data points from 2004 to 2010. In [22], the authors adapted a developed deep neural network (DNN) for heating energy consumption forecasting. Alth ough, the model had an acceptable performance when it was compared to simulation model from EnergyPlus, the authors did not consider comparing their model to other popular Deep Learning models that are suitable for time-series forecasting. Transformer model is considered one of the newly suggested models in the literature that was mentioned to have a great potential. In [23], the authors trained the Transformer model on 87,648 sampling data point before applying it in STLF , which makes it impractical for new buildings or those that do not collect data at a frequent basis. In [24], the authors explored transfer of knowledge from another domain, then applied this knowledge in two machine learning models (Random Forrest and Feed Forward Networ k). In [25], Transformer model was trained on a very huge-scale of data (19 years of data points) for wind speed forecasting, which clearly proves that Transformer model is considered one of the most data -hungry models. One of the most recent studies [26], The authors modeled Multi - Layer Perceptron (MLP) and Long Short -Term Memory (LSTM) on a medium-sized office building with data scarcity to predict the building thermal dynamics. They applied both traditional Machine Learning and Transfer Learning. The results of deep analysis leveraged using Transfer Learning and lead to higher accuracy. It is noteworthy that none of the previous work considered the use of Transformer model with Transfer learning to tackle the case of limited date. This paper proposes a medium-term load forecasting methodology that can be used for buildings with scarce historical data . The methodology is based on taking the knowledge and information learnt from buildings data in the same domain and transferring it to the targeted building. The paper also applies the framework of knowledge transfer to the Transformer deep learning model that is known for its ability in capturing trends and relations. To the best of our knowledge, this is the first time to apply transfer learning to the Tran sformer model in energy forecasting domain. Finally, the paper compares the proposed methodology to common deep learning techniques such as RNN and LSTM. The paper is organized as follows: Section 2 introduces the transfer learning and background concepts concerning the transfer learning technique. Section 3 presents deep learning models and discusses the specific models used in this paper. Furthermore, Section 4 shows a case study used to evaluate the proposed method and its results . Finally, our conclusion and future work will be discussed in section 5. II. TRANSFER LEARNING TECHNIQUE A. Sequential Deep Learning Models Deep Learning (DL) has become progressively popular in the field of time-series forecasting. The building-unit of any DL model is neural network. The neural network receives inputs, which can be text, image, video, or time -series data like this paper. Then, the input is processed in one or more hidden layer by using weights. These weights can be adjusted during the training process. Finally, the model comes up with the prediction in the output layer. Sequential DL models are the models that use temporal data as inputs. Time-series data are form of sequence data. The main advantage of sequence models is assuming that all inputs and outputs are dependent of each other. Therefore, these models can work efficiently in energy forecasting since the previous inputs in time-series data are inextricably significant in predicting the energy-consumption output. Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) are popular sequential models used in energy forecasting using time -series data as inputs, while Transformer model is rarely used in this domain. Noteworthy, t he three models can capture the patterns and trends, especially the Transformer . The architecture of each model will be discussed in detail in section III, and based on the patterns and trends, the models can predict accurately the energy consumption value. B. Building-to-Building Transfer Learning Transfer Learning (TL) is a promising approach that can help in enhancing the model performance by applying the prior knowledge gained from a so urce task to a similar or different target task. This approach was inspired by the way humans learn new things. Humans have always benefited from their transfer of knowledge and its usage in other different areas. From computer science point of view, the a im of TL is to leverage a pre-trained models knowledge, then applying this knowledge in performing another task. Fig. 1 illustrates the difference between traditional learning in part (a) and Transfer Learning in part (b). It shows that Transfer Learning depends on transferring the knowledge to a pre-trained model. Hence the complexity of the training process will be much easier. Considering the load forecasting problem, it is customary using massive amount of historical data to train an AI -based model to predict the energy consumption. Thus, relying on a large data has become unavoidable, which is impracticable in the case of newly constructed buildings. Unlike TL case, it is available to use scarce historical data to train the DL model as long as it is possible to take advantage of previous knowledge gained from an older task or another source task. Given that the DL models are always data-hungry, the main advantage of TL in these models is that it makes the use of small amount of data for training not only possible, but also efficient. Nevertheless, other advantages of TL are faster training, better model-initializing and higher learning rate for training. From a theoretical perspective of TL, the source (first task) and target tasks (other tasks) could be from the same or different domains. When it comes to load forecasting field, it is more popular in the literature to apply TL between two different domains. This paper demonstrates for the first -time applying building -to-building Transfer Learning in MTLF. The pre-trained models used in this study are initially trained for a load forecasting task for an old building. Then, th e prior knowledge gained from training on previous data, is used to enhance the accuracy of MTLF for new or other commercial buildings with scarce data. Briefly, TL in this paper is mainly used to acquire the knowledge from some old buildings to improve modellingforecasting efficiency on new buildings, with limited data. III. DEEP LEARNING MODELS A. RNN and LSTM Models RNNs have the same general principle of ANNs. While traditional ANNs can only deal with the input data individually and process the relationships between inputs and outputs without linking each output to the preceding. The main difference that RNNs can add ress the sequential data by using internal memory and assume that every output is related to the previous state. Once the output of the RNN is generated, it is copied and returned to the system as an input, hence the name Recurrent of the RNN comes from. Fig. 2 illustrates the structure of RNN, where X and O indicate the input and the output through the time t. Hidden layers are represented by h. U and V denote the weight m atrices between inputs and hidden layers, hidden layers and outputs. W represents t he weight metric between hidden layers in several time stamps. Although RNN is capable of processing sequential data, it fails to extract long -term information after many repeated iterations, because RNN suffers from vanishing gradient problem (this means that the internal memory of RNN cannot keep tracking of the gradient after multiple loops). To overcome this problem, LSTM networks were proposed in [ 27]. LSTMs are considered modified RNNs, however LSTMs are more effective in time -series forecasting for t he long -term dependency than RNNs. The structure of LSTM is similar to RNN, with an addition hidden state, it is also called LSTM cell as shown in Fig. 3 (where c denotes the computations of weights). This cell works as a separate memory for the network, and it is responsible for remembering the long -term information in LSTMs. This cell contains three gates; forget, input and output gate. The forget gate decides wh ich state in the cell can be forgotten, as this information is no longer necessary for the next state prediction. The input gate is responsible for adding or updating the internal cell with the new information. The output gate selects which part should be addressed as an output among all of the information in LSTM cell. The gates in LSTM are represented by the following equations: 𝑖𝑡  𝜎(𝑤𝑖 [ℎ𝑡1, 𝑥𝑡]  𝑏𝑖) (1) 𝑓𝑡  𝜎(𝑤𝑓[ℎ𝑡1, 𝑥𝑡]  𝑏𝑓) (2) 𝑜𝑡  𝜎(𝑤𝑜[ℎ𝑡1, 𝑥𝑡]  𝑏𝑜) (3) (a) (b) (a) (b) Fig. 1 The difference between traditional learning (a) and transfer learning (b). Fig. 2 Fundamental topology of RNN where: 𝑖𝑡 is the input gate, 𝑓𝑡 is the forget gate and 𝑜𝑡 is the output gate of the network. Sigmoid function is represented by 𝜎, 𝑤 denotes the weight for the respective gates and ℎ𝑡1 is the previous output at timestep 𝑡  1 . The current input is represented by 𝑥𝑡 and 𝑏 indicates the biases for the respective gates. Although LSTM has overcome the obstacle of vanishing gradient in RNN, this model created other challenges. Since relying on the LSTM cell including the three gates, the information from previous steps has to go through a long sequence of computations. This problem leads to difficulty in training LSTM due to a very long gradient path. This recursion of the sequence will cause information loss eventually. As mentioned earlier, RNN and LSTM suffer from forgetting the information in the long -term, since both models process the inputs sequentially. Hence, the need for a new model addressing the inputs in parallel rather than sequentially. B. Attention mechanism and Transformer Model The solution to the long -term dependency problem was solved in 2014 and 2015 by [28], [29]. These pioneering papers proposed a technique called the Attention Mechanism. Attention Mechanism is the backbone of the Transformer model, illustrated in [ Fig. 4], Transformers are currently the state -of- the-art solution for many problems such as computer vision and natural language processing. It consists of two parts, encoder and decoder, where each block contains one or more attention layer followed by a multi -layer perceptron (MLP). Using this technique dramatically improved the quality of any sequence - related tasks and used later in the time -series forecasting domain. The attention allows the model to concentrate on only the important and relevant subsets in the long sequences of the input. Concretely, the model has to decide by learning on its own which information from the past steps are relevant for encoding the available input, and then takes the encoded information and decodes it to representative features that is used for forecasting. The core of attention mechanism is assuming that the data source contains elements that can be represented as Key (K) and Value (V). Another element; Query (Q) is associated with the target data, where: KT  R3x5 , V  R5x3 and Q  R4x3. There are three steps to a pply the attention mechanism [27]. First, calculating the similarity score between the key vector and the query vector according to ( 4). Secondly in ( 5), converting the similarity score into weights, then arranging these weights in a probability distribution. Finally, (6) represents calculating the attention value by weighted summation of all the resulted coefficients. The reader is referred to [27] for more information. 𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦(𝑄𝑉, 𝑘𝑖)  𝑄𝑉𝑘𝑖 𝑄𝑉𝑘𝑖 (4) 𝑊𝑖  𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦𝑖 ) 𝑓𝑜𝑟 𝑖  1, 2, 3,  , 𝑛 (5) 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 𝑆𝑐𝑜𝑟𝑒(𝑄, 𝐾, 𝑉)   𝑤𝑖 𝑛 𝑖1  𝑉 (6) In 2017, a new DL model, called Transformer, was proposed based on the attention mechanism [30]. Transformer was rapidly used in many different areas such as translation, image classification and time-series forecasting, and it overcame state- of-the-art models in these areas. In comparison, the main advantage to the Transformers architecture is that it allows the model to access the input data directly using parallel computation, not sequentially as the case of RNN and LSTM. Processing the inputs in parallel avoids the recursion and the iterations, which leads to reduction in the training time and the probability of losing information in the long dependencies. The Transformer does not depend on th e previous hidden states to capture the patterns in order to predict the output. Instead, it processes a batch of input data as a one unit learning positional embeddings to encode the relationships between each observation and search for dependencies and patterns in the time -series data. Positional embedding is a technique that was introduced to replace recurrence by using weights that can encode the information related to a specific position of a certain input , then the transformer decodes the information and transforms it into prediction for the next time - step. IV. CASE STUDY This paper used hourly collected data from two buildings over time interval of one year starting from 1 January 2016 to 1 January 2017. The data were adopted from American Society of Heating , Refrigerating and Air -Conditioning Engineers (ASHRAE) [31]. The proposed models , Transformer, LSTM and RNN, were trained on a subset of that data , representing only 20 of the total samples , which counted up to 336 data samples. The time window used in the experimentations is 6 hours. Fig. 3 Simple architecture of LSTM (a), the internal state or the cell of LSTM and its gates (b). Fig. 4 The Architecture of the Transformer Model Fig. 5 The load consumption over one week for one building including weekend and workdays Fig. 6 The comparison of the three models The dataset contained 14 features, including information about weather conditions like temperature, cloud coverage, precipitation depth and others. It also included buildings metadata like buildings sizes and primary usage. To improve the performance of the model , some engineered features were added like day-of-the-week, seasons, and day-of-the-month, to account for the vacations, holidays and seasonal trends. This paper focuses on educational buildings as they captured most of buildings types by 38. An example of the pattern of the load consumption over one week for one building is shown in Fig. 5 According to the floor count, the number of floors for the two buildings is equal to five floors, representing a large building type Finally, the data used in the study had many features. Some of these features were repre sentative and informative for the modeling process, while others where not relevant to the task or might harm the prediction. After analyzing the data, it was decided to go with the chosen featurism while ditching the rest. An example for the neglected data is the sea level pressure as it is almost constant across all data points. Also, the wind direction as it does not affect the temperature, but it is an indication for the presence of the wind. This would force the model to learn this correlation, instead of learning about the desired task. As well as, engineering the features to give more information to the proposed model, such as the day, month, and hour. It is crucial to determine If the day is a weekend or not, because the usage of electricity highly depends on this feature. The model can also learn to combine information from multiple features together. The results will first show how the transfer learning can improve the accuracy of the deep learning model , enabling forecasting in building with limited data. Then, the paper will compare the proposed transfer transformer model to other sequential models. A. Transfer-Learning Effect on the Transformer model This section aims to see the effect of applying building-to- building Transfer Learning on improving the performance of the Transformer model which were trained on limited data . In theory, transfer learning takes the weights of the pre -trained model and uses them as a starting point to train from, which increases the speed and probability of better convergence to reach high performance . Since it is the first time to apply this type of Transfer Learning to the Transformer model in the domain of energy forecasting and train it on limited data to forecast the load demand, it is essential to evaluate the impact of applying Transfer Learning . Considering the case of large commercial buildings, after adapting building -to-building Transfer Learning the performance of the Transformer model has been improved by 56.8  in terms o f mean squared error (MSE), and almost 34  in terms of root mean square error (RMSE) and mean absolute percentage error (MAPE) as shown in Table. I. B. Forecasting the Load of Large Buildings To evaluate the performance of the proposed model compared to the traditional sequential models such as the RNN and the LSTM, the three models were trained for 15 epochs to prevent models overfitting and make it easier to measure the performance of the models under the same training period. The same large building was used as the source of data for the three models. Also, transfer learning was applied to all of them to ensure apple to apple comparison. The results are presented in Fig. 6 and Table. II. The figure shows that the RNN model has the tendency to overestimate the load for both during heavy and light loading conditions compared to the other two models. The LSTM shows close performance to the transformer model though the transformer is better in following the trend, resulting in a better accuracy as given in Table. II. This result is because the Transformer is built for capturing the patterns, trends, and relations between data points. Regardless of the improvement in the accuracy, the th ree models do not seem to perform well under light loading conditions for the case of weekends as shown after hour 125 in Fig. 6. This will be investigated more in the future. However, typically load forecasting for weekends in commercial buildings is not of high importance since the load is very low and there are not any occupants in the building. TABLE I. IMPACT OF TRANSFER LEARNING IN TRANSFORMERS PERFORMANCE IN CASE OF LARGE BUILDINGS Type of Metric Transformer model Before Transfer Learning After Transfer Learning MSE 0.021 0.009 RMSE 0.146 0.096 MAPE 0.311 0.203 TABLE II. EVALUATION METRICES OF THE THREE MODELS IN LOAD - DEMAND FORECASTING OF LARGE BUILDINGS Metric Transformer LSTM RNN MSE 0.009 0.011 0.051 RMSE 0.096 0.106 0.227 MAPE 0.203 0.217 0.471 V. CONCLUSIONS This paper proposed building-to-building transfer learning for energy load forecasting, which aims for enhancing the performance of the Deep Learning models used currently in the field; by harnessing the knowledge lea rnt from buildings with enough data, and use it to boost the accuracy of predictions for buildings with scarce data. To validate the approach , a case study for a large building was presented, where the data used in our experiments was representing only 2.5 months generated hourly. The paper compared the performance of the mostly used Deep Learning models in time -series forecasting before and after building-to-building transfer learning. This work was the first in utilizing transfer learning and applying it to the Transformer model in the field of energy load forecasting, where the performance gain achieved by the Transformer according to MSE was 56.8 after applying our method. Future work will apply the same methodologies to different building domains and sizes to investigate the different outcomes. More research will focus into finding the limitations of this approach, mainly the least amount of data used with transfer learning to increase the performance. REFERENCES [1] Eia.gov, 2022. [Online]. Available: https:www.eia.govconsumptioncommercialdata2018pdfCBECS_2 018_Building_Characteristics_Flipbook.pdf. [Accessed: 15- Feb- 2022]. [2] Eia.gov, 2022. [Online]. Available: https:www.eia.govtodayinenergydetail.php?id39092. [Accessed: 15- Feb- 2022]. [3] Frequently Asked Questions (FAQs) - U.S. Energy Information Administration (EIA), Eia.gov, 2022. [Online]. Available: https:www.eia.govtoolsfaqsfaq.php?id87t1. [Accessed: 15- Feb- 2022]. [4] Pinzon, P. Vergara, L. da Silva and M. Rider, Optimal Management of Energy Consumption and Comfort for Smart Buildings Operating in a Microgrid, IEEE Transactions on Smart Grid, vol. 10, no. 3, pp. 3236 - 3247, 2019. Available: 10.1109tsg.2018.2822276. [5] T. Yalcinoz and U. Eminoglu, Short -term and medium -term power distribution load forecasting by neural networks, Energy Conversion and Management, vol. 46, no. 9 -10, pp. 1393 -1405, 2005. Available: 10.1016j.enconman.2004.07.005. [6] H. A l-Hamadi and S. Soliman, Long -termmid-term electric load forecasting based on short-term correlation and annual growth, Electric Power Systems Research, vol. 74, no. 3, pp. 353 -361, 2005. Available: 10.1016j.epsr.2004.10.015 [7] M. Ghiassi, D. K. Zimbra, a nd H. Saidane, Medium term system load forecasting with a dynamic artificial neural network model, Electric Power Systems Research, vol. 76, no. 5, pp. 302 316, Mar. 2006, doi: 10.1016j.epsr.2005.06.010. [8] N. Ayub et al., Big Data Analytics for Short and Medium-Term Electricity Load Forecasting Using an AI Techniques Ensembler, Energies, vol. 13, no. 19, Art. no. 19, Jan. 2020, doi: 10.3390en13195193. [9] L. Han, Y. Peng, Y. Li, B. Yong, Q. Zhou, and L. Shu, Enhanced Deep Networks for Short -Term and Mediu m-Term Load Forecasting, IEEE Access, vol. 7 , pp. 40454055, 2019 , doi: 10.1109ACCESS.2018.2888978. [10] L. Han, Y. Peng, Y. Li, B. Yong, Q. Zhou, and L. Shu, Enhanced Deep Networks for Short -Term and Medium -Term Load Forecasting, IEEE Access, vol. 7 , pp. 40454055, 2019 , doi: 10.1109ACCESS.2018.2888978. [11] P. S. Kalekar, Time series Forecasting using Holt -Winters Exponential Smoothing, Kanwal Rekhi School of Information Technology, p. 13, 2004. [12] I. Mpawenimana, A. Pegatoquet, V. Roy, L. Rodriguez, and C. Belleudy, A comparative study of LSTM and ARIMA for energy load prediction with enhanced data preprocessing, in 2020 IEEE Sensors Applications Symposium (SAS), Mar. 2020, pp. 1 6. doi: 10.1109SAS48726.2020.9220021. [13] E. Mocanu, P. H. Nguyen, M. Gibescu, and W. L. Kling, Deep learning for estimating building energy consumption, Sustainable Energy, Grids and Networks, vol. 6, pp. 91 99, Jun. 2016, doi: 10.1016j.segan.2016.02.005. [14] C. Fan, Y. Sun, Y. Zhao, M. Song, and J. Wang, Deep learn ing-based feature engineering methods for improved building energy prediction, Applied Energy, vol. 240, pp. 35 45, Apr. 2019, doi: 10.1016j.apenergy.2019.02.052. [15] M. Hambali, Akinyemi, M. Oladunjoye, and Y. N., Electric Power Load Forecast Using Decision Tree Algorithms, vol. 7, pp. 2942, Jan. 2017. [16] Y. Fu, Z. Li, H. Zhang, and P. Xu, Using Support Vector Machine to Predict Next Day Electricity Load of Public Buildings with Sub-metering Devices, Procedia Engineering, vol. 121, pp. 10161022, Jan. 2015, doi: 10.1016j.proeng.2015.09.097. [17] N. G. Paterakis, E. Mocanu, M. Gibescu, B. Stappers, and W. van Alst, Deep learning versus traditional machine learning methods for aggregated energy demand prediction, in 2017 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe), Sep. 2017, pp. 16. doi: 10.1109ISGTEurope.2017.8260289.. [18] C. Yang, Q. Cheng, P. Lai, J. Liu, and H. Guo, Data -Driven Modeling for Energy Consumption Estimation: Applications, in Green Energy and Technology, 2018, pp. 10571068. doi: 10.1007978-3-319-62575-1_72. [19] A. Gonzalez-Vidal, A. P. Ramallo -Gonzalez, F. Terroso -Saenz, and A. Skarmeta, Data driven modeling for energy consumption prediction in smart buildings, in 2017 IEEE International Conference on Big Data (Big Data), Boston, MA, Dec. 2017, pp. 4562 4569. doi: 10.1109BigData.2017.8258499. [20] V. J. Mawson and B. R. Hughes, Deep learning techniques for energy forecasting and condition monitoring in the manufacturing sector, Energy and Buildings, vol. 217, p. 10996 6, Jun. 2020, doi: 10.1016j.enbuild.2020.109966. [21] R. K. Agrawal, F. Muchahary, and M. M. Tripathi, Long term load forecasting with hourly predictions based on long -short-term-memory networks, in 2018 IEEE Texas Power and Energy Conference (TPEC), Feb. 2018, pp. 16. doi: 10.1109TPEC.2018.8312088. [22] .S. Lee et al., Deep Neural Network Approach for Prediction of Heating Energy Consumption in Old Houses, Energies, vol. 14, no. 1, Art. no. 1, Jan. 2021, doi: 10.3390en14010122. [23] Z. Zhao et al., Short-Term Load Forecasting Based on the Transformer Model, Information, vol. 12, no. 12, Art. no. 12, Dec. 2021, doi: 10.3390info12120516. [24] M. Jain, K. Gupta, A. Sathanur, V. Chandan, and M. M. Halappanavar, Transfer-Learnt Models for Predicting Electr icity Consumption in Buildings with Limited and Sparse Field Data, in 2021 American Control Conference (ACC), May 2021, pp. 2887 2894. doi: 10.23919ACC50511.2021.9483228. Model [25] Huijuan Wu, Keqilao Meng, Daoerji Fan, Zhanqiang Zhang, Qing Liu, Multistep Short -Term Wind Speed Forecasting Using Transformer, Energy, Vol. 276, Dec. 2022, doi: 10.1016j.energy.2022.125231. [26] G. Pinto, R. Messina, H. Li, T. Hong, M. S. Piscitelli, and A. Capozzoli, Sharing is caring: An extensive analysis of parameter -based transfer learning for the prediction of building thermal dynamics, Energy and Buildings, vol. 276, p. 112530, Dec. 2022, doi: 10.1016j.enbuild.2022.112530. [27] S. Hochreiter and J. Schmidhuber, Long Short -Term Memory, Neural Computation, vol. 9, no. 8, pp. 1735 1780, Nov. 1997, doi: 10.1162neco.1997.9.8.1735. [28] D. Bahdanau, K. Cho, and Y. Bengio, Neural Machine Translation by Jointly Learning to Align and Translate, undefined, 2015, Accessed: Mar. 03, 2022. [Online]. Available: https:www.semanticscholar.orgpaperNeural-Machine-Translation-by- Jointly-Learning-to-Bahdanau- Chofa72afa9b2cbc8f0d7b05d52548906610ffbb9c5 [29] T. Luong, H. Pham, and C. D. Manning, Effective Approaches to Attention-based Neural Machine Translation, in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, Sep. 2015, pp. 14121421. doi: 10.18653v1D15-1166. [30] A. Vaswani et al., Attention is All you Need, in Advances in Neural Information Processing Systems, 2017, vol. 30. Accessed: Mar. 01, 2022. [Online]. Available: https:proceedings.neurips.ccpaper2017hash3f5ee243547dee91fbd05 3c1c4a845aa-Abstract.html [31] Ashrae - Great Energy Predictor III, Kaggle. [Online]. Available: https:www.kaggle.comcashrae-energy-prediction. [Accessed: 17-Mar- 2022]. View publication stats",
    "page_start": null,
    "page_end": null,
    "word_count": 5323,
    "created_at": "2025-08-18T07:03:33",
    "updated_at": "2025-08-18T07:03:33"
  },
  {
    "id": "2f176110a58a438098af1d209224072b",
    "doc_id": "f08ce7e53eb146108f401bb43982715a",
    "doc_name": "A_comprehensive_survey_of_federated_transfer_learn_2.pdf",
    "heading": "Document",
    "content": "A comprehensive survey of federated transfer learning: challenges, methods and applications Wei GUO1, Fuzhen ZHUANG ()1,2, Xiao ZHANG ()3, Yiqi TONG1, Jin DONG4 1 Institute of Artificial Intelligence, Beihang University, Beijing 100191, China 2 SKLSDE, School of Computer Science, Beihang University, Beijing 100191, China 3 School of Computer Science and Technology, Shandong University, Shandong 266237, China 4 Beijing Academy of Blockchain and Edge Computing, Beijing 100080, China The Author(s) 2024. This article is published with open access at link.springer.com and journal.hep.com.cn Abstract Federated learning (FL) is a novel distributed machine learning paradigm that enables participants to collaboratively train a centralized model with privacy preservation by eliminating the requirement of data sharing. In practice, FL often involves multiple participants and requires the third party to aggregate global information to guide the update of the target participant. Therefore, many FL methods do not work well due to the training and test data of each participant may not be sampled from the same feature space and the same underlying distribution. Meanwhile, the differences in their local devices (system heterogeneity), the continuous influx of online data (incremental data), and labeled data scarcity may further influence the performance of these methods. To solve this problem, federated transfer learning (FTL), which integrates transfer learning (TL) into FL, has attracted the attention of numerous researchers. However, since FL enables a continuous share of knowledge among participants with each communication round while not allowing local data to be accessed by other participants, FTL faces many unique challenges that are not present in TL. In this survey, we focus on categorizing and reviewing the current progress on federated transfer learning, and outlining corresponding solutions and applications. Furthermore, the common setting of FTL scenarios, available datasets, and significant related research are summarized in this survey. Keywords federated transfer learning, federated learning, transfer learning, survey 1 Introduction In recent years, we have witnessed breakthroughs in machine learning, especially deep neural networks (DNNs), in various fields such as computer vision, smart cities, health care, and recommendation systems. Driven by high-quality training data, these methods have achieved impressive performance and even outperformed humans in certain tasks. With the rapid growth of the mobile Internet, a large amount of data is produced by billions of smart devices. However, these collected data cannot be directly uploaded to cloud servers or data centers for centralized processing due to limitations in data security, user privacy protection, and network bandwidth, which poses substantial challenges to the traditional machine learning approach. Such phenomena is commonly known as  isolated data islands. One emerging paradigm for enabling distributed machine learning to solve this problem is federated learning (FL), which was first proposed by [1]. The main idea of FL is to collaboratively train a centralized machine learning model with privacy preservation by transmitting and aggregating model parameters between the distributed participants, which eliminates the requirement of local data sharing and each participant can maintain ownership of their data. However, in certain FL scenarios, the data distribution varies widely between participants. For example, the training data from different participants share the same feature space but may not share the same sample ID space, or the training data from different participants may not even share the same feature space [2]. Therefore, when participants want to utilize global information to improve model utility through FL aggregation, the difference in data distributions, feature space, and label space among participants will influence the model convergence to the optimum [36]. Furthermore, due to inconsistent local storage, computational, and communication capabilities among different participant devices, FL may grapple with system heterogeneity challenges, leading to straggler situations or high error rates. In addition to the above-mentioned data heterogeneity and system heterogeneity problems, FL also suffers from model heterogeneity, incremental data, and labeled data scarcity challenges, which are also focal points of attention among many researchers. To address the aforementioned challenges, transfer learning (TL) is employed in FL as an effective method of facilitating knowledge transfer between source and target domains [7]. The main concept of TL is to minimize the divergence between the distributions of different domains. Similarly, in one communication round of FL, we could consider each Received January 12, 2024; accepted April 21, 2024 E-mail: zhuangfuzhenbuaa.edu.cn; xiaozhangsdu.edu.cn Front. Comput. Sci., 2024, 18(6): 186356 https:doi.org10.1007s11704-024-40065-x REVIEW ARTICLE participant as the target domain and the other participants as the source domains. Given that FL often involves multiple participants, i.e., multiple source domains, and requires the central server to aggregate information (e.g., model parameters) from multiple participants to guide the update of the target participant. In the process of continuous interaction among participants, knowledge is mutually transferred, which allows a local model obtained from a specific domain to be used by other participants through TL, thus alleviating limitations such as data heterogeneity, system heterogeneity, incremental data, and labeled data scarcity. We rethink FL in [ 8] from the perspective of TL, and refer to the combination of FL and TL as federated transfer learning (FTL) shown in Fig. 1. However, in classical TL strategies, the target domain can directly access the source domain data or model information, which contradicts the principle of FL. Hence, these TL strategies could not be directly applied in the FL. Moreover, the standard FL scheme contains a sending and receiving process through the communication between participant and server to ensure that the global model is updated and optimized across all local participants. So in a communication round, local participants can act as source and target domains at different stages. Concretely, during the sending stage, each participant acts as a source domain to transfer local knowledge to other participants. During the receiving stage, each participant serves as the target domain to receive knowledge from others. These conditions increase the difficulty of applying TL to FL. Overall, the above unique challenges of FTL have captured the attention of numerous researchers, and many significant contributions have been made. Existing surveys in the FL field mainly focus on traditional FL [3,911], including horizontal federated learning, vertical federated learning [6], incentive mechanism [12 ], privacy protection [13,14], or introducing FL applications such as healthcare [15,16], mobile edge networks [ 17], and internet of things (IoT) [18]. Despite some studies [19,20] focus on not identically and independently distributed (Non-IID) or other heterogeneous scenarios, such as model heterogeneity, device heterogeneity in FL, there is still a lack of systematic and comprehensive review on the definition, challenges, and corresponding solutions specific for the application of TL in FL, i.e., FTL. To fill this gap, this survey is dedicated to giving a comprehensive survey of FTL, including definitions, a categorization, and a discussion of existing challenges and corresponding solutions, common setting scenarios of distribution heterogeneity, available datasets, as well as an outline of current FTL applications and future prospects. In detail, Fig. 2 shows the categorizations of FTL and corresponding solutions. Section 2.1 demonstrates related definitions of FL and TL, we classify the common settings of FTL scenarios into six categories, including homogeneous FTL, heterogeneous FTL, dynamic heterogeneous FTL, model adaptive FTL, semi-supervised FTL, and unsupervised FTL. Sections 3.1 to 3.5 systematically summarize the corresponding solutions of existing FTL works in these scenarios, including motivation, core algorithm, model design, privacy-preserving mechanism, and communication architecture they adopt. Since some studies have involved multiple FTL scenarios, we only describe the major issues addressed by these studies. Finally, recognizing that systems and infrastructure are critical to the success of FTL, we outline current applications of FTL and propose future prospects. The key contributions of this work are summarized as follows. Fig. 1 The overview of FTL 2 Front. Comput. Sci., 2024, 18(6): 186356 1. This survey is the first to systematically and comprehen- sively rethink FL based on TL (FTL). We provide the definitions of FTL and its challenges including homogeneous FTL, heterogeneous FTL, dynamic heterogeneous FTL, model adaptive FTL, semi- supervised FTL, and unsupervised FTL, and further detail these challenges of FTL through examples. 2. Based on existing FTL solutions, which include both data-based and model- based strategies, we give the current research status for FTL challenges. 3. We summarize the scenario settings of the homogene- ous FTL shown in Section 2.3.1, which is the most common situation in FTL, including the setup methods and applied datasets. Meanwhile, to make checking convenient, we outline the existing research on FTL. 2 Overview In this section, the common notations used in this survey are listed in Table 1 for convenience. Besides, we further introduce the definitions, categorizations, and open challenges related to transfer learning, federated learning, and federated transfer learning. 2.1 Definition Following with previous works [7,8], we first give the definitions of domain, task, transfer learning, and  federated learning that are used in this survey, respectively. Fig. 2 Categorizations of FTL Table 1 The common notations Symbol Definition Symbol Definition n Number of instances m Number of domains k Number of participants k Actual number of participants z Number of classes l Number of model layers p Number of servers s Server u Participant g Global d Threshold f Decision function x Feature vector y Label e Communication round F Device A Active participant B;C Passive participant D Domain T Task X Feature space Y Label space X Instance space I Sample ID space Y Label set corresponding to X S Source domain T Target domain L Labeled instances U Unlabeled instances L Loss function R Relationship matrix M Model E Extractor  Mean  Variance  Importance variable  Tradeoff parameter  Interpolation coefficient Ω Structural risk  Model parameters Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 3 The involved common notations are summarized in Table 1. D X P(X) X X  fxjxi 2 X;i  1;:::; ng D D  fX; P(X)g X P(X) Definition 1 (Domain) A domain is constituted by two elements: a feature space and an probability distribution , where the symbol represents an instance set, i.e., . Thus, a domain can be denoted as . In general, if two domains are different, then they may have different feature spaces or different probability distributions [21]. T Y f T  fY; f g f y 2 Y f Definition 2 (Task) A task is constituted by a label space and a decision function , denoted as . Given the training data, the decision function is used to predict the corresponding label , where is not explicit but can be inferred from the sample data. mS 2 N f(DS i ;TS i )ji  1; :::; mS g mT 2 N f(DTj ;TT j )jj  1; :::; mT g f T j ( j  1; :::; mT ) Definition 3 (Transfer Learning) Given ansome observa- tion(s) corresponding to source domain(s) and task(s) (i.e., ), and ansome observation(s) about target domain(s) and task(s) (i.e., ), transfer learning aims to utilize the knowledge implied in the source domain(s) to improve the performance of the learned decision functions on the target domain(s) [7]. k u1; :::; uk X1; :::; Xk X  X1 [ [ Xk MS UM MF ED Xi Definition 4 (Federated Learning) Assume there are participants , each aiming to train a machine learning model with their own private datasets . A conventional approach is to upload all data together and use to train a global model . However, in many application scenarios, participants cannot directly upload their own data or access the data of other participants. Therefore, a typically federated learning system is a distributed learning process in which the participant collaboratively train a model without sharing local private data [8]. i j i  1; :::; k Xi , Xj Yi , Yj Pi(Xi;Yi) , Pj(Xi;Yj) X X Y Fi , F j Xi e1 , Xi e  i Xi  XL ! 0 k u1; :::;uk fspjp  1; :::;Ng E Definition 5 (Federated Transfer Learning) Given there are some challenges in FL for participants and ( ), including data heterogeneity ( or or , where instance space consists of feature space and label space ), system heterogeneity ( ), incremental data ( ), and scarcity of labeled data ( ), the FL combines the TL to solve these challenges, called FTL. The specific definition of FTL as follows. Given participants in FL, the central server set is designed to achieve model convergence over the communication rounds. During each communication round, the typical federated transfer learning process includes two distinct stages: ui 1  i  k DS i 1  i  mS DS i TS i DS i TS i sp X1; :::; Xk 1. Sending stage: participant(s) ( ) is(are) assumed as the role of the source domain(s) ( ), where they are responsible for contributing local ansome observation(s) ( , ) corresponding to and task(s) to the central server without sharing local raw data . The server then leverages the collected sending information to implement the aggregation process. uj 1  j  k f(DTj ;TT j )jj  1; :::;mT g 2. Receiving stage: once the aggregation is complete, participant(s) ( ) then are assumed as the role of the target domain(s) and utilize the received global aggregation information to perform local model updates. E p  0 The above sending and receiving stages are assumed to repeat for communication rounds, or until the model is observed to converge. Particularly, when , the above process is considered as a decentralized federated transfer learning process. 2.2 Category of federated learning According to the characteristics of data distribution among connected participants, FL can be categorized into horizontal FL (HFL) and vertical FL (VFL). Generally, HFL considers the distributed participants to have data with the same features but are different in sample space, while VFL considers the distributed participants to have the same samples but different features to jointly train a global model [6,19]. Federated transfer learning in [8] refers that these participants have differences in both feature space and label space. Due to the limited research on federated transfer learning in [8], this survey categorizes federated transfer learning and VFL as a type of VFL for description. On the other hand, depending on whether there is asome central server(s) responsible for coordinating participants, FL can also be divided into centralized FL (CFL) and decentralized FL (DFL), where CFL assumes that there is asome server(s) to gather local model-related information or other training information from the participants and then distributes the updated global model back to the participants, while DFL assumes participants directly aggregate information from neighboring participants [8]. In the following, we will provide a brief introduction to these FL frameworks and discuss the various settings of source domains, target domains, and tasks when employing transfer learning within these frameworks. 2.2.1 Horizontal federated learning X I i j Xi  Xj Ii  Ij S T HFL is commonly found in scenarios where participants share the same feature space but different sample space , which meets homogeneity FTL described in Section 2.3.1. For example, the medical record data of two regional hospitals and may be very similar due to they use the same information system, which both record the patients name, age, gender, and other user private data, so their feature spaces are the same ( ). However, the two hospitals have different user groups ( ) from their respective regions, and the user intersection of their local datasets is very limited. In FTL, any participant can serve as a source domain ( ) to provide knowledge or as a target domain ( ) to receive knowledge from other participants in the same feature space, therefore, we define HFL in homogeneous FTL as: XS i  XT j ;YS i  YT j ; IS i , IT j ;8XS i ; XT j orXT i  XS j ;YT i  YS j ; IT i , IS j ;8XT i ; XS j ;i , j: From an extended perspective, HFL meets heterogeneous FTL 4 Front. Comput. Sci., 2024, 18(6): 186356 when participants label space is inconsistent in the knowledge-transferring process, the HFL in heterogeneous FTL can be represented as: XS i  XT j ;YS i , YT j ; IS i , IT j ;8XS i ; XT j orXT i  XS j ;YT i , YS j ; IT i , IS j ;8XT i ; XS j ;i , j: 2.2.2 Vertical federated learning X I I i j i Xi Xj Xi , Xj Unlike HFL where all participants have their own local data labels, in the VFL scenario, participants feature spaces are inconsistent, and their sample spaces may also not be entirely the same. For example, suppose there is a high degree of overlap in the customer groups between a bank and a telecommunications company in the same region. The bank has information on users credit history ( ), such as loan repayment details and credit card usage, while the telecommunications company holds data on users call logs, data usage, and payment records ( ), where the feature space is different ( ). These two entities, which all act both as source domains or as target domains, can engage in VFL to mutually enhance their services in different feature spaces. We summarize VFL in heterogeneous FTL as: XS i , XT j ;YS i  (,)YT j ; IS i  (,)IT j ;8XS i ; XT j orXT i , XS j ;YT i  (,)YS j ; IT i  (,)IS j ;8XT i ; XS j ;i , j: 2.2.3 Centralized federated learning Standard CFL requires one or more central servers to build a global model by collecting local information from distributed participants [22], which involves three fundamental steps as described below: 1. Receiving stage: each participant receives the initial model sent by the server. ui Xi 2. Sending stage: participants use their own private data to train the local model (add local model notion), and then send the local model to the server. Mg 3. Receive stage: The central server updates the global model by collecting and aggregating all the local updates and then sends the updated global model back to the participants. In the FTL setting, during the sending and receiving stages, participants share knowledge through a central aggregation strategy, where each participant can act as a source domain providing knowledge or a target domain receiving knowledge. For instance, during the sending stage, participants act as source domains providing model parameters, while the server acts as the target domain, aggregating these parameters to form a global model. Conversely, in the receiving stage, the server serves as the source domain providing global model parameters to each participant. 2.2.4 Decentralized federated learning fu1; :::; ukg Compared with CFL, DFL is conducted over different participants without a central parameter server for global model aggregation. Each participant uses a private local dataset to optimize their local model after receiving model updates from other participants. This process involves two fundamental steps as described below: ui Mi Xi Mi fu1; :::; uk1g 1. Receiving stage: participant federally train its initial model locally with its own dataset , and then send the model to other participants without direct data exposure. ui Mg fM1; :::; Mkg 2. Sending stage: participant obtain the aggregated model by aggregating the received local model , and then update local model with aggregated model. Similar to CFL, each participant in DFL could still serve as either a source domain or a target domain without a central server during different stages of the FL process. 2.3 Federated transfer learning Transfer learning has achieved remarkable success by enabling the application of knowledge from one domain to improve performance in another, significantly reducing the need for extensive data collection and training time in new tasks [2326]. However, as shown in Fig. 3, constrained by the unique distributed learning paradigm of FL, current FTL studies face many additional challenging situations, including homogeneous FTL, heterogeneous FTL, dynamic heterogeneity FTL, model adaptive FTL, semi-supervised FTL and unsupervised FTL. More descriptions are presented below. 2.3.1 Homogeneous federated transfer learning i j Di Dj Di , Dj Xi , Xj Pi(X) Pj(Y) i j Ti , Tj Yi , Yj Pi(yjx) , Pj(yjx) Assume that the local data of participant and participant constitute a source domain and a target domain , respectively. represents a difference in either their feature spaces ( ) or marginal distributions ( or ). Similarly, if the task between participant and participant is not same, that is , then there is a difference in their label spaces ( ) or in their conditional distribution ( ). Pi(X) Pj(Y) Pi(yjx) , Pj(yjx) ni , nj HOFTL refers to differences in marginal distributions ( or ), conditional distributions ( ), or sample sizes between participant data, which is often caused by diversity in domain or task between participants. Based on this, HOFTL includes five scenarios: Pi(Y) , Pj(Y)  Prior shift: Pi(X) , Pj(X)  Covariate shift: Pi(xjy) , Pj(xjy)  Feature concept shift: Pi(yjx) , Pj(yjx)  Label concept shift: ni , nj  Quantity shift: Specifically, as described in Subsection 2.2.1, horizontal federated learning assumes that participants have the same feature space, so HOFTL is a form of transfer learning under this HFL assumption. Moreover, HOFTL can also be presented in vertical federated learning when there is partial overlap in the feature space between participants. Unless specifically stated, the HOFTL methods discussed in this survey are all related to HFL. Overall, compared with homogeneous transfer in traditional transfer learning that only considers marginal and conditional probability distributions, Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 5 HOFTL also considers changes in the total sample size, which corresponds to the Non-IID data setting in federated learning. The detailed challenges of HOFTL are described below, and considering that homogeneous FTL is one of the most frequently discussed, we have summarized the specific settings for each homogenous FTL scenario demonstrated in Table 2.  Prior shift P(Y) P(yjx) Prior shift, also known as class imbalance, implies that the prior probability distribution could be inconsistent between different participants when the conditional probability distribution is consistent [182]. In FL, the prior probability distribution inconsistency may occur when different participants have different class distributions in their local datasets. If these differences are not properly handled, they can lead to a federated model that performs unfair and suboptimal performance. For example, an FL system is designed to improve predictions for a specific disease (e.g., diabetes) across different hospitals that participate in model training without sharing private patient records. Hospital A is located in an urban area with a high prevalence of diabetes, possibly due to lifestyle factors prevalent in the population it serves. As a result, in hospital As patient data, 30 of patients might have diabetes. On the other hand, hospital B serves a rural area with a different demographic and lifestyle, resulting in only 10 of its patients having diabetes. This difference in the prevalence of diabetes is a classic example of prior probability shift. In some extreme cases, hospital B may even have no positive cases for this disease.  Covariate shift P(X) P(yjx) Covariate shift, or feature distribution imbalance, describes a situation where the input feature distribution is varied between participants while the conditional probability remains consistent. This presents a unique challenge in FL because the global model is trained on data from multiple participants, and each participants local data may represent a different underlying distribution of input features. For example, the patient population at hospital A had a higher average body mass index (BMI), which is a known risk factor for diabetes, while the patient population at hospital B had a lower average BMI. There is a significant difference in the input data (in this case, the BMI distribution) between the two hospitals, known as covariate shift.  Feature concept shift x y P(xjy) P(y) x y Concept drift, which includes feature concept shift and label concept shift, refers to the change in the relationship between variables and , where feature concept shift implies to discrepancy among participants with the same prior distribution [182]. This type of shift can be particularly challenging in federated learning because models need to generalize across all participants data. For example, consider two hospitals A and B jointly predicting the incidence of diabetes, where   represents the patients health characteristics and   represents the presence or absence of diabetes. Hospital As diabetic population mostly has a higher socioeconomic status, resulting in a different set of health characteristics, such as better control of blood sugar levels and fewer complications. In contrast, patients with diabetes at Fig. 3 The challenges of FTL 6 Front. Comput. Sci., 2024, 18(6): 186356 x y P(xjy) hospital B may have lower socioeconomic status and poorer health characteristics, such as uncontrolled blood sugar levels and higher rates of complications. Differences in the distribution of health characteristics ( ) for a given diabetic patient ( ) are an example of a shift.  Label concept shift P(yjx) P(x) P(yjx) A A ui A Similar to feature concept shift, the label concept drift refers to inconsistent among participants with the same covariate distribution [182]. Some external events or changes may lead to changes of in either the sourcetarget domain, which further renders the models from the sourcetarget domain no longer suitable for tasks in the targetsource domain. For example, in a federated recommendation system, geographical location is commonly used as the input feature to predict users favorite items. Thus, if the emergence of tendentious policy or new pillar industries supports the economic development of area , the consumption level of will be improved. In this situation, the expected user preference will change, causing the prediction results of participant from to become invalid and unsuitable for the improvement of model predictions from other regions participants.  Quantity shift Different from the prior shift, quantity shift refers to the situation where there is a significant imbalance in the number of training samples available among participants. In FL, some participants might have a large dataset, while others may have a relatively small one. This can lead to a situation where the global model is disproportionately influenced by participants with more data, potentially leading to biases or overfitting to the characteristics of those datasets. For example, a large-scale hospital may have thousands of patient records, while a small clinic may only have a few hundred. This difference in data volume is a classic example of quantity shift in FL. In summary, with uniform feature and label spaces, participants in homogeneous FTL still face data distribution shift problems, including prior shift, covariate shift, feature concept shift, label concept shift, and quantity shift. Most current FTL studies focus on prior and quantity shifts, with few studies tackling covariate shifts. Feature concept shift and label concept shift are even less explored. However, external elements change, like time or policy, may change the Table 2 Data heterogeneity settings of HOFTL and HEFTL Problem categorization Setting Reference Dataset HOFTL Prior shift Fixed ratio [2733] CIFAR-101), CIFAR-1001), MNIST2), Tiny-Imagenet3), ImageNet3), FEMNIST4), OFFICE [34], DIGIT [35], OpenImage [36], WESAD [37], KDD995), SVHN6), HAR7), OFFICE-Caltech 108), MIMIC-III9), Shakespeare [1], DomainNet10), NSL-KDD9911), CINIC10 [38], CelebA [39], StackOverflow [40] Natural partition [4157] 1 classparticipant [37,5860]  1 classesparticipant [ 1,48,59,61117] Dirichlet Distribution [118121,121133], [32,81,93,112,134146] JensenShannon divergence [147] Half-normal distribution [47,148] Log-normal distribution [79] Covariate shift 1 domainparticipant [41,65,125,149151], [1,70,76,129,152156], [89,91,96,112,157166] Mixed domainparticipant [167,168] Feature concept shift 1 degreeparticipant [169] Label concept shift [89,163] Quantity shift Natural [52,54,89,98,159,170,171], [109,114,161,172] By data source [1,61,125,157] By parameter [88] HEFTL Feature space hetergeneity Overlapped feature [ 37,87,133,173175] CIFAR-101), CIFAR-1001), MNIST2), MovieLens [176], ModelNet [177], FEMNIST4) NUS-WIDE [176] Non-overlapped feature [176,178181] Label space heterogeneity Feature and label space heterogeneity 1) See cs.toronto.edukrizcifar.html website. 2) See kaggle.comdatasetshojjatkmnist-dataset website. 3) See kaggle.comctiny-imagenet website. 4) See github.comwenzhu23333Federated-Learning website. 5) See kdd.ics.uci.edudatabaseskddcup99 website. 6) See ufldl.stanford.eduhousenumbers website. 7) See github.comxmouyangFL-Datasets-for-HAR website. 8) See v7labs.comopen-datasetsoffice-caltech-10 website. 9) See physionet.orgcontentmimiciii-demo1.4 website. 10) See ai.bu.eduM3SDA website. 11) See s.uci.edudataset227nomao website. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 7 relationship between features and labels for partial participants while leaving others unchanged. This intensifies the feature concept drift and label concept drift among participants, which is worth deeper study in the future. 2.3.2 Heterogeneous federated transfer learning i j Di Dj Di , Dj Ti , Tj Xi , Xj Yi , Yj HEFTL mainly refers to the problem of inconsistency in feature or label space between participants in FL. To the specific, similar to HOFTL, it is assumed that there are two participants and , whose private data constitute the source domain and the target domain , respectively. HEFTL demonstrates the differences in either their domain ( ) or task ( ), which caused by their various feature spaces ( ) orand label space ( ). Based on this, HEFTL has three scenarios: Xi , Xj  Feature space heterogeneity: Yi , Yj  Label space heterogeneity: Xi , Xj Yi , Yj  Feature and label space heterogeneity: and Unlike HOFTL, where all participants train models with the same data structure, HEFTL allows for collaboration between datasets that are not identically structured. Thus, vertical federated learning is a prime case for HEFTL, since the local private data among participants in VFL may contain different sets of attributes or dimensions. Next, we give a detailed description of the settings as mentioned above.  Feature space heterogeneity X Y A B Feature space heterogeneity refers to the situation that the feature space of different participants is inconsistent, while the label space is consistent, particularly when different datasets involved in the training process have different sets of features. For example, in FL, two retailers are trying to identify fake reviews by local model prediction. Retailer has a feature space that includes review length, the number of purchases, and purchase history, whereas retailer utilizes review timing, user location, and account age as feature space. They all annotated their reviews with binary labels as true (0) or fake (1). Although the reviews obtained by different retailers have inconsistent feature space, these retailers still aim to leverage FL to enhance the predictive performance of their respective local models within a consistent label space.  Label space heterogeneity X Y A C Label space heterogeneity refers to the situation where different participants have consistent feature space but inconsistent label space , which is the exact opposite of feature space heterogeneity. For example, in FL, two international e-commerce platforms are aiming to improve their recommendation systems. Each platform operates in a different region and thus has different product categories that are relevant to their local markets. Platform serves the Asian market and uses categories like Apparel, Gadgets, Furniture, and Anime Merchandise. Platform is based in North America, and uses labels such as Clothing, Tech, Home Improvement, and Sports Equipment. All two platforms collect user data including browsing time, click- through rates, purchase history, and search queries, which make up their consistent feature space. However, the way they categorize their products (labels) varies due to regional differences in terminology and market demand, leading to an inconsistent label space.  Feature and label space heterogeneity X Y A B Feature and label space heterogeneity, indicates the feature space and label space are both inconsistent among different participants. For example, there are two different specialty health clinics using federated learning to predict if patients will need to return for more treatment. Each clinic has its own set of measurements and outcomes. Clinic focuses on heart health, measuring things like heartbeat patterns and blood tests, and is concerned with whether patients might come back with heart issues. Clinic is a general clinic in a remote area, tracking health indicators like blood pressure and weight, and wants to predict if patients will return for any follow-up care or need a specialist. Each clinic collects different health information (different feature spaces) and has different categories for what counts as a patient needing to return (different label spaces). Considering the health indicators may be helpful in predicting patients heart issues in the future. Thus, they want to use FL to build better prediction models without sharing sensitive patient data. In summary, heterogeneous FTL may occur when there is inconsistency in the participants feature spaces or label spaces. The existing research primarily focuses on FTL with heterogeneous feature spaces where only the feature spaces are inconsistent. Other heterogeneous situations in FTL remain worthy of deeper investigation. 2.4 Dynamic heterogeneous FTL DHFTL refers to the condition where the participant set that contributes to the FTL aggregation or the local raw data of partial participants in this set is dynamically changing at each round. We further provide detailed descriptions of the causes of dynamic heterogeneity. 2.4.1 System heterogeneity F rg e1 u1; :::;un e m(0  m  k;n , m) e rg e e rg e1 Each participants local device in FL could have different storage, computation, and communication abilities. Due to the varying storage or computational capabilities, some devices may not be able to complete the local training in time before aggregation. Meanwhile, the communication ability among participants is also influenced by network connections, and some devices may lose connection during a communication round because of connectivity or power issues [70,183,184]. These aspects greatly amplify the straggler issue in the aggregation process [185], forming a dynamically changing set of participants during FL iterations as shown in Fig. 4. Assuming that there is a global optimal direction for the global model aggregated by participants in communication round , and participants could send their local model to server in time due to devices limitation in communication round , the global optimal direction aggregated by participants in round may have a significant difference with when there is data heterogeneity among participants, which is not conducive to 8 Front. Comput. Sci., 2024, 18(6): 186356 u1; :::; uk Fi , F j;i; j 2 (1;k) the global model convergence. Therefore, how to transfer the knowledge among participants within a dynamically changing participant set is a key challenge for DHFTL. Dynamic heterogeneous FTL for participants caused by system heterogeneity ( ) can be expressed as: DS 1 ; :::; DS n  Se1 , Se  DS 1 ;:::; DS m; n m e 1 e Se e where and represent the actual number of participants in communication round and of FTL, respectively. indicates the set of actual participants in the communication round . 2.4.2 Incremental data u1; :::; uk Xi e1 , Xi e Real-world FL applications are often dynamic, where local participants receive the new data, classes or tasks in an online manner [102,186,187], which proposes a key challenge is how to execute FTL from dynamically changing data distributions [ 188,189]. If only some participants are constantly adding data, or even if each participant synchronously adds new data, the newly added data could disrupt the original local data distribution, potentially exacerbating the differences between participant distributions as represented in Fig. 5. This requires the model to generalize well across both the old and new domains [190,191]. In addition, its also possible that the feature space of the newly added data is inconsistent with the original feature space. Dynamic heterogeneous FTL for participants caused by incremental data ( fX;Y gi e1 , fX;Ygi e;i 2 (1;k) e or ) in communication round . Dynamic heterogeneous FTL in a participant can be written as: P(XS i;e1) , P(XS i;e) or fX;Y gS i;e1 , fX;YgS i;e: Another situation where dynamic heterogeneous FTL of multiple participants can be denoted as: P(XS i;e) , P(XS j;e) or fX;Y gS i;e , fX;YgS j;e:(i; j 2 (1;k)): Nevertheless, we can only observe popularity in typical incremental learning approach [188,189,191,192], while these problems in incremental FTL receive relatively less attention. 2.4.3 Model adaptive FTL M In practical scenarios, due to differences in training objectives, participants may employ different model architectures for training [164,193]. Therefore, employing conventional aggregation methods for heterogeneous models output representations or parameters, such as averaging participants parameters in FedAvg [1], cannot effectively complete knowledge transfer between participants in FL. Additionally, even if the dimensions of the intermediate feature outputs are consistent across different models, the representational capacity of these features for local data could still vary, hindering performance improvement of the model in the target participant [66,164,193,194]. TL generally assumes that the model architectures in the source domain and the target domain are consistent, however, the inconsistency of models Fig. 4 The detailed description of system heterogeneity in FTL. In each round of global communication, due to the resource heterogeneity among the participants, partial participants could not participate in the global aggregation in time, which results in the actual optimization direction of the aggregated model dynamically changing and deviating from the global optimal optimization direction Fig. 5 The detailed description of incremental data in FTL. In each round of global communication, due to the increase in the local user data or class, the local data distribution of participants may change, causing the actual optimization direction of the aggregated model to constantly vary and deviate from the global optimal optimization direction Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 9 u1; :::; uk Mi , Mj;i; j 2 (1;k) in FL poses a new challenge for the performance of target participants tasks by aggregating process. This challenge, implementing effectively federated training in a model heterogeneous setting, is referred to as model adaptive FTL, which can be denoted as for participants : 2.4.4 Semi-supervised and unsupervised FTL u1; :::; uk  i Xi  XL ! 0;i 2 (1;k) Real-world FL applications especially need to use unlabeled data more than others [195,196]. On the one hand, in cross- device federated learning [182], individual devices create a lot of unlabeled data, like photos, texts, and health record data from wearables. Its unrealistic to label all this data due to its large volume. On the other hand, cross-silo FL [182] involves businesses, where data labeling often needs special knowledge. This is common in finance and healthcare sectors. Labeling this data would be time-consuming and expensive. Thus, SSFTL and USFTL have caught the interest of some researchers [122,123,195]. Overall, SSFTL has two common scenarios: ① only one participant has labeled data; ② several participants each have a small amount of labeled data locally, where case ③ is often seen in VFL, where it s usually assumed that only one active party has data label information. USFTL in FTL refers to the scenario where all participants lack labeled information. The labeled data scarcity in FTL for participants is denoted as: . 3 Methodology We elaborate on the current research strategies for each FTL challenge mentioned in Section 2.3. As shown in Fig. 6, it mainly includes two mainstreams: data-based and model- based strategies. Specifically, data-based strategies emphasize knowledge transfer by modulating and transforming participants data for space adaptation, distribution adaptation, and data attribute preservation or adjustment [7 ] without exposing any raw private data. The model-based strategies aim to improve the predictive accuracy of any given participant by the models from other participants in FTL. Tables 35 demonstrate related works on solving FTL challenges through these strategies. Note that currently there are very few FL works that specifically address the issues of label space heterogeneity or label  feature space heterogeneity. Therefore, we will not discuss them in a separate subsection. 3.1 Homogeneous federated transfer learning Homogeneous FTL and heterogeneous FTL are two of the most studied challenges in FTL. We will first illustrate the strategies for addressing homogeneous FTL challenges from both data-based and model-based perspectives as shown in Fig. 6. 3.1.1 Prior shift This subsection describes solutions to the prior shift challenge in HOFTL, which is one of the most common issue in FTL.  Instance augmentation Instance augmentation in FTL aims to enhance data homogeneity of various participants through techniques like oversampling [27,28,208 210] and undersampling [211213], which mainly occurs on the side of the participants. In detail, FedHome [28], Astraea [47], and FEDMIX [61] consider resolving prior probability bias at the local participant level. However, they ignore the effectiveness of global information in the local augmentation process. Therefore, some studies [ 118,119] suggest bridging the gap between participant and global distribution by creating a public dataset, but this also increases the risk of privacy leakage. To mitigate this issue, Faug [ 29], an FTL approach based on the generative adversarial network (GAN), is proposed to avoid privacy issues from multiple data transfers. Faug trains a GAN on minority class data at a central location, then sends it back to participants for data generation, helping build independent and identically distributed (IID) datasets. However, the construction of the generator increases extra computational and communication costs. the study [62] introduces a batch normalization (BN) based data augmentation approach, involving the following steps: t ith i 2 1; :::; l 1. BN layer parameterization: In the round of global iteration, the BN layer ( ) of the global Fig. 6 Data-based and model-based strategies of FTL 10 Front. Comput. Sci., 2024, 18(6): 186356 Mg (i 1)th i i y( j)(1  j  z) z x(z) M( x( j)) 1  j  z ai i; i model from the round can be parameterized as a distribution of means and variances . For a given target category , where represents the total number of possible categories, participants will sample from the Gaussian distribution to generate samples by forward propagate ( ), meanwhile the intermediate activation values produced in sample process are applied to obtain the BN statistics ). x( j)  ar gminx l i1 i i2 2  i i2 2 LH (M ( x( j)); y( j)) H 2. Augmented Data Update: The computation of the loss function follows the formula: , where represents the cross-entropy loss. During the M x( j) backpropagation process, the parameters of the model are kept fixed, and only is updated to obtain augmented data that is closer to the real distribution.  Instance selection Instance selection aims to select a subset of the available data that is most representative or informative for the training process. In distributed learning, numerous studies [214220] suggest that constructing local training samples that are closer to the target distribution through instance selection methods can effectively enhance model performance. However, these methods are designed for traditional distributed training where training data is public. They are not suitable for FL, which uses private datasets from different owners. To solve this Table 3 FTL frameworks Reference FTL challenges Architecture StrategyDHFTL MAFTL SSFTL USFTL HOFTL HEFTL HFL VFL CFL DFLSystem Incremental PS CS FCS LCS QS FSH LSH FLSH [27,28,62]    IA [47]    IA,MS,FC [120]     IA [61]     IA [29]    IA, KD [121]     IA, FC [30,58]    IS [197]     IS [198]     IS [123]     IS [149]    FA [176]      FA [172]     FA [125]      FA [126]    FM [199]     FM [200]    FM [173]     FC [167]    FC,CR [37,175]    FS [174,178]    FS [49]    FS [181]     FS [179]     FS [65]     FC,MC [48]     FS [180]     FS,MS [150]     FAI [103]     FAI [201]     FAI [66]      FAI,MC [41]     CR [151]    CR [102]     CR,MS [50]     CR,PD [72]    CR,PR [67]     DCR,KD [128]    DCR [153]    DCR 1 Abbreviation: PS: prior shift; CS: covariate shift; FCS: feature concept shift; LCS: label concept shift; QS: quantity shift; FSH: feature space heterogeneity; LSH: label space heterogeneity; FLSH: feature and label space heterogeneity; HFL: horizontal FL; VFL: vertical federated learning; CFL: centralized FL; DFL: decentralized FL; IA: instance augmentation; IS: instance selection; FA: feature augmentation; FM: feature mapping; FS: feature selection; FC: feature clustering; FAI: feature alignment; CR: consistency regularization; DCR: domain-dependent consistency regularization; PS: parameter sharing; PR: parameter restriction; PD: parameter decoupling; PP: parameter pruning; MW: model weighting; MS: model selection; MC: model clustering; MI: model interpolation; KD: knowledge distillation. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 11 problem, the study [58] suggests using a benchmark model that is trained on a targeted small dataset before FL starts to evaluate the relevance of each participants data, where only highly relevant data is used for local training. However, this method does not consider the influence of other participants on the benchmark model during FL training. Accordingly, the study [197] finds that calculating the sample loss during FL training can reflect the samples homogeneity with the global data distribution. Among these, local samples with stronger homogeneity are more conducive to improving the global models utility. As a result, this study proposes an FL framework named FedBalancer, which employs a selection strategy based on sample loss to filter local samples, aiming to build a local training set that is better aligned with the global sample distribution. However, FedBalancer increases computational costs because it requires calculating the loss value for every individual local sample. The study [30] introduces a less computationally expensive method for selecting samples. This method uses the gradient upper bound norms of samples to assess their importance to global model performance. It calculates gradients from the loss of the last layers pre-activation output, rather than calculating the gradient from the overall model parameters. This usually requires just one forward pass to accurately estimate a samples importance. Specifically, the proposed algorithm includes the following two steps: 1. Participant selection: using the private set intersection (PSI) protocol, each participant is informed about the target categories relevant to the target task. Participants Table 4 FTL frameworks (continued) Reference FTL challenges Architecture StrategyDHFTL MAFTL SSFTL USFTL HOFTL HEFTL HFL VFL CFL DFLSystem Incremental PS CS FCS LCS QS FSH LSH FLSH [69]    PS [154]    PS [68,117]    PS,PD [202]      PR [75,77,78,130,131]    PR [74,76]     PR [73]    PR,MI [71]     PR,MI [63,79,80,83]    PD [155]    PD [81]     PD [84]     PD [82]     PD [156]     PD [110]     PD,MI [106]     PD,MC [133]        PD,KD [132]    PD,PP,MS [129]     PD,PR,MI [87]       PP [85]     PP [86]     PP,MC [134]     PP,KD [1]      MW [157]      MW [88]     MW [158]    MW [203]    MW [204]    MW [90]     MW [135]    MW,CR [89]       MW,CR [152]     MW,PD [122]     MW,MC [91]     MS [60,92,93,136,148]    MS [51]     MS [52]     MS 1 Abbreviation: PS: prior shift; CS: covariate shift; FCS: feature concept shift; LCS: label concept shift; QS: quantity shift; FSH: feature space heterogeneity; LSH: label space heterogeneity; FLSH: feature and label space heterogeneity; HFL: horizontal FL; VFL: vertical federated learning; CFL: centralized FL; DFL: decentralized FL; IA: instance augmentation; IS: instance selection; FA: feature augmentation; FM: feature mapping; FS: feature selection; FC: feature clustering; FAI: feature alignment; CR: consistency regularization; DCR: domain-dependent consistency regularization; PS: parameter sharing; PR: parameter restriction; PD: parameter decoupling; PP: parameter pruning; MW: model weighting; MS: model selection; MC: model clustering; MI: model interpolation; KD: knowledge distillation. 12 Front. Comput. Sci., 2024, 18(6): 186356 d with low relevance to the target task are filtered out. Reversely, if their total number of samples, which match the target categories, reach a certain threshold , these qualified participants can participate in global aggregation. This prevents participants with large category distribution bias from interfering with the global models convergence. tth u u (xu;i;t) fxu;ign i1 (xu;i;t) 2. Sample selection: during round, each participant of the selected participants measures the importance of samples related to the target tasks categories, where is defined as: (xu;i;t)   j  u;l t;l u;i t;l u;i f (xu;i;t)j2; t;l u;i; t;l u;i lth xu;i tth  t;l l  diag(l( 1); :::; l( rl )) j( )j ,  where are the input and output of the last layer ( ) of sample in the iteration, respectively. . and f (x;) : k u1 nu n fu()  (xu;i;t) . is the output matrix, is a trade-off parameter. The importance of a sample is indicated by the value of : higher values mean higher importance. (xu;i;t) Additionally, this selection strategy assumes that there are mislabeled samples locally, and these are often significantly more important than correctly labeled samples [30]. Therefore, by filtering out outlier samples where is significantly higher than most other samples, the above algorithm can effectively measure the true distribution of local categories, selecting samples closer to the global distribution for local model training.  Feature clustering Feature clustering seeks to find a more abstract representation of original features to group similar data distributions together [7]. In the past, most clustering methods in FTL used model- Table 5 FTL frameworks (continued) Reference FTL challenges Architecture StrategyDHFTL MAFTL SSFTL USFTL HOFTL HEFTL HFL VFL CFL DFLSystem Incremental PS CS FCS LCS QS FSH LSH FLSH [53,55,95,97,137], [100,101,147]     MS [96]     MS [54,99]      MS [56]      MS [205]    MS [108,162,206]    MS [94]    MS,MW [98]      MS,MW [111]     MS,MC [42,45,46,59,64,127]    MC [43]     MC [169]    MC [160,168]    MC [170]    MC [145]     MC [159]     MC [107]    MC [171]    MC [207]    MC [109,161]     MC [44,104]    MC,MI [105]    MC,KD [112]     MI [31]     MI [116]     MI [163]      KD [32,119,138140], [33,57,141]    KD [113,115,144,166]     KD [164]     KD [124]      KD [114]       KD [118,142,143]    KD [165]    KD [146]     KD 1 Abbreviation: PS: prior shift; CS: covariate shift; FCS: feature concept shift; LCS: label concept shift; QS: quantity shift; FSH: feature space heterogeneity; LSH: label space heterogeneity; FLSH: feature and label space heterogeneity; HFL: horizontal FL; VFL: vertical federated learning; CFL: centralized FL; DFL: decentralized FL; IA: instance augmentation; IS: instance selection; FA: feature augmentation; FM: feature mapping; FS: feature selection; FC: feature clustering; FAI: feature alignment; CR: consistency regularization; DCR: domain-dependent consistency regularization; PS: parameter sharing; PR: parameter restriction; PD: parameter decoupling; PP: parameter pruning; MW: model weighting; MS: model selection; MC: model clustering; MI: model interpolation; KD: knowledge distillation. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 13 related information for clustering [4246,59,64,127,169,221]. This model-related information includes things like model parameters [46,64,127], gradient information [42,59], training loss [44,169,221], and other external info [43,45]. Except for these, clustering methods based on data-related information also can be applied to mitigate the prior shift issue. For example, Astraea [47] traverses all unassigned participant data distributions by a greedy strategy, looking for a group of participants that can make the overall data distribution of each cluster as close to a uniform distribution as possible.  Feature selection For example, Fed-FiS [175] generates local feature subsets on each participant by estimating the mutual information between features and between features and categories. Then, the server ranks each feature and uses classification tasks to obtain a global subset of features. Similarly, research [37] utilizes a federated feature selection algorithm based on MI in the operation of autonomous vehicles (AV). This algorithm completes global iterative feature selection by locally executing an aggregation function based on Bayes theory, which greatly reduces the computational cost. Moreover, Feature selection is a common idea to extract important features, which can obtain similar performance across different domains, and these important features can serve as a connection for knowledge transfer [7]. In HOFTL, the local dataset of different participants may have similarities in feature space, and high dimensional features can delay the training time, leading to more energy consumption [48]. In this case, removing irrelevant features and selecting useful overlapping features is crucial to address the distribution shift problem in FTL. Current FL researches [37,48,49,175] have proposed a variety of solutions to the above problems, which mainly include three steps: 1. Local filtering: filter the optimal subset of local features of each participant. 2. Global filtering: aggregate local optimal feature subsets to obtain global feature set. 3. Sharing: feed back the global feature set to the participants, allowing participants to focus on the features most relevant to the global representation. For example, FPSO-FS [49] is a federated feature selection algorithm based on particle swarm optimization (PSO), which proposes two global filtering strategies to determine the global optimal feature subset: i acci j; j  1;:::; k X  maxfXij1 k k j1 acci j(Xi;Datj);i  1;:::; kg Xi ith B Datj jth B acci j(Xi Datj) Xi Datj 1. Mean assembly strategy: for the private optimal feature subset of the participant, its average classification accuracy is obtained by the classification accuracy from all participants, i.e., . Then, an optimal subset with the highest average classification accuracy is selected as the overall optimal feature subset, as follows: , where is the private optimal feature subset from the participant, is the sample data held by the participant, and , is the classification accuracy of evaluated by . i acci j; j  1;:::; k X  maxfXijminj1;:::;k(acci j(X;Datj));i  1; :::; kg 2. Maximum and minimum assembly strategy: the first step is to identify the minimum classification accuracy for participant  s optimal feature subset, using the classification accuracy obtained by all participants. Following this, the subset with the highest minimum classification accuracy among all private optimal subsets is selected as the overall optimal fea- ture subset: .  Consistency regularization The FTL atrategies also can be explained from a model perspective. Figure 6 shows the corresponding strategies. Among them, consistency regularization [7] refers to the addition of regularization terms to the objective function of local (or global) model optimization, which aims to improve the model robustness of participants, facilitating the transfer of knowledge from the source model to the target model during the training process. In traditional transfer learning, domain adaptation machine [222,223] and consensus regularization framework [224,225] are widely used for knowledge transfer in multi-source domains [7], which are applicable to FL scenarios with two or more participants. The objective function is represented as: minf T LT;L( f T ) 1ΩD( f T ) 2Ω( f T ); f T where the first term, as a loss function, is used to minimize the classification error of labeled target domain instances, the second term represents different regularizers, and the third term is used to control the complexity of the final decision function . In addition, according to the research [7], domain-dependent consistency regularization can be expressed as: minf T nT;L j1 ( f T (xT;L j ) yT;L j ) 2 2Ω( f T ) 1 kS u1 u nT;U i1 ( f T (xT;U i )  f S u (xT;U i )) 2 ; u uth LT;L u ( f T ) where represents the weighting parameter that is determined by the relevance between the target domain and the source domain. For example, pFedMe [72] utilizes Moreau envelopes for regularizing participants loss functions. This approach effectively separates the optimization of individualized models from the learning process of the overarching global model within a structured bi-level framework tailored for FTL. MOON [128] proposes a contrastive learning-based federated optimization algorithm that uses the distribution difference in intermediate outputs between global and local models. Each participants local optimization goal, beyond the cross-entropy loss term , aims to minimize the distance between the local and global model representations (reducing weight divergence) and maximize the distance between the local model and its previous version (accelerating convergence). 14 Front. Comput. Sci., 2024, 18(6): 186356  Parameter sharing The parameters of a model essentially reflect the knowledge that the model has learned. Therefore, in FL, participants can also transfer knowledge at the parameter level [7] by parameter sharing, which avoids the privacy risks brought by direct transmission of local data [1]. For instance, the source and target models share parameters, and the target models use their local data to fine-tune the final layers of the source model, thereby creating a new model [68,69,154]. Since parameter sharing is a common approach in FL and often forms the basis for other methods, this section will focus on explaining the basic method of locally fine-tuning the global model. Specifically, study [154] finds that fine-tuning global model parameters with local training sets significantly improves prediction accuracy, particularly for local models that differ greatly from global predictions. Unlike FedPer [68], which fine-tunes the global models top parameters using local data, Per-FedAvg [69] averages all participant model parameters and fine-tunes all global model parameters using the MAML meta-learning method. However, the majority of existing FL frameworks based on parameter sharing mainly focus on improving the global models performance on each participant using the participants local data, overlooking the enhancement of a global models generalization performance from the servers perspective. [117] proposes an FL framework based on a fine-tuning and head model aggregation method, called FedFTHA, which includes FedFT and FedHA. From the participants perspective, FedFT focuses on improving the performance of the global model to the participants local dataset by retaining and fine-tuning the local head model. From the servers perspective, FedHA works to reconstruct a global model that exhibits generalized performance, leveraging the participants head model developed during FedFT. This approach enables both participants and the server engaged in FL to mutually benefit and realize a situation where all parties are advantaged.  Parameter restriction The knowledge learned from participants is kept as model parameters and is transferred by the server in CFL. Using the global model directly as the local model usually requires a strong correlation between the global and local data distributions. If there are large differences in data distribution among participants, using the global model directly and optimizing it with local data could lead to a significant decrease in the models generalization ability. To address this, some studies [7176,129,202] in FTL restrict the similarity between the source and target models by parameter restriction [7]. For example, FedProx [202] controls the differences between local and global model parameters by a proximal term, which aims to avoid the global model being significantly skewed by too many local updates and further affecting robust convergence. This approach keeps updates close to the initial model, helping to tackle the problem of prior distribution shift and covariate shift issues. However, this proximal term could not align local and global optimal points, and considering the potential loss of important parameter information when the global model is transferred locally. FedCL [130] introduces elastic weight consolidation (EWC) from continual learning [226]. By using a server-side proxy dataset to estimate the importance of global model weights, local updates can be adjusted to prevent significant changes in vital parameters during local adaptive training: minf T LT;L u ( f T )   i; j Ri jT;L u;i jS;L g;i j 2 ; R  Ri j cu (c cu) c(c  fcug(u  1)k) where represents the importance matrix of the global model, derived using the servers proxy dataset. FedCL prevents divergence between global and local model weights and ensures better generalization and accuracy. Additionally, FedNova [131] addresses distribution inconsistencies between participants by normalizing and scaling local updates, enhancing model convergence. SCAFFOLD [77] focuses on reducing gradient variance, which first introduces a control variable for the direction of the participant model gradient, and then corrects local model updates based on the difference between this control variable and a global variable , alleviating shift issue. Additionally, FedCSA [78] adjusts the weights of classifier parameters based on the distribution of each category on the participant. This adjustment enhances performance when dealing with class imbalance.  Parameter decoupling Research [227] indicates that the classifier of the model may exhibit significant accuracy decreasing when dealing with imbalanced prior probabilities. Therefore, many studies [227230] have suggested that sending partial local models for aggregation by decomposing models of participants into body (extractor) and head (classifier) parameters can improve accuracy in the target domain, which is called parameter decoupling. The body parameters can capture general data information and be maintained locally, enabling each participant to learn data characteristics for specific tasks, while head parameters learn specific features of the target domain for sharing with the server to improve the effectiveness of knowledge transfer. For instance, FedRep [79], FedBABU [80], FedAlt [155], FedPer [68] and SPATL [132] perform global aggregation by sharing a homogeneous feature extractor. LG-FedAvg [63], CHFL [133], and FedClassAvg [81] share a homogeneous classifier. Different from these, Fed-ROD [129] shows great effectiveness by splitting the model head into general and personalized layers. Two predictors are trained using a shared body model to handle the competing objectives of generic and personalized federated learning. One predictor employs empirical risk minimization (ERM) for personalization, while the other uses balanced risk minimization (BRM) for general learning. Moreover, FedU [82] proposes a local sharing protocol based on a Siamese network. By aggregating only the online models from the source Siamese network to update the target model, it effectively enables knowledge transfer between participants. However, the study [80] finds that existing parameter decoupling methods by updating the entire model during the training process, lead to a decrease in personalization Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 15 performance. Therefore, it proposed an FL framework, called FedBABU. It updates only the body part of the model parameters during the federated training process, and the head is fine-tuned for personalization during the evaluation process. To more accurately determine the degree of impact each layer of the model has on the target domain, the study [83] introduces a layered sharpness-aware Minimization (LWSAM) algorithm, which addresses the problem of poor participant performance due to the biased generic information shared by all participants. This method first calculates the distance between the global and local models at each layer, determining how much each layer is affected by the target domain. It accurately divides the model into head and body parts. Then it uses the sharpness-aware minimization (SAM) algorithm as a local optimizer. By adding more disturbances to the model body, the method adjusts the influence of the target distribution on the model from a global perspective.  Parameter pruning Due to varying data distributions among participants, directly applying an aggregated global model to a target domain often does not give optimal results. One popular solution [132] is parameter pruning, which selects a subset of model parameters from the source domain to apply to the target domain. For example, FedMask [85] uses a method called model binary masks to selectively activate certain model parameters for training. This can happen after just one step of communication, without needing to fine-tune the model on local data, which reduces redundancy in communication and computation. In study [132], each participant uses a pre- trained reinforcement learning agent to choose parameters for combining data in a federated manner. They then use a global encoder and a local predictor to transfer knowledge from the combined model to individual models. Another study [86] proposes a federated search method, which uses a lightweight search controller to find an accurate local sub-network for each participant. This method is good at extracting useful information and lowers the energy used for analysis and training. HeteroFL [87] proposes splitting the global model along its width while maintaining the full depth of the participants DNN models, which aims to find more suitable local models. However, this approach could construct very thin and deep subnetworks, leading to a significant loss of basic features. To overcome this issue, the study [134] introduces a federated learning framework, named ScaleFL, which uses early exits to adaptively reduce DNN models width and depth, finding models best suited for training with limited local resources.  Model weighting The knowledge transfer between participants can be accomplished by sharing local model-related information, such as model parameters, which involves aggregating them before local training, called model aggregation. However, different participants may have distinct optimal goals, simply averaging their model information with the same weight could result in the combined results not being the best solution [1]. The global model in the server may also be overly influenced by a single participants model, causing model drift [131,231]. Thus, model weighting is applied to aggregate models according to their contributions. This prevents model performance degradation caused by directly averaging information from different actors into a domain. For example, the study [88] finds local data with higher prediction errors has more contributions to improve the overall model performance, and then introduces an FL framework, called FedCav. FedCav measures the quality of local data using their prediction errors to decide the weights in the model aggregation process. Considering that the server doesnt know the local data distribution, FedFusion [203] uses a global representation of multiple virtual components with different parameters and weights to portray the data distribution of different participants. The server uses a variational autoencoder (VAE) to learn the best parameters and weights of the distribution components based on limited statistical information taken from the original model parameters. Additionally, the study [135] treats the blending of multiple models in FL as a graph-matching task, and then proposes an algorithm, called GAMF. It views channels and weights as nodes and edges of a graph, respectively. Then it uses a new hierarchical algorithm to increase the similarity of weights between channels, and proposes a cycle-consistent multi-graph matching method to merge various local source models in FL, enhancing the global models generalization. Experiments show that GAMF can be used as a plug-in to boost the performance of existing FL systems.  Model selection In reality, participants local data may significantly differ from the optimal global distribution, and each participants data characteristics can not be directly controlled, thus it is important to select participants related to the data or specific target labels for training, called model selection. For example, the study [91] proposes a DFL algorithm based on directed acyclic graphs (DAG). In the DAG, each participant selects the model updates of other participants based on their data similarity, which has been demonstrated effectively in both prior and covariate shift scenarios. Astraea [47], a scheduler-based multi-participant rescheduling framework, re-schedules multiple participants through a scheduler, which follows that the data distribution of multiple participants is most similar to a uniform distribution. Dubhe [148] is an FL algorithm based on repeated model selection, which allows the server to repeatedly select local models to participate in aggregation, and then send the encrypted distribution of the selected participants to the server to check the similarity between the global data distribution after aggregation and the consistency distribution. This process continuously adjusts the aggregation strategy and improves classification accuracy. Another study [60] proposes a Shapley value-based federated averaging algorithm. It calculates the Shapley value of each participant to assess its relevance to the servers learning objective, estimating the participants contribution in the next communication round of FL. This allows the server to select local models with higher contributions for training in each round of aggregation. In addition, some studies [51,92] require each participant to 16 Front. Comput. Sci., 2024, 18(6): 186356 collect models from all other participants and use an additional local validation set to evaluate the similarity between participants. In contrast, the study [93] utilizes mathematical analysis methods instead of using empirical search from validation data sets to characterize the similarity between participants. Apart from data distribution, another study [52] considers differences in local data volumes between participants. Note that uniform sampling could overlook participants with more data, reducing their contributions to the global model training and impacting the models performance. To address this, the researchers propose FedSampling [52], a framework that uses a data uniform sampling strategy. When the participants data distributions are highly imbalanced, participants randomly select others based on the ratio of the servers desired sample volume to the total available participant sample volume, further improving FL model performance.  Model clustering Some studies [44,103] suggest that grouping similar participants for FL can address the model drift issue caused by data distributions heterogeneity among participants, called model clustering. They determine participant similarity based on factors like model parameters [44,64,104,168,170,232], gradients [42,59], training loss [44,169], or other external information [43,45]. For example, FedCluster [42] uses cosine similarity of the model gradient to split participants into multiple clusters, maximizing similarity within clusters while minimizing it between clusters. To further enhance model adaptability in the target domain by utilizing the sub-model clustering method, the study [105] designs a scale-based aggregation strategy, which scales parameters according to the pruning rate of the sub-models and aggregates overlapping parameters. It further introduces a server-assisted model adjustment mechanism to promote beneficial collaboration between device source models and suppress detrimental collaboration. This mechanism dynamically adjusts the sub-model structure of server devices based on a global view of device data distribution similarity. In addition, studies [43,45] use exogenous information like the types of local devices participants use or patient drug features (drugs given within the initial 48 hours of ICU admission, comprising 1399 binary drug features) for clustering. However, these methods tend to overlook the cluster skew issue caused by grouping, leading to the global model overfitting to a specific clusters data distribution. Cluster skew refers not only to an imbalance in the category distribution among groups after clustering but also to an imbalance in the number of participants in each cluster. To address this issue, study [109] suggests a new FL aggregation method with deep reinforcement learning, called FedDRL, which can tap into the self-learning capability of the reinforcement learning agent, rather than setting explicit rules. Specifically, FedDRL utilizes a unique two-stage training process designed to augment the training data and reduce the training time of the deep reinforcement learning model. Moreover, the study [107] proposes a DFL framework based on hierarchical aggregation, named Spread. In this framework, the server acts as the FL coordinator, and edge devices are grouped into different clusters. Selected edge devices, as cluster leaders, responsible for model aggregation tasks. Spread monitors training quality and manages model aggregation congestion by adjusting both intra-cluster and inter-cluster aggregations.  Model interpolation      Different from model weighting methods that combine local models with varying weights, model interpolation aims to blend global and local model parameters proportionally to increase local prediction accuracy [104,110,129]. For example, research [73] prevents the local model and global model from diverging excessively by setting an interpolation coefficient artificially. When is set to 0, the local model only performs local model learning; as increases, the local model gradually becomes similar to the global model, realizing mixed model learning; when is very large, all local models are forced to be similar, maximizing the transfer of knowledge from the global to the local model. Research [112] interpolates a global model trained globally with a local k- nearest neighbors (KNN) model based on the shared representation, which is provided by the global model. The experiments show that it is also suitable for covariate issues except for prior shift issues. However, research [44] demonstrates that these methods, which rely on the separated training of the global model and local model to find the optimal interpolation coefficient , may not always be the best. Therefore, it proposes a combined optimization strategy that improves both local and global models at the same time. Similarly, research [31] proposes a model interpolation method based on elastic aggregation. They measure the sensitivity of each parameter by calculating the change in the overall prediction function output when each parameter changes, which reduces the update magnitude for more sensitive parameters, preventing the global model from excessively interfering with the local data distribution. 3.1.2 Covariate shift  Feature augmentation Some studies [125,149] have proposed solving covariate shift issues from the perspective of feature augmentation. For example, the study [149] proposes an FL paradigm based on a feature representation generator, called FRAug. It optimizes a common feature representation generator to help each participant generate synthetic feature representations locally, which are converted into participant-specific features by a locally optimized RTNet, which aims to make global and local feature distributions as similar as possible, increasing the training space for each participant. In addition, similar to the study [62] in data augmentation, the study [125] further proposes FedFA to augment features from a statistical perspective. The statistics of augmented data should match as closely as possible with the statistics of the original training data. FedFA follows two preconditions: 1. The data distribution of each participant can be characterized as the statistics of the latent feature distribution, i.e., mean and variance [125]. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 17 2. The statistics of latent features can capture basic domain perceptual characteristics [233235]. Therefore, when dealing with data shift in FL, discrepancies in feature statistics across local participants are inconsistent, and they display uncertain changes compared to the actual distributions statistics. Based on this, FedFA leverages a statistical probability augmentation algorithm based on a normal distribution to enhance the local feature statistics for each participant following as: x   xu u u  u; u u u  N(u; 2 u )   N(; 2 u ) xu xu  (xuu) u ( u; u) u u where the mean and variance are the original statistics of the latent features, and , . is normalized by , then expanded using the new statistical values . The variance dictates how much the latent features are augmented. The magnitude of this variance represents how much the latent feature distribution deviates statistically from the target distribution. By adjusting the variance appropriately, the skewness issue can be resolved either at the level of individual participants or across all participants. Moreover, it can be integrated as a plugin into any layer of any network to enhance features or solve any feature statistical bias, such as skewness in test time distribution. FedFA also exhibits excellent accuracy performance when addressing prior shift or quantity shift problems of homogeneous FTL.  Feature clustering PFA [65] first clusters participants with similar data distribution by computing Euclidean Distance of local representations, and then orchestrates an FL procedure on a group basis to effectively achieve adaptation based on their clustering results. Additionally, the study [167] introduces a new FL framework, called FPL, which is based on prototype clustering. FPL uses the mean features of local data as prototypes, and clusters these prototypes at the server using a comparison learning method. This process brings similar prototypes closer and pushes different ones further apart. Moreover, to increase the stability of model training, FPL uses consistency regularization to minimize the distance between the representative prototypes and their unbiased counterparts. Compared to transferring model parameters in the FPL, the size of the prototypes is much smaller than that of the model parameters, which significantly reduces communication costs.  Consistency regularization From the models perspective, directly adding model-level regularizers to the local objective function of the participants or server is a natural idea [7]. In this way, the knowledge maintained in the model(s) of the participants (source model) can be transferred to the model of another participant (target model) during the training process. For fully supervised learning, each participant first uses their local labeled data to obtain the classification loss term. For example, FedBN [151] keeps participant-specific batch normalization layers to normalize local data distribution. Its classification loss term can be written as: LT;L( f T )  k i1 xi j1 ( f T (xi j);yi j) 2 : Except for the classification loss term, a consistency loss term can be introduced based on a cluster-aware mechanism, which uses the differences in both intermediate outputs and predictions between the global and local models to guide local model optimization [153]. It further groups the participants into clusters based on the feature clustering method by harnessing the similarity among lower-level features of each participants model. Each cluster has its own global feature vector and average prediction value. By minimizing the L2 norm between each participant and the global feature and prediction value within its cluster, the method improves the robustness of local models under the covariate shift issue of homogeneous FTL. Additionally, PFL [167] introduces a consistency regularization term based on a global unbiased prototype. It suggests that the cluster prototype averaged by the server, as an unbiased prototype, can provide a relatively fair and stable optimization point. Calculating the square difference loss between the local feature vector and the unbiased prototype can address the issue of unstable prototype convergence. The regularizer in PFL can be expressed as: Lre gularizer  v j1 (xi; j U k j ) 2 ; i j u v U where and index samples in dataset of participant and the dimensions of feature output, respectively. is the number of dimensions. is the unbiased prototype.  Parameter decoupling Parameter decoupling is not only suitable for prior shift or quantity shift problems [87], but also for solving covariate shift problems in homogeneous FTL. For example, the study [156] proposes a more flexible way of decoupling, designing an FL algorithm based on structured pruning, called Hermes. In this method, participants determine the sub-networks to participate in server aggregation through model pruning. To prevent information loss that could result from directly averaging local models, Hermes only averages overlapping sub-network parameters on the server, keeping the parameters of the remaining non-overlapping parts unchanged. The aggregated parts of the sub-networks are then sent back to the local devices for network updates, thereby improving the models performance on local tasks.  Model weighting FedUReID [157] enhances the adaptability of the aggregated global model to each participants local model by applying an exponential moving average (EMA) to update the global model for each participant, where the weight of the EMA represents the similarity between the global and local models. Additionally, FedDG [158] takes advantage of the domain flatness constraint, which serves as a substitute for the complex domain divergence constraint, to approximate the optimal aggregate weights. Moreover, FedDG uses a momentum mechanism to dynamically assign a weight to each isolated domain by tracking the domain generalization gap, 18 Front. Comput. Sci., 2024, 18(6): 186356 improving its generalization capability. Past studies often simplify the blending of source models into a straightforward allocation problem, ignoring complex interactions between channels. Meanwhile, since model weights are shuffled during training, before merging, channels of each layer need to be aligned to maximize similarities in weights between multiple source models. This presents a quadratic assignment property problem, which is NP-hard problem. To tackle this problem, GAMF [135] propose to treat the channels and weights as nodes and edges of a graph to obtain the weights information. These weighting methods typically rely on model parameters or gradient differences to measure each participants contribution to the target prediction. However, the transmission of this information involves potential privacy leakage risks. Thus, the study [204] views the local model of each participant as black-box model, in which all data is stored locally and only the source models input and output interfaces are accessible. Each participants soft outputs are given a weight based on their inter-class variance. These weighted outputs are then used to create target pseudo-labels. Therefore, it proposes a federated adaptive learning framework called Co-MDA, called CO-MDA. CO-MDA changes the label noise learning section into a semi-supervised learning approach and proposes a Co2-Learning strategy. This strategy involves training two networks at the same time that filter each others errors through epoch-level co-teaching [236], and gradually co-guess the pseudo-labels with the outputs of both target networks to further reduce the impact of label noise.  Model clustering FedDL [170], FedAMP [104], and HYPCLUSTER [44] use model-related information (such as model parameters, convolution layer channels, LSTM hidden states, and neurons in fully connected layers) to construct a shared global model based on model clustering method. However, these methods require several communication rounds to separate all inconsistent participants, potentially affecting computational and communication efficiency. Therefore, the study [168] proposes a method FedMA to achieve hierarchical clustering of participants with a single round of communication. This method uses the difference between the initial global model parameters and local model parameters to generate multiple sub-clusters. Then, by calculating the pairwise distances between participants within all sub-clusters, similar sub- clusters are iteratively merged until only a single cluster remains, containing all samples. Furthermore, similar to study [127] in addressing prior shift issue, study [160] treats the multi-center participant clustering issue as an optimization problem, which can be effectively resolved using the expectation-maximization (EM) algorithm. In addition, different from traditional federated clustering methods, which associate each participants data distribution with only one cluster distribution (known as hard clustering), the study [159] introduces a soft-clustering-based FL paradigm, called FedSoft. FedSoft allows each local dataset to follow a mixture of multiple cluster distributions, improving the training of high-quality local and cluster models.  Model selection Model selection, a classic transfer learning method, has seen widespread use in FTL, either on its own or in combination with other methods, and it is equally effective in addressing covariate shift issues of FTL. For example, CMFL [96] compares the local update of each participant with the global update during each iteration of learning by calculating the proportion of parameters in the local update that have opposite signs to those in the global update, which aims to assess the degree of alignment between the two sets of gradients. A higher proportion indicates a greater deviation from the direction of joint convergence, rendering the local update less relevant. CMFL thus selectively excludes such divergent local updates from being uploaded, effectively minimizing communication costs in FL while ensuring convergence can still be significantly achieved. 3.1.3 Feature concept shift  label concept shift To mitigate feature concept shift challenges in FTL, study [169] utilizes an iterative federated hierarchical clustering algorithm, called IFCA. Different from traditional methods, IFCA does not require centralized clustering algorithms. The server only plays a role in average model parameters, which substantially decreases the servers computational load. However, IFCA needs to run a federated stochastic gradient descent (SGD) algorithm in each round until it converges. This process could increase computational and communication efficiency in large-scale FL systems. Regarding the label concept shift issue, current studies address it from the perspectives of feature alignment [150] or model clustering methods [89,207]. Feddg [150] uses the amplitude spectrum in the frequency domain as data distribution information and exchanges it among participants. The goal is that each participant can fully utilize multi-source data distribution information to learn parameters with higher generalization, which proves equally effective under covariate shift. In addition, considering that Bayesian optimization is a powerful surrogate-assisted algorithm for solving label concept shift issues in FL and black-box optimization problems where the local model-related information is not visible to other participants for privacy. Some researchers have turned their attention to federated Bayesian optimization [108,162,206,207]. However, these methods either have all participants work together on the same task, or make only one participant learn from others to tackle a specific task. However, in real life, the tasks of participants are often related to each other. the study [207] introduces an efficient federated multi-task Bayesian optimization framework, called FMTBO, which dynamically aggregates multi-task models based on a dissimilarity matrix derived from predictive rankings. Additionally, FMTBO designs a federated ensemble acquisition function that effectively searches for the best solution by using predictions from both global and local hyperparameters, enhancing the generalization of the global model. Besides, pFEDVEM [89] introduces an FL framework based on Bayesian models and latent variables, and combines the model weighting strategy to mitigate label concept shift issues. In this setup, a hidden shared model identifies common Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 19 patterns among local models, meanwhile, local models adapt to their specific environments using the information from the shared model, which determines the confidence levels of each participant. The confidence levels are then used to set the weights when combining local models. The extensive experiments have demonstrated that pFEDVEM robustly addresses three types of distribution shift issues including prior shift, covariate shift, and label concept shift, and obtains significant accuracy improvement compared to the baselines. 3.1.4 Quantity shift FEDMIX [61] has proven that the instance enhancement method is equally effective for quantity shift problems. Moreover, studies [161,171] view FL as a hedonic game, where each participant produces some cost (error) when joining in the FL process. Theres a Nash equilibrium between minimizing individual errors and overall errors. For example, a school may aim to minimize its local error, while a region or city may aim to minimize the overall error. Study [171] proposes to find a relatively stable participant partition by accurately estimating the expected error of each participant, which may overlook the need to minimize the overall error. Additionally, the numbers of samples for participants in [171] are only assumed to be small or large, Different from it, study [161] not only more focuses on overall social well-being, but also is suitable for any number of participants with any various numbers of samples. 3.2 Heterogeneous federated transfer learning This section will discuss existing works addressing the challenges of heterogeneous FTL, dynamic heterogeneous FTL, and model adaptive FTL from data-based and model- based perspectives as shown in Fig. 6. However, it is worth noting that there is very little research on scenarios with heterogeneous label space and heterogeneous feature and label space, so we will not elaborate on it here. 3.2.1 Feature space heterogeneity A B XB com (A;B) B;A XB com (A;B) XB com (B;A) (A;B) (B;A) XB pr i XB com (A;B) XA pr i XB com (B;A) A B Heterogeneous feature spaces often occur in VFL, thus, we mainly focus on the VFL scenario for feature space heterogeneous FTL. To address this issue, researchers can utilize methods such as feature alignment [172,237,238] or feature concatenation [176] to construct new feature datasets for model training. Among them, feature alignment in VFL can be completed by constructing a novel feature subspace [237], or filling in missing or incomplete features of each participants feature spaces [172]. These approaches enable knowledge transfer under a homogeneous feature space. Specifically, the study [172] assumes an inconsistency in the feature spaces between two participants, the active participant and the passive participant . Both of them map their features using their respective mapping functions and to the same feature space, resulting in new feature representations and . They optimize the mapping functions and by minimizing the similarity between the private features and , as well as and . Finally, participants and , through secure bilateral computation, obtain the complete XA b XB a features and respectively. Based on this, the federated aggregation can be implemented under the aligned feature space. However, these methods rely on the existing feature space of participants, ignoring the relationships among these features. By combining feature clustering methods, the active participant can create new, more valuable feature space for knowledge transfer. For example, study [173] proposes a VFL paradigm based on feature space decomposition clustering, called PrADA. The specific steps include: C p C q h h  p q 1. Feature grouping: participant applies domain expertise to divide raw features into groups, each containing tightly related features. Moreover, participant constructs interactive feature among pairs of feature groups, resulting in a total of feature groups (where ). B C fE  ffE;ig i  1 h 2. Pretraining stage: this stage involves collaborative efforts between source participant and participant , to train a set of feature extractors ( for to ) that are capable of learning features which are both invariant across domains and discriminative for labels. A C A C fE  ffE;ig i  1 h 3. Fine-tuning stage: this process is executed in collaboration between active participant and participant , with the goal of training participant s target label predictor by utilizing the pre-trained set of feature extractors ( , where to ). In addition, PrADA enhances privacy and security using a secure protocol based on partial homomorphic encryption. However, not all features of the participants are relevant to the task. Therefore, the novel feature space generated by aggregating the features of all parties needs to filter features that are not relevant to the task through feature selection. However, current feature selection techniques [232,239] in distribution learning, often need numerous training iterations, particularly when dealing with high-dimensional data. Directly applying them in FTL to solve feature space heterogeneous issues produces significant computational and communication overhead, as each training round involves multiple encryptions, decryption operations, and intermediate parameter transfers. For example, study [179] suggests using an embedded method to combine autoencoders with L2 constraints on feature weights for feature selection, and sets a threshold for post-training to determine the selected features for mitigating the problem of model parameter shrinkage [240]. Different from previous VFL research scenarios where there were mostly two participants and binary classification tasks, study [178] proposes an FL feature selection scheme suitable for multi-participant multi-classification. In addition, previous studies that mainly focus on the relationship between features and labels [180], ignoring the relationship between features, to solve this problem, similar to research [37,175] using MI theory into federated feature selection in HFL, study [174] proposes a feature selection VFL framework based on conditional mutual information, called FEAST. FEAST integrates feature information into a single statistical variable for FL transmission, which not only accomplishes key feature selection and further reduces communication costs while 20 Front. Comput. Sci., 2024, 18(6): 186356 ensuring privacy and security. In addition, study [181] first proposes a theoretically verifiable feature selection method, formalizing the feature selection problem in the VFL environment, and providing a theoretical framework to prove that unimportant features have been removed. 3.3 Dynamic heterogeneous FTL 3.3.1 System heterogeneity System heterogeneity among participants could lead to the emergence of stragglers in each iteration. To address this problem, instance selection can be leveraged by researchers to mitigate the computational burden of participants when they have heterogeneous local computational resources. However, it could lead to decreased model performance due to the reduced statistical utility of the training dataset. Study [198] obtains the optimal data selection scheme through an optimization function that includes lower and upper limits of resources, as well as arbitrary, non-decreasing cost functions per resource, meanwhile, it treats this problem as a scheduling problem of tasks assignment to resources, seeking to maximize the number of participants in each round of FL updates. In addition, FedBalancer [197] chooses samples for training by measuring their statistical utility, derived from the sample loss list based on the latest model. However, it is inefficient to wait for every participant to finish local training before proceeding with aggregation due to system heterogeneity. Thus, under a constant FL round deadline setting, instance selection could not immediately enhance the time-to-accuracy ratio. Furthermore, model-based strategies, such as consistency regularization [202], model selection [1], model clustering [55,97,98,145], parameter decoupling [106], parameter pruning [85] can also be applied as effective ways to address the straggler issue in FL. For example, Fedavg [1] directly drops models of these stragglers which can not accomplish local training in time when the other participants have completed the same amount of training. Based on this, FedProx [202] allows varying local training epochs across participants, tailored to each devices system capabilities. Subsequently, it aggregates the non-convergent updates submitted by stragglers, rather than dropping these less responsive participants from this iteration. However, these frameworks may come at the cost of sacrificing accuracy due to the omission of partial information and waiting for all participants to complete a uniform number of training epochs tends to extend the convergence time of FL. FedAT [106] blends synchronous and asynchronous updates by decoupling the model parameters at the layer level, which stratifies local models based on the time each participant needs to complete a round of training. During each training round, FedAT randomly selects some local models in each layer to calculate the loss gradient of local data, completing the synchronous update of models in that specific layer. Each layer, acting as a new training entity, then asynchronously updates the global model. The faster layers have shorter round-response delays, speeding up the convergence of the global model. The slower layers contribute to global training by asynchronously sending model updates to the server, which further improves the predictive performance of the model. Besides, [85,86,134] selectively use local models for transfer knowledge by parameter pruning methods under the limited computational resources. Study [145] introduces a FL framework, called FedHiSyn, which uses a resource-based hierarchical clustering approach. This framework first categorizes all available devices according to their computing capabilities. Given that a ring topology is more suitable for models with uniform resources, after local training, the models are sent to the server. Then, within their respective categories, they exchange local model weight updates based on the ring topology structure to mitigate the lag effect caused by system heterogeneity. Considering that the main challenge of dynamic heterogeneous FTL is the appropriate scheduling of participants, essentially a local model selection issue at each iteration. Some researchers [53,9496,136,137] assume the central party has 1-lookahead in source model selection strategies, which means that the dynamic input data beforehand is known. However, this can not be applied when dealing with unpredictable time series inputs. Thus, reinforcement learning has been increasingly used to design source model selection strategies [54,94,97,111,132]. Studies [97,98] propose FL paradigms based on the multi-armed bandit (MAB). In situations where the available computing resources of participants are unknown, these approaches calculate the difference between the data distribution of multiple combined participants and a class-balanced data distribution, and then pick local models for aggregation and assign weights based on these differences. In another study, Study [55] proposes a participant scheduling strategy by age of update (AoU) measurement. This strategy considers the age of the received parameters and the current channel quality at the same time, which improves efficiency and allows effective aggregation in federated joint learning. Except for the above-mentioned reinforcement learning methods in dynamic heterogeneous FTL, TiFL [99], a hierarchical federated learning framework, addresses system heterogeneity issues by grouping participants with similar training performance. Meanwhile, in each round of updates, TiFL adaptively selects local models for training within each group by simultaneously optimizing accuracy and training time. To accelerate model convergence under system heterogeneity, FedSAE [100] suggests choosing participants with larger local training losses to take part in aggregation during each training round. FedSAE also designs a mechanism to predict each participants maximum tolerable workload, aiming to dynamically adjust their local training rounds. Research [205] found that differences in bit-width among participant devices can impact the performance of the global model. Low-bit-width models are more compatible with hardware but could limit the models generalization, leading to poor performance of high-bit-width models. To tackle this, the study [205] introduces ProWD, a FL framework that considers bit-width heterogeneity. This framework selects sparse sub- weights compatible with full-precision model weights from the low-bit-width models received by the server. These selected sub-weights then participate in central aggregation along with the full-precision model weights. Another study Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 21 [101] proposes a FL framework, called Aergia, which freezes the most computation-heavy parts of the model and trains the unfrozen parts. Moreover, the server chooses a more reasonable offloading solution based on the training speed reported by each participant and the similarity between their datasets. In this way, the training of the frozen parts can be offloaded to participants with ample resources or faster training speeds. PyramidFL [147] is a fine-grained participant selection, which considers not only the distribution and system heterogeneity between the selected and non-selected participants but also within the selected participants themselves. Specifically, the server uses feedback from past training rounds to rank participants based on their importance, participants then use their rank to determine the number of iterations for data efficiency and the parameters to drop for system efficiency. Furthermore, the utility of each participant isnt static but varies across training rounds. If a participant is selected, its data utility will then decrease since these data have been seen by the model. Thus, reducing the likelihood of selection in subsequent training rounds allows participants who were not selected previously to have a higher probability of being chosen, which further improves the model performance under the dynamic heterogeneity and further enhances the fairness of participant selection. 3.3.2 Incremental heterogeneity T(t)i ui t T(t 1)j uj t 1 uj ui Existing FTL strategies mainly focus on model-based techniques, for example, GLFC [102] addresses the continuous emergence of new classes in federated online learning by consistency regularization method. It introduces a class-aware gradient compensation loss to ensure the consistency of the learning pace for new classes with the forgetting pace of old classes. It separately normalizes the gradients for new and old classes and reweights them for the local optimization goal, where the relationships between classes are obtained based on the best old classification model from the previous tasks. As the local data or tasks increase, where the tasks may be a new batch of data, the task learned by participant in round could be similar or related to the task learned by participant in round . In this situation, the transmission of aggregated global information among participants can facilitate knowledge transfer across participants. Nonetheless, when a new joined task in participant is irrelevant to the tasks of participant , it could affect the optimization direction of the local model, leading to a decrease in accuracy. To mitigate this issue, each participant selectively utilizes only the knowledge of the relevant tasks that have been trained on other participants during each iteration, while ignoring as much as possible the knowledge of irrelevant tasks that may interfere with local learning. Thus, parameter decomposition and model weighting methods have attracted the attention of researchers [110]. For example, FedWeIT [110] solves this problem by decomposing parameters into three different types for training: global parameters that capture the global and generic knowledge across all participants, local base parameters that capture generic knowledge for each participant, and task-adaptive parameters for each specific task per participant. Meanwhile, FedWeIT applies sparse masks to select parameters relevant to a given task, minimizing interference from irrelevant tasks of other participants and allocating attention to the servers aggregated parameters to selectively filter parameter information. However, all these methods lack a theoretical basis for ensuring convergence. FedL [56] uses dynamic adaptation to measure the extent to which online decision constraints are breached and calculates a maximum limit for this measure, which guarantees that the expected contribution of the chosen source model to the FL models performance matches its actual contribution. 3.4 Model adaptive FTL Q R R Q X R Model adaptive FTL is often caused by model heterogeneity, i.e., the local models of different participants may be inconsistent in architecture, which could cause incompatibility in the feature dimension and representational capacity among participants [115]. It means that the average aggregation approach based on consistent features can not be used directly in FL. To mitigate this issue, data-based strategies are proposed in FTL. For example, the study [200] proposes to apply a feature mapping method to obtain consistent representation space and complete FL. Considering that even with different model structures, they possess some common knowledge for the same input, i.e., the feature extraction layers generate similar feature maps, research [200] uses model drafts to align local data distributions of participants. These outputs from specific layers or models are interpreted as blurred images of data and defined as model drafts. By minimizing the similarity difference between the local and global drafts, the data distribution difference between participants can be reduced. Except for feature mapping [7,126] using explicit features, some implicit features can be utilized to align the source and target domains, facilitating knowledge transfer within this aligned space [7], called feature alignment. Implicit features include subspace attributes [201], spectral characteristics [150], prototype graphs [66]. For example, the study [66] uses prototypes to effectively transfer information under the model adaptive FTL by minimizing local and global prototype graphs within the same feature space, thereby capturing the semantic information of class structures. It avoids the possibility of data from different classes (across various participants) merging into a single class, or data from the same class being spread across multiple classes. Similarly, FedHeNN [103] is a FL framework based on instance-level representation. Each participant randomly selects part of local data and obtains instance-level representation to guide local training. By introducing a distance function based on centralized kernel alignment as a proximal term of the local loss function, it aligns local and global model representations, enabling federated learning across heterogeneous models. FedFoA [201] adds a linear calibration layer at the end of each local model to first calibrate the different feature dimensions among participants to the same dimension space. Participants use decomposition to obtain feature subspace and feature correlation matrix . The central server minimizes the reconstruction of local features and the product of global 22 Front. Comput. Sci., 2024, 18(6): 186356 Q Q and initial vector to get the optimal , guiding the update of the local sub-feature space. Inconsistent model architectures also make the simple average aggregation of model parameters ineffective. From the model-based perspective, studies [87,156] implement FedAvg on top of local sub-networks by parameter decoupling. [87] assumes that the model architectures of participants can dynamically change with each iteration, and further proposes to leverage parameter decoupling to obtain at least one fixed sub-network for each type of heterogeneous situation and aggregate them into a single global model. Thus, smaller local models can gain more from global aggregation by performing less global aggregation on a subset of the parameters from larger local models. Similarly, study [84] combines parameter decoupling with model clustering method to group local models based on the similarity of their personalized sub-networks, maximizing the level of knowledge sharing between participants. Besides, research [144] designed a Mapper at the local level to convert feature representations from different semantic spaces to the same feature space, and accomplish the knowledge transfer based on knowledge distillation (KD). They deploy a global generator at the server to extract global data distribution information and distill it into the local model of each participant. Then, local models are viewed as discriminators to reduce the difference between global and local data distributions in heterogeneous feature spaces. Since the feature representations synthesized by the global generator are usually more faithful and homogeneous to the global data distribution, they can achieve faster and better convergence. Additionally, local generators also can be used to enhance hard-to-judge sample data, improving model performance [144]. In real-world scenarios, cross-institutional FL is often more content with the VFL scenario. However, traditional VFL can only benefit from samples shared among multiple parties, which severely limits its application. Therefore, research [115] proposes a VFL framework based on representation distillation, called VFedTrans. This framework collaboratively models common features among multiple parties and extracts federated representations of shared samples, aiming to maximize data utility as much as possible through KD. Knowledge distillation is also often used for dealing with model adaptive FTL induced by model heterogeneity as shown in Table 5. FedMD [163] uses KD for federated learning in situations where different models are used. Instead of just combining model parameters, FedMD calculates class scores for each participant using a shared dataset. These scores are then sent to a server to calculate an average, which guides the training of local models. This method allows for knowledge sharing while keeping private data and model structures secure, and it works even when different local models are used. Contrary to the assumption that participants local models are entirely different, research [119] assumes that local models of participants are not fully heterogeneous, and there are cases where some models share the same structure. Therefore, they propose an FL framework based on ensemble distillation, called FedDF. It creates several prototype models, which represent participants with identical model structures. In each round, FedAvg is performed among participants with the same prototype model to initialize a global model (student model), followed by cross-architecture learning through knowledge distillation. In this process, the parameters of the local model (teacher model) are tested on an unlabeled public dataset to generate predictions for training each student model on the server. Research [113] adopts a federated communication strategy, denoted as FSFL, which is similar to FedMD, innovatively adding a latent embedding adaptive module to alleviate the impact of domain discrepancies between public and private datasets. However, these studies [32,33,57,114,119,139141,163,164,241] strongly rely on the construction of public datasets, which undoubtedly compromises data privacy and is operationally challenging in practice. The impact of the quality of these prerequisites on the performance of federated learning is also unknown [124]. Therefore, research [124] proposes an FL framework based on zero-shot knowledge distillation, called FedZKT. FedZKT requires no prerequisites for local data, and its distillation tasks are assigned to the server to reduce the local workload. In addition, some studies [29,115,142144,165] introduce generators to avoid the need for public datasets, enabling the aggregation of local information in a data-free manner. For example, FedFTG [143] uses the log-odds of each local model as a teacher to train a global generator and fine-tunes the global model using pseudo-data generated by the global generator. Due to the additional computational and communication costs imposed by the introduction of a generator, ScaleFL [134] proposes a self-distillation method based on exit predictions. This method treats self-distillation as an integral part of the local training process, requiring no extra overhead. ScaleFL enhances the knowledge flow among local sub-networks by minimizing the KullbackLeibler divergence (KL divergence) between early exits (students) and final predictions (teachers). Furthermore, the study [166] introduces a FL framework based on a data-free semantic collaborative distillation, called MCKD. This framework transfers soft predictions from local models to a server to learn representations that dont change across different domains. MCKD also introduces a knowledge filter to mitigate the potential amplification of irrelevant or malicious participants influences on the target domain by traditional averaging aggregation. This knowledge filter generates consensus knowledge for unlabeled data and sets a threshold to drop models where local model predicted classes are inconsistent with consensus classes, further adapting the central model to target data. However, most of these methods construct ensemble knowledge by merely averaging the soft predictions of multiple local models, overlooking that local models have a differential understanding of distillation samples. Research [138] suggests that a model is more likely to make the correct predictions when the samples are included in the domain used for the models training. Based on this, the study [138] proposes treating each participants local data as a specific domain and designs a domain-aware federated distillation method named DaFKD. DaFKD can recognize the importance of each model to the distillation samples. For a given distillation sample, if the local model has a significant Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 23 relevance factor with it, DaFKD assigns a higher weight to this local model. 3.5 Semi-supervised and unsupervised FTL The labeled data scarcity is a common scenario in both HFL and VFL scenarios. Data-based methods receive some attention in reality for SSFTL issues, some studies [120,121] utilize these methods to obtain valuable augmented data by strategically optimizing predictions or similarity calculations on these data. For example, SemiFed [120] uses both past local and global models to predict labels for local unlabeled data. When predictions have high confidence, the pseudo- labeled data is used to train as available data. Orchestra [121] uses a feature clustering method to obtain more available data for training. It refines local data clusters through interactions with the server, which helps find effective samples in unlabeled data, further increases sample size, and reduces distribution heterogeneity when labeled data is scarce. Different from them without considering the high-class imbalance of unlabeled data, based on the instance selection method, CBAFed [123] uses the empirical distribution of all training data from the last round of global communication to design category-balanced adaptive thresholds. That is, if the model uses more data for training in one class, the threshold for labeling unlabeled data in this class will increase, and vice versa. It aims to shrink the gap between local and global distributions by selecting a local training set, and further influence category distribution, preventing a decline in global model performance due to prior probability bias. Model-based strategies similarly show the effectiveness in addressing the SSFTL issue, study [50] introduces a framework based on the consistency regularization method, called FedMatch. This framework splits local model parameters into two parts, one for updating with labeled data, and another for unlabeled data. For the parameters associated with labeled data, the loss function solely includes cross- entropy loss, and the loss function related to unlabeled data follows as: minf T;U LT;U ( f T;U ) L2  L U 2 2 L1 U 1; L1 L2 U U  L where and regularization on aims to make sparse, while not drifting far from the current optimal parameter trained with labeled data. The first term differs from the studies [120,152], which not only uses the prediction results of unlabeled data and its augmented data to obtain cross-entropy loss but also considers pseudo-labels as real labels to obtain new category loss, making the model perform the same on original data and slightly perturbed data. Contrary to previous studies that assume the presence of both labeled and unlabeled data locally, research [50] describes a federated semi-supervised learning scenario where some participants have fully labeled data while others have only unlabeled data. To guide unlabeled participants learning by building the interaction between the learning at labeled and unlabeled participants [120,152], FedIRM [41] applies the intermediate output of the local model, which is trained by the labeled participants, to construct a category relationship matrix. By transmitting the relationship matrix of each labeled participant, it guides the unlabeled participants to learn their local relationship matrix. The formula is as follows: minf T;U (w)(LT;U ( f T;U ) LIRM); LIRM  1 z z j1 (LKL(RL j jjRU j ) LKL(RU j jjRL j )); Rj j where denotes the relation vector of class . Furthermore, researchers [67,128] also apply domain-dependent consistency regularization to solve this issue. For example, [67] uses the KL divergence between local and global model predictions to control the update of the local objective function. Additionally, other model-based strategies, such as parameter decoupling [50], model weighting [90,122,152], also attract some researchers attention. FedMatch [50] accomplishes the FTL under semi-supervised learning scenarios by training labeled and unlabeled data separately through model parameter decomposition. FedConsist [152] and RscFed [122] improve the performance of models in semi-supervised FTL by changing the weights of participants with labeled and unlabeled data. Since semi-supervised learning often deals with data that only has positive and unlabeled (PU) samples, one participants negative class could be made up of multiple positive classes from other participants. However, traditional PU learning mostly focuses on binary problems with only one kind of negative sample. Thus, study [90] assumes that the available data has multiple types of positive and negative classes (MPMM-PU), and further proposes to redefine the expected risk of MPMM-PU, which aims to decide each participants weight and examines the limits of the models generalization. In response to unsupervised FTL, a straightforward approach is to combine self-supervised methods with FL, such as [195,242]. However, this challenge is often accompanied by homogeneous or heterogeneous FTL issues. The feature- based strategies have obtained some attention to solve this problem, such as FedCA [199] based on feature selection, FSHFL [48] based on feature mapping, and FedFoA [201] based on feature augmentation. Moreover, studies also propose to alleviate the data drift issues between participants by model-based strategies, such as FedX [146] based on KD, and FedEMA [116] based on model interpolation. FedCA [199] shares features of local data and employs an auxiliary pubic dataset to minimize disparities in the representation space across participants. However, it ignores the inconsistency between the feature representation of local unlabeled data and global feature representation. To solve this problem, FSHFL [48], an FL framework based on an unsupervised federated feature selection approach, proposes the feature cleaning locally and global feature selection. The local feature cleaning utilizes an enhanced version of the one- class support vector machine (OCSVM) algorithm, called FAR-OCSVM, to identify features that lack sufficiently representative global features. The identification relies on local feature clustering, features within each cluster exhibit strong interrelationships, thus the clusters with more features contribute more significantly to the FTL process. Meanwhile, 24 Front. Comput. Sci., 2024, 18(6): 186356 the server selects global features from the collected local feature sets and then returns these global features to participants, directing them to select local features that are closest to the global representation. Considering that the public dataset has potential privacy leakage risk [199], FedX [146] incorporates local knowledge distillation and global knowledge distillation into the FedAvg [1] without any public data. Local knowledge distillation trains the network using the feature representations of local unlabeled datasets, and global knowledge distillation aims to mitigate data shift, which only relies on global model sharing. Besides, FedEMA [116] utilizes self-supervised learning methods with predictors, including MoCo and BYOL, to update local encoders through the EMA of the global encoder. 4 Application In this section, we explore and outline the prevalent applications where FTL makes a significant impact. 4.1 Federated cross-domain recommendation Cross-domain recommendation (CDR) aims to reduce data sparsity by transferring knowledge from a data-rich source domain to a target domain. However, this process presents significant challenges related to data privacy and knowledge transferability [243,244]. Therefore, federated learning is introduced to CDR to improve the performance of the target domain model while providing privacy protection. Currently, classic recommendation algorithms have been widely applied to federated learning, such as federated collaborative filtering [245,246], federated matrix factorization [247249], and federated graph neural networks [250]. However, these methods neglect the heterogeneity among them including data, resource, or model heterogeneity. Researchers have combined transfer learning techniques to accomplish tasks related to federated cross-domain recommendation. They utilize parameter sharing [251], parameter decoupling [252,253], and model clustering [254] to facilitate the process. 4.2 Federated medical image classification Medical data involving patient information is sensitive and its use is strictly regulated, limiting the application of current artificial intelligence technologies in the medical field. Federated learning, which trains models on local devices without sharing raw data, could protect patients data privacy security. However, data provided by different medical institutions, acting as source domains, often have heterogeneity in format, features, or distribution [255]. By integrating FL with transfer learning, we can leverage data from different healthcare institutions for model training, enhancing the models performance while preserving data privacy. This approach presents a promising direction for the application of artificial intelligence in the medical field. Common methods include federated knowledge distillation [139,256], federated weighting aggregation [204,257260], federated consistency regularization [261,262], federated model selection [263], federated model interpolation [264], federated model decoupling methods [265], federated model clustering [266], and feature clustering methods [267]. 4.3 Federated financial service Federated financial services include credit risk control [268272], stock prediction [273275], financial fraud transaction detection [276,277], etc. Among them, credit risk control is a standard procedure for financial institutions, which estimates whether an individual or entity is able to make future required payments [269]. Stock prediction allows economists, governors, and investors to model the market, manage the resources and enhance stock profits [273]. Fraudulent transaction detection is another difficult problem for individual banks to curb company financial losses and maintain positive customer relationships [276]. However, their predictive models all rely on large amounts of data to achieve training. Since the customers data or fraudulent transaction data can not be shared among different institutions, the prediction model of each financial institution often suffers from the limited sample issue, which significantly hinders the performance improvement of the model. The emergence of federated learning not only breaks this data silos problem but also can combine instance augmentation [271,275], instance selection [270], feature selection [268,269,277], feature alignment [237,238], parameter sharing [272], model weighting [273], model selection [274,276], knowledge distillation [272], and other technical approaches to achieve information exchange between institutions. 4.4 Federated traffic flow prediction Given that precise and up-to-date traffic flow data is of immense significance for traffic management, predicting traffic flow has emerged as a crucial element of intelligent transportation systems. However, current traffic flow prediction techniques, which rely on centralized machine learning, require the collection of raw data from mobile phones and cameras for model training, causing potential privacy issues. Researchers found that this issue also can be addressed using FTL, such as federated clustering aggregation [278283], federated weighting aggregation [284286], federated parameter sharing [285], and federated parameter control methods [283]. 5 Conclusion and future work This survey provides a systematic summary of federated transfer learning, which emerges from the combination of transfer learning with federated learning, offering the corresponding definitions, challenges, and strategies. For convenience for researchers, we compile settings for the most common data heterogeneity scenarios, including both homogeneous and heterogeneous federated transfers, and summarize some significant FTL studies for various challenges. In the future, the utility performance of FL models in more complex scenarios deserves further exploration, including label concept shift, feature concept shift, label space heterogeneity, and feature  label space heterogeneity. Due to differences or mutations in unknown or hidden relationships between input and output variables between participants, the concept shift becomes an important but less studied direction in FL. Meanwhile, label space heterogeneity and feature  Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 25 label space heterogeneity also deserve more in-depth study. Furthermore, although federated transfer learning uses model selection, weight aggregation and other transfer learning methods to solve the problems of data heterogeneity, dynamic heterogeneity and labeled data scarcity in FL, new privacy concerns may have also emerged about these strategy preferences. After participants crack the weighting strategy or selection mechanism used by the server to aggregate information from all parties, they may use unfair means to make the training of the global model develop in a direction more beneficial to themselves. Thus, possible leakage of strategy preferences should be properly considered when designing FTL strategies in the future. Finally, the communication costs, communication efficiency, and computational costs caused by these FTL strategies could be paid more attention by FL researchers. Compared with traditional FL methods, FTL strategies, such as instance augmentation, instance selection, feature selection, model selection, and model clustering, etc., increase additional computational costs and introduce more shared information among participants. Based on some traditional methods [287290] for enhancing computational or communication efficiency, FTL researchers can further optimize federated algorithms. In summary, FTL still has great potential research value in utility, privacy, and communication issues. Acknowledgements The research work was supported by the National Key RD Program of China (No. 2021ZD0113602), the National Natural Science Foundation of China (Grant Nos. 62176014 and 62202273), and the Fundamental Research Funds for the Central Universities. Competing interests The authors declare that they have no competing interests or financial conflicts to disclose. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the articles Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the articles Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:creativecommons.orglicenses by4.0. References McMahan B, Moore E, Ramage D, Hampson S, Arcas B A Y. Communication-efficient learning of deep networks from decentralized data. In: Proceedings of the 20th International Conference on Artificial Intelligence and Statistics. 2017, 12731282 1. Feng S, Li B, Yu H, Liu Y, Yang Q. Semi-supervised federated heterogeneous transfer learning. Knowledge-Based Systems, 2022, 252: 109384 2. Zhang C, Xie Y, Bai H, Yu B, Li W, Gao Y. A survey on federated learning. Knowledge-Based Systems, 2021, 216: 106775 3. Gao L, Fu H, Li L, Chen Y, Xu M, Xu C Z. FedDC: Federated learning with non-IID data via local drift decoupling and correction. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 1010210111 4. Shi Y, Zhang Y, Xiao Y, Niu L. Optimization strategies for client drift in federated learning: a review. Procedia Computer Science, 2022, 214: 11681173 5. Liu Y, Kang Y, Zou T, Pu Y, He Y, Ye X, Ouyang Y, Zhang Y Q, Yang Q. Vertical federated learning: concepts, advances and challenges. 2022, arXiv preprint arXiv: 2211.12814 6. Zhuang F, Qi Z, Duan K, Xi D, Zhu Y, Zhu H, Xiong H, He Q. A comprehensive survey on transfer learning. Proceedings of the IEEE, 2021, 109(1): 4376 7. Yang Q, Liu Y, Chen T, Tong Y. Federated machine learning: concept and applications. ACM Transactions on Intelligent Systems and Technology, 2019, 10(2): 12 8. Rahman K M J, Ahmed F, Akhter N, Hasan M, Amin R, Aziz K E, Islam A K M M, Mukta M S H, Islam A K M N. Challenges, applications and design aspects of federated learning: a survey. IEEE Access, 2021, 9: 124682124700 9. Liu J, Huang J, Zhou Y, Li X, Ji S, Xiong H, Dou D. From distributed machine learning to federated learning: a survey. Knowledge and Information Systems, 2022, 64(4): 885917 10. Li L, Fan Y, Lin K Y. A survey on federated learning. In: Proceedings of the16th IEEE International Conference on Control  Automation (ICCA). 2020, 791796 11. Zhan Y, Zhang J, Hong Z, Wu L, Li P, Guo S. A survey of incentive mechanism design for federated learning. IEEE Transactions on Emerging Topics in Computing, 2022, 10(2): 10351044 12. Yin X, Zhu Y, Hu J. A comprehensive survey of privacy-preserving federated learning: A taxonomy, review, and future directions. ACM Computing Surveys, 2022, 54(6): 131 13. Lyu L, Yu H, Yang Q. Threats to federated learning: a survey. 2020, arXiv preprint arXiv: 2003.02133 14. Pfitzner B, Steckhan N, Arnrich B. Federated learning in a medical context: a systematic literature review. ACM Transactions on Internet Technology, 2021, 21(2): 50 15. Nguyen D C, Pham Q V, Pathirana P N, Ding M, Seneviratne A, Lin Z, Dobre O, Hwang W J. Federated learning for smart healthcare: a survey. ACM Computing Surveys, 2023, 55(3): 60 16. Lim W Y B, Luong N C, Hoang D T, Jiao Y, Liang Y C, Yang Q, Niyato D, Miao C. Federated learning in mobile edge networks: a comprehensive survey. IEEE Communications Surveys  Tutorials, 2020, 22(3): 20312063 17. Nguyen D C, Ding M, Pathirana P N, Seneviratne A, Li J, Poor H V. . Federated learning for internet of things: a comprehensive survey. . IEEE Communications Surveys  Tutorials, 2021, 23(3): 16221658 18. Zhu H, Xu J, Liu S, Jin Y. Federated learning on non-IID data: a survey. Neurocomputing, 2021, 465: 371390 19. Tan A Z, Yu H, Cui L, Yang Q. Towards personalized federated learning. IEEE Transactions on Neural Networks and Learning Systems, 2023, 34(12): 95879603 20. Pan S J, Yang Q. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 2010, 22(10): 13451359 21. Konečný J, McMahan H B, Ramage D, Richtárik P. Federated optimization: distributed machine learning for on-device intelligence. 2016, arXiv preprint arXiv: 1610.02527 22. Long M, Wang J, Sun J, Yu P S. Domain invariant transfer kernel learning. IEEE Transactions on Knowledge and Data Engineering, 2015, 27(6): 15191532 23. Long M, Cao Y, Wang J, Jordan M I. Learning transferable features with deep adaptation networks. In: Proceedings of the 32nd International Conference on Machine Learning. 2015, 97105 24. Bengio Y. Deep learning of representations for unsupervised and transfer learning. In: Proceedings of 2011 International Conference on Unsupervised and Transfer Learning Workshop. 2011, 1737 25. Raina R, Battle A, Lee H, Packer B, Ng A Y. Self-taught learning: transfer learning from unlabeled data. In: Proceedings of the 24th 26. 26 Front. Comput. Sci., 2024, 18(6): 186356 International Conference on Machine Learning. 2007, 759766 Younis R, Fisichella M. FLY-SMOTE: re-balancing the non-IID IoTedge devices data in federated learning system. IEEE Access, 2022,10: 6509265102 27. Wu Q, Chen X, Zhou Z, Zhang J. FedHome: cloud-edge basedpersonalized federated learning for in-home health monitoring. IEEETransactions on Mobile Computing, 2022, 21(8): 28182832 28. Jeong E, Oh S, Kim H, Park J, Bennis M, Kim S L. Communication-efficient on-device machine learning: Federated distillation andaugmentation under non-IID private data. 2018, arXiv preprint arXiv: 1811.11479 29. Li A, Zhang L, Tan J, Qin Y, Wang J, Li X Y. Sample-level data selection for federated learning. In: Proceedings of the IEEE Conference on Computer Communications. 2021, 110 30. Chen D, Hu J, Tan V J, Wei X, Wu E. Elastic aggregation for federated optimization. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 1218712197 31. Chen H Y, Chao W L. Fedbe: Making Bayesian model ensemble applicable to federated learning. In: Proceedings of the 9th International Conference on Learning Representations. 2021 32. Seo H, Park J, Oh S, Bennis M, Kim S L. Federated knowledge distillation. 2020, arXiv preprint arXiv: 2011.02367 33. Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of 2012 IEEE Conference on Computer Vision and Pattern Recognition. 2012, 20662073 34. Peng X, Bai Q, Xia X, Huang Z, Saenko K, Wang B. Moment matching for multi-source domain adaptation. In: Proceedings of the IEEECVF International Conference on Computer Vision. 2019, 14061415 35. Kuznetsova A, Rom H, Alldrin N, Uijlings J, Krasin I, Pont-Tuset J, Kamali S, Popov S, Malloci M, Kolesnikov A, Duerig T, Ferrari V. The open images dataset V4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision, 2020, 128(7): 19561981 36. Cassara P, Gotta A, Valerio L. Federated feature selection for cyber- physical systems of systems. IEEE Transactions on Vehicular Technology, 2022, 71(9): 99379950 37. Darlow L N, Crowley E J, Antoniou A, Storkey A J. CINIC-10 is not ImageNet or CIFAR-10. 2018, arXiv preprint arXiv: 1810.03505 38. Liu Z, Luo P, Wang X, Tang X. Deep learning face attributes in the wild. In: Proceedings of the IEEE International Conference on Computer Vision. 2015, 37303738 39. Reddi S J, Charles Z, Zaheer M, Garrett Z, Rush K, Konečný J, Kumar S, McMahan H B. Adaptive federated optimization. In: Proceedings of the 9th International Conference on Learning Representations. 2021 40. Liu Q, Yang H, Dou Q, Heng P A. Federated semi-supervised medical image classification via inter-client relation matching. In: Proceedings of the 24th International Conference. 2021, 325335 41. Sattler F, Müller K R, Samek W. Clustered federated learning: model- agnostic distributed multitask optimization under privacy constraints. IEEE Transactions on Neural Networks and Learning Systems, 2021, 32(8): 37103722 42. Qayyum A, Ahmad K, Ahsan M A, Al-Fuqaha A, Qadir J. Collaborative federated learning for healthcare: multi-modal covid-19 diagnosis at the edge. IEEE Open Journal of the Computer Society, 2022, 3: 172184 43. Mansour Y, Mohri M, Ro J, Suresh A T. Three approaches for personalization with applications to federated learning. 2020, arXiv preprint arXiv: 2002.10619 44. Huang L, Shea A L, Qian H, Masurkar A, Deng H, Liu D. Patient clustering improves efficiency of federated machine learning to predict mortality and hospital stay time using distributed electronic medical records. Journal of Biomedical Informatics, 2019, 99: 103291 45. Ouyang X, Xie Z, Zhou J, Xing G, Huang J. ClusterFL: a clustering- based federated learning system for human activity recognition. ACM Transactions on Sensor Networks, 2023, 19(1): 17 46. Duan M, Liu D, Chen X, Liu R, Tan Y, Liang L. Self-balancing federated learning with global imbalanced data in mobile systems. IEEE Transactions on Parallel and Distributed Systems, 2021, 32(1): 5971 47. Zhang X, Mavromatics A, Vafeas A, Nejabati R, Simeonidou D. Federated feature selection for horizontal federated learning in IoT networks. IEEE Internet of Things Journal, 2023, 10(11): 1009510112 48. Hu Y, Zhang Y, Gao X, Gong D, Song X, Guo Y, Wang J. A federated feature selection algorithm based on particle swarm optimization under privacy protection. Knowledge-Based Systems, 2023, 260: 110122 49. Jeong W, Yoon J, Yang E, Hwang S J. Federated semi-supervised learning with inter-client consistency  disjoint learning. In: Proceedings of the 9th International Conference on Learning Representations. 2021 50. Li S, Zhou T, Tian X, Tao D. Learning to collaborate in decentralized learning of personalized models. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 97569765 51. Qi T, Wu F, Lyu L, Huang Y, Xie X. FedSampling: a better sampling strategy for federated learning. In: Proceedings of the 32nd International Joint Conference on Artificial Intelligence. 2023, 41544162 52. Chen M, Yang Z, Saad W, Yin C, Poor H V, Cui S. A joint learning and communications framework for federated learning over wireless networks. IEEE Transactions on Wireless Communications, 2021, 20(1): 269283 53. Deng Y, Lyu F, Ren J, Wu H, Zhou Y, Zhang Y, Shen X. AUCTION: automated and quality-aware client selection framework for efficient federated learning. IEEE Transactions on Parallel and Distributed Systems, 2022, 33(8): 19962009 54. Yang H H, Arafa A, Quek T Q S, Poor H V. Age-based scheduling policy for federated learning in mobile edge networks. In: Proceedings of 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2020, 87438747 55. Su L, Zhou R, Wang N, Fang G, Li Z. An online learning approach for client selection in federated edge learning under budget constraint. In: Proceedings of the 51st International Conference on Parallel Processing. 2022, 72 56. Wu C, Wu F, Lyu L, Huang Y, Xie X. Communication-efficient federated learning via knowledge distillation. Nature Communications, 2022, 13(1): 2032 57. Tuor T, Wang S, Ko B J, Liu C, Leung K K. Data selection for federated learning with relevant and irrelevant data at clients. 2020, arXiv preprint arXiv: 2001.08300 58. Duan M, Liu D, Ji X, Liu R, Liang L, Chen X, Tan Y. FedGroup: Efficient clustered federated learning via decomposed data-driven measure. 2020, arXiv preprint arXiv: 2010.06870 59. Nagalapatti L, Narayanam R. Game of gradients: mitigating irrelevant clients in federated learning. In: Proceedings of the 35th AAAI Conference on Artificial Intelligence. 2021, 90469054 60. Yoon T, Shin S, Hwang S J, Yang E. FedMix: approximation of Mixup under mean augmented federated learning. In: Proceedings of the 9th International Conference on Learning Representations. 2021 61. Hao W, El-Khamy M, Lee J, Zhang J, Liang K J, Chen C, Carin L. Towards fair federated learning with zero-shot data augmentation. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2021, 33053314 62. Liang P P, Liu T, Liu Z, Allen N B, Auerbach R P, Brent D,63. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 27 Salakhutdinov R, Morency L P. Think locally, act globally: federated learning with local and global representations. 2020, arXiv preprint arXiv: 2001.01523 Briggs C, Fan Z, Andras P. Federated learning with hierarchical clustering of local updates to improve training on non-IID data. In: Proceedings of 2020 International Joint Conference on Neural Networks (IJCNN). 2020, 19 64. Liu B, Guo Y, Chen X. PFA: privacy-preserving federated adaptation for effective model personalization. In: Proceedings of the Web Conference 2021. 2021, 923934 65. Tan Y, Long G, Liu L, Zhou T, Lu Q, Jiang J, Zhang C. FedProto: federated prototype learning across heterogeneous clients. In: Proceedings of the 36th AAAI Conference on Artificial Intelligence. 2022, 84328440 66. Shen T, Zhang J, Jia X, Zhang F, Huang G, Zhou P, Kuang K, Wu F, Wu C. Federated mutual learning. 2020, arXiv preprint arXiv: 2006.16765 67. Arivazhagan M G, Aggarwal V, Singh A K, Choudhary S. Federated learning with personalization layers. 2019, arXiv preprint arXiv: 1912.00818 68. Fallah A, Mokhtari A, Ozdaglar A. Personalized federated learning with theoretical guarantees: a model-agnostic meta-learning approach. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 300 69. Li T, Sahu A K, Talwalkar A, Smith V. Federated learning: challenges, methods, and future directions. IEEE Signal Processing Magazine, 2020, 37(3): 5060 70. Deng Y, Kamani M M, Mahdavi M. Adaptive personalized federated learning. 2020, arXiv preprint arXiv: 2003.13461 71. Dinh C T, Tran N H, Nguyen T D. Personalized federated learning with Moreau envelopes. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 1796 72. Hanzely F, Richtárik P. Federated learning of a mixture of global and local models. 2020, arXiv preprint arXiv: 2002.05516 73. Dinh C T, Tran N H, Nguyen T D, Bao W, Zomaya A Y, Zhou B B. Federated learning with proximal stochastic variance reduced gradient algorithms. In: Proceedings of the 49th International Conference on Parallel Processing. 2020, 48 74. Hanzely F, Hanzely S, Horváth S, Richtárik P. Lower bounds and optimal algorithms for personalized federated learning. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 194 75. Li T, Hu S, Beirami A, Smith V. Ditto: fair and robust federated learning through personalization. In: Proceedings of the 38th International Conference on Machine Learning. 2021, 63576368 76. Karimireddy S P, Kale S, Mohri M, Reddi S J, Stich S U, Suresh A T. SCAFFOLD: stochastic controlled averaging for federated learning. In: Proceedings of the 37th International Conference on Machine Learning. 2020, 476 77. Ma Z, Zhao M, Cai X, Jia Z. Fast-convergent federated learning with class-weighted aggregation. Journal of Systems Architecture, 2021, 117: 102125 78. Collins L, Hassani H, Mokhtari A, Shakkottai S. Exploiting shared representations for personalized federated learning. In: Proceedings of the 38th International Conference on Machine Learning. 2021, 20892099 79. Oh J, Kim S, Yun S Y. FedBABU: towards enhanced representation for federated image classification. 2021, arXiv preprint arXiv: 2106.06042 80. Jang J, Ha H, Jung D, Yoon S. FedClassAvg: local representation learning for personalized federated learning on heterogeneous neural networks. In: Proceedings of the 51st International Conference on 81. Parallel Processing. 2022, 76 Zhuang W, Gan X, Wen Y, Zhang S, Yi S. Collaborative unsupervised visual representation learning from decentralized data. In: Proceedings of the IEEECVF International Conference on Computer Vision. 2021, 48924901 82. Qu Z, Li X, Han X, Duan R, Shen C, Chen L. How to prevent the poor performance clients for personalized federated learning? In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 1216712176 83. Wang K, He Q, Chen F, Chen C, Huang F, Jin H, Yang Y. FlexiFed: personalized federated learning for edge clients with heterogeneous model architectures. In: Proceedings of the ACM Web Conference 2023. 2023, 29792990 84. Li A, Sun J, Zeng X, Zhang M, Li H, Chen Y. FedMask: joint computation and communication-efficient personalized federated learning via heterogeneous masking. In: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems. 2021, 4255 85. Yang Z, Sun Q. Personalized heterogeneity-aware federated search towards better accuracy and energy efficiency. In: Proceedings of the 41st IEEEACM International Conference on Computer Aided Design. 2022, 19 86. Diao E, Ding J, Tarokh V. HeteroFL: computation and communication efficient federated learning for heterogeneous clients. In: Proceedings of the 9th International Conference on Learning Representations. 2020 87. Zeng H, Zhou T, Guo Y, Cai Z, Liu F. FedCav: contribution-aware model aggregation on distributed heterogeneous data in federated learning. In: Proceedings of the 50th International Conference on Parallel Processing. 2021, 75 88. Zhu J, Ma X, Blaschko M B. Confidence-aware personalized federated learning via variational expectation maximization. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 2454224551 89. Lin X, Chen H, Xu Y, Xu C, Gui X, Deng Y, Wang Y. Federated learning with positive and unlabeled data. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1334413355 90. Beilharz J, Pfftzner B, Schmid R, Geppert P, Arnrich B, Polze A. Implicit model specialization through dag-based decentralized federated learning. In: Proceedings of the 22nd International Middleware Conference. 2021, 310322 91. Zhang M, Sapra K, Fidler S, Yeung S, Álvarez J M. Personalized federated learning with first order model optimization. In: Proceedings of the 9th International Conference on Learning Representations. 2021 92. Liu J, Wu J, Chen J, Hu M, Zhou Y, Wu D. FedDWA: personalized federated learning with dynamic weight adjustment. In: Proceedings of the 32nd International Joint Conference on Artificial Intelligence. 2023, 39934001 93. Wang H, Kaplan Z, Niu D, Li B. Optimizing federated learning on Non-IID data with reinforcement learning. In: Proceedings of the IEEE Conference on Computer Communications. 2020, 16981707 94. Nishio T, Yonetani R. Client selection for federated learning with heterogeneous resources in mobile edge. In: Proceedings of 2019 IEEE International Conference on Communications (ICC). 2019, 17 95. Wang L, Wang W, Li B. CMFL: mitigating communication overhead for federated learning. In: Proceedings of the 39th International Conference on Distributed Computing Systems (ICDCS). 2019, 954964 96. Xia W, Quek T Q S, Guo K, Wen W, Yang H H, Zhu H. Multi-armed bandit-based client scheduling for federated learning. IEEE Transactions on Wireless Communications, 2020, 19(11): 71087123 97. Yang M, Wang X, Zhu H, Wang H, Qian H. Federated learning with class imbalance reduction. In: Proceedings of the 29th European Signal Processing Conference (EUSIPCO). 2021, 21742178 98. 28 Front. Comput. Sci., 2024, 18(6): 186356 Chai Z, Ali A, Zawad S, Truex S, Anwar A, Baracaldo N, Zhou Y, Ludwig H, Yan F, Cheng Y. TiFL: a tier-based federated learning system. In: Proceedings of the 29th International Symposium on High- Performance Parallel and Distributed Computing. 2020, 125136 99. Li L, Duan M, Liu D, Zhang Y, Ren A, Chen X, Tan Y, Wang C. FedSAE: a novel self-adaptive federated learning framework in heterogeneous systems. In: Proceedings of the International Joint Conference on Neural Networks. 2021, 110 100. Cox B, Chen L Y, Decouchant J. Aergia: leveraging heterogeneity in federated learning systems. In: Proceedings of the 23rd ACMIFIP International Middleware Conference. 2022, 107120 101. Dong J, Wang L, Fang Z, Sun G, Xu S, Wang X, Zhu Q. Federated class-incremental learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 1015410163 102. Makhija D, Han X, Ho N, Ghosh J. Architecture agnostic federated learning for neural networks. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1486014870 103. Huang Y, Chu L, Zhou Z, Wang L, Liu J, Pei J, Zhang Y. Personalized cross-silo federated learning on non-IID data. In: Proceedings of the 35th AAAI Conference on Artificial Intelligence. 2021, 78657873 104. Zhu S, Qi Q, Zhuang Z, Wang J, Sun H, Liao J. FedNKD: a dependable federated learning using fine-tuned random noise and knowledge distillation. In: Proceedings of 2022 International Conference on Multimedia Retrieval. 2022, 185193 105. Chai Z, Chen Y, Anwar A, Zhao L, Cheng Y, Rangwala H. FedAT: a high-performance and communication-efficient federated learning system with asynchronous tiers. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021, 117 106. Hu C, Liang H H, Han X M, Liu B A, Cheng D Z, Wang D. Spread: decentralized model aggregation for scalable federated learning. In: Proceedings of the 51st International Conference on Parallel Processing. 2022, 75 107. Zhang X, Li Y, Li W, Guo K, Shao Y. Personalized federated learning via variational Bayesian inference. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 2629326310 108. Nguyen N H, Le Nguyen P, Nguyen T D, Nguyen T T, Nguyen D L, Nguyen T H, Pham H H, Truong T N. FedDRL: deep reinforcement learning-based adaptive aggregation for non-IID data in federated learning. In: Proceedings of the 51st International Conference on Parallel Processing. 2022, 73 109. Yoon J, Jeong W, Lee G, Yang E, Hwang S J. Federated continual learning with weighted inter-client transfer. In: Proceedings of the 38th International Conference on Machine Learning. 2021, 1207312086 110. Qu Z, Duan R, Chen L, Xu J, Lu Z, Liu Y. Context-Aware online client selection for hierarchical federated learning. IEEE Transactions on Parallel and Distributed Systems, 2022, 33(12): 43534367 111. Marfoq O, Neglia G, Vidal R, Kameni L. Personalized federated learning through local memorization. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1507015092 112. Huang W, Ye M, Du B, Gao X. Few-shot model agnostic federated learning. In: Proceedings of the 30th ACM International Conference on Multimedia. 2022, 73097316 113. Itahara S, Nishio T, Koda Y, Morikura M, Yamamoto K. Distillation- based semi-supervised federated learning for communication-efficient collaborative training with non-IID private data. IEEE Transactions on Mobile Computing, 2023, 22(1): 191205 114. Zhang J, Guo S, Guo J, Zeng D, Zhou J, Zomaya A Y. Towards data- independent knowledge transfer in model-heterogeneous federated learning. IEEE Transactions on Computers, 2023, 72(10): 28882901 115. Zhuang W, Wen Y, Zhang S. Divergence-aware federated self-116. supervised learning. In: Proceedings of the 10th ACM International Conference on Multimedia. 2022 Wang Y, Xu H, Ali W, Li M, Zhou X, Shao J. FedFTHA: a fine-tuning and head aggregation method in federated learning. IEEE Internet of Things Journal, 2023, 10(14): 1274912762 117. Gong X, Sharma A, Karanam S, Wu Z, Chen T, Doermann D, Innanje A. Preserving privacy in federated learning with ensemble cross- domain knowledge distillation. In: Proceedings of the 36th AAAI Conference on Artificial Intelligence. 2022, 1189111899 118. Lin T, Kong L, Stich S U, Jaggi M. Ensemble distillation for robust model fusion in federated learning. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 198 119. Lin H, Lou J, Xiong L, Shahabi C. SemiFed: semi-supervised federated learning with consistency and pseudo-labeling. 2021, arXiv preprint arXiv: 2108.09412 120. Lubana E S, Tang C I, Kawsar F, Dick R P, Mathur A. Orchestra: unsupervised federated learning via globally consistent clustering. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1446114484 121. Liang X, Lin Y, Fu H, Zhu L, Li X. RSCfed: random sampling consensus federated semi-supervised learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 1014410153 122. Li M, Li Q, Wang Y. Class balanced adaptive pseudo labeling for federated semi-supervised learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 1629216301 123. Zhang L, Wu D, Yuan X. FedZKT: zero-shot knowledge transfer towards resource-constrained federated learning with heterogeneous on-device models. In: Proceedings of the 42nd International Conference on Distributed Computing Systems (ICDCS). 2022, 928938 124. Zhou T, Konukoglu E. FedFA: federated feature augmentation. In: Proceedings of the 11th International Conference on Learning Representations. 2023 125. Yue K, Jin R, Pilgrim R, Wong C W, Baron D, Dai H. Neural tangent kernel empowered federated learning. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 2578325803 126. Long G, Xie M, Shen T, Zhou T, Wang X, Jiang J. Multi-center federated learning: clients clustering for better personalization. World Wide Web, 2023, 26(1): 481500 127. Li Q, He B, Song D. Model-contrastive federated learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2021, 1070810717 128. Chen H Y, Chao W L. On bridging generic and personalized federated learning for image classification. In: Proceedings of the 10th International Conference on Learning Representations. 2022 129. Yao X, Sun L. Continual local training for better initialization of federated models. In: Proceedings of 2020 IEEE International Conference on Image Processing (ICIP). 2020, 17361740 130. Wang J, Liu Q, Liang H, Joshi G, Poor H V. Tackling the objective inconsistency problem in heterogeneous federated optimization. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 638 131. Yu S, Nguyen P, Abebe W, Qian W, Anwar A, Jannesari A. SPATL: salient parameter aggregation and transfer learning for heterogeneous federated learning. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2022, 114 132. Liu C, Yang Y, Cai X, Ding Y, Lu H. Completely heterogeneous federated learning. 2022, arXiv preprint arXiv: 2210.15865 133. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 29 Ilhan F, Su G, Liu L. ScaleFL: Resource-adaptive federated learning with heterogeneous clients. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 2453224541 134. Liu C, Lou C, Wang R, Xi A Y, Shen L, Yan J. Deep neural network fusion via graph matching with applications to model ensemble and federated learning. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1385713869 135. Cho Y J, Wang J, Joshi G. Client selection in federated learning: convergence analysis and power-of-choice selection strategies. 2020, arXiv preprint arXiv: 2010.01243 136. Huang T, Lin W, Wu W, He L, Li K, Zomaya A Y. An efficiency- boosting client selection scheme for federated learning with fairness guarantee. IEEE Transactions on Parallel and Distributed Systems, 2021, 32(7): 15521564 137. Wang H, Li Y, Xu W, Li R, Zhan Y, Zeng Z. DaFKD: domain-aware federated knowledge distillation. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 2041220421 138. Gong X, Song L, Vedula R, Sharma A, Zheng M, Planche B, Innanje A, Chen T, Yuan J, Doermann D, Wu Z Y. Federated learning with privacy-preserving ensemble attention distillation. IEEE Transactions on Medical Imaging, 2023, 42(7): 20572067 139. Li Q, He B, Song D. Practical one-shot federated learning for cross- silo setting. In: Proceedings of the 30th International Joint Conference on Artificial Intelligence. 2021, 14841490 140. Sattler F, Marban A, Rischke R, Samek W. Communication-efficient federated distillation. 2020, arXiv preprint arXiv: 2012.00632 141. Zhu Z, Hong J, Zhou J. Data-free knowledge distillation for heterogeneous federated learning. In: Proceedings of the 38th International Conference on Machine Learning. 2021, 1287812889 142. Zhang L, Shen L, Ding L, Tao D, Duan L Y. Fine-tuning global model via data-free knowledge distillation for non-IID federated learning. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 1016410173 143. Yang Y, Yang R, Peng H, Li Y, Li T, Liao Y, Zhou P. FedACK: federated adversarial contrastive knowledge distillation for cross- lingual and cross-model social bot detection. In: Proceedings of the ACM Web Conference 2023. 2023, 13141323 144. Li G, Hu Y, Zhang M, Liu J, Yin Q, Peng Y, Dou D. FedHiSyn: a hierarchical synchronous federated learning framework for resource and data heterogeneity. In: Proceedings of the 51st International Conference on Parallel Processing. 2022, 8 145. Han S, Park S, Wu F, Kim S, Wu C, Xie X, Cha M. FedX: unsupervised federated learning with cross knowledge distillation. In: Proceedings of the 17th European Conference on Computer Vision. 2022, 691707 146. Li C, Zeng X, Zhang M, Cao Z. PyramidFL: a fine-grained client selection framework for efficient federated learning. In: Proceedings of the 28th Annual International Conference on Mobile Computing and Networking. 2022, 158171 147. Zhang S, Li Z, Chen Q, Zheng W, Leng J, Guo M. Dubhe: towards data unbiasedness with homomorphic encryption in federated learning client selection. In: Proceedings of the 50th International Conference on Parallel Processing. 2021, 83 148. Chen H, Frikha A, Krompass D, Gu J, Tresp V. FRAug: tackling federated learning with non-IID features via representation augmentation. In: Proceedings of the IEEECVF International Conference on Computer Vision. 2023, 48264836 149. Liu Q, Chen C, Qin J, Dou Q, Heng P A. FedDG: federated domain generalization on medical image segmentation via episodic learning in continuous frequency space. In: Proceedings of the IEEECVF 150. Conference on Computer Vision and Pattern Recognition. 2021, 10131023 Li X, Jiang M, Zhang X, Kamp M, Dou Q. FedBN: federated learning on non-IID features via local batch normalization. In: Proceedings of the 9th International Conference on Learning Representations. 2021 151. Yang D, Xu Z, Li W, Myronenko A, Roth H R, Harmon S, Xu S, Turkbey B, Turkbey E, Wang X, Zhu W, Carrafiello G, Patella F, Cariati M, Obinata H, Mori H, Tamura K, An P, Wood B J, Xu D. Federated semi-supervised learning for COVID region segmentation in chest CT using multi-national data from China, Italy, Japan. Medical Image Analysis, 2021, 70: 101992 152. Wang H, Zhao H, Wang Y, Yu T, Gu J, Gao J. FedKC: federated knowledge composition for multilingual natural language understanding. In: Proceedings of the ACM Web Conference 2022. 2022, 18391850 153. Wang K, Mathews R, Kiddon C, Eichner H, Beaufays F, Ramage D. Federated evaluation of on-device personalization. 2019, arXiv preprint arXiv: 1910.10252 154. Pillutla K, Malik K, Mohamed A, Rabbat M G, Sanjabi M, Xiao L. Federated learning with partial model personalization. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 1771617758 155. Li A, Sun J, Li P, Pu Y, Li H, Chen Y. Hermes: an efficient federated learning framework for heterogeneous mobile clients. In: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking. 2021, 420437 156. Zhuang W, Wen Y, Zhang S. Joint optimization in edge-cloud continuum for federated unsupervised person re-identification. In: Proceedings of the 29th ACM International Conference on Multimedia. 2021, 433441 157. Zhang R, Xu Q, Yao J, Zhang Y, Tian Q, Wang Y. Federated domain generalization with generalization adjustment. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 39543963 158. Ruan Y, Joe-Wong C. FedSoft: soft clustered federated learning with proximal local updating. In: Proceedings of the 36th AAAI Conference on Artificial Intelligence. 2022, 81248131 159. Xie H, Xiong L, Yang C. Federated node classification over graphs with latent link-type heterogeneity. In: Proceedings of the ACM Web Conference 2023. 2023, 556566 160. Donahue K, Kleinberg J M. Optimality and stability in federated learning: a game-theoretic approach. In: Proceedings of the International Conference on Neural Information Processing Systems. 2021, 12871298 161. Dai Z, Low B K H, Jaillet P. Federated Bayesian optimization via Thompson sampling. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 812 162. Li D, Wang J. FedMD: Heterogenous federated learning via model distillation. 2019, arXiv preprint arXiv: 1910.03581 163. Li Y, Zhou W, Wang H, Mi H, Hospedales T M. FedH2L: federated learning with model and statistical heterogeneity. 2021, arXiv preprint arXiv: 2101.11296 164. Wu Y, Kang Y, Luo J, He Y, Fan L, Pan R, Yang Q. FedCG: leverage conditional GAN for protecting privacy and maintaining competitive performance in federated learning. In: Proceedings of the 31st International Joint Conference on Artificial Intelligence. 2022, 23342340 165. Niu Z, Wang H, Sun H, Ouyang S, Chen Y W, Lin L. MCKD: Mutually collaborative knowledge distillation for federated domain adaptation and generalization. In: Proceedings of 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2023, 15 166. 30 Front. Comput. Sci., 2024, 18(6): 186356 Huang W, Ye M, Shi Z, Li H, Du B. Rethinking federated learning with domain shift: a prototype view. In: Proceedings of 2023 IEEECVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023, 1631216322 167. Wang H, Yurochkin M, Sun Y, Papailiopoulos D S, Khazaeni Y. Federated learning with matched averaging. In: Proceedings of the 8th International Conference on Learning Representations. 2020 168. Ghosh A, Chung J, Yin D, Ramchandran K. An efficient framework for clustered federated learning. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020, 1643 169. Tu L, Ouyang X, Zhou J, He Y, Xing G. FedDL: federated learning via dynamic layer sharing for human activity recognition. In: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems. 2021, 1528 170. Donahue K, Kleinberg J. Model-sharing games: Analyzing federated learning under voluntary participation. In: Proceedings of the 35th AAAI Conference on Artificial Intelligence. 2021, 53035311 171. Gao D, Liu Y, Huang A, Ju C, Yu H, Yang Q. Privacy-preserving heterogeneous federated transfer learning. In: Proceedings of 2019 IEEE International Conference on Big Data (Big Data). 2019, 25522559 172. Kang Y, He Y, Luo J, Fan T, Liu Y, Yang Q. Privacy-preserving federated adversarial domain adaptation over feature groups for interpretability. IEEE Transactions on Big Data, 2022, doi: 10.1109TBDATA.2022.3188292 173. Fu R, Wu Y, Xu Q, Zhang M. FEAST: a communication-efficient federated feature selection framework for relational data. Proceedings of the ACM on Management of Data, 2023, 1(1): 107 174. Banerjee S, Elmroth E, Bhuyan M. Fed-FiS: a novel information- theoretic federated feature selection for learning stability. In: Proceedings of the 28th International Conference on Neural Information Processing. 2021, 480487 175. Wu Z, Li Q, He B. Practical vertical federated learning with unsupervised representation learning. IEEE Transactions on Big Data, 2022 176. He Y, Kang Y, Zhao X, Luo J, Fan L, Han Y, Yang Q. A hybrid self- supervised learning framework for vertical federated learning. 2022, arXiv preprint arXiv: 2208.08934 177. Feng S, Yu H. Multi-participant multi-class vertical federated learning. 2020, arXiv preprint arXiv: 2001.11154 178. Feng S. Vertical federated learning-based feature selection with non- overlapping sample utilization. Expert Systems with Applications, 2022, 208: 118097 179. Jiang J, Burkhalter L, Fu F, Ding B, Du B, Hithnawi A, Li B, Zhang C. VF-PS: how to select important participants in vertical federated learning, efficiently and securely? In: Proceedings of the 36th International Conference on Neural Information Processing Systems. 2022, 152 180. Castiglia T, Zhou Y, Wang S, Kadhe S, Baracaldo N, Patterson S. LESS-VFL: Communication-efficient feature selection for vertical federated learning. In: Proceedings of the 40th International Conference on Machine Learning. 2023, 37573781 181. Kairouz P, McMahan H B, Avent B, Bellet A, Bennis M, et al. Advances and open problems in federated learning. Foundations and Trends in Machine Learning, 2021, 14(12): 1210 182. Chai Z, Fayyaz H, Fayyaz Z, Anwar A, Zhou Y, Baracaldo N, Ludwig H, Cheng Y. Towards taming the resource and data heterogeneity in federated learning. In: Proceedings of 2019 USENIX Conference on Operational Machine Learning. 2019, 1921 183. Ye M, Fang X, Du B, Yuen P C, Tao D. Heterogeneous federated learning: state-of-the-art and research challenges. ACM Computing 184. Surveys, 2024, 56(3): 79 Schlegel R, Kumar S, Rosnes E, Amat A G I. CodedPaddedFL and CodedSecAgg: Straggler mitigation and secure aggregation in federated learning. IEEE Transactions on Communications, 2023, 71(4): 20132027 185. You C, Xiang J, Su K, Zhang X, Dong S, Onofrey J, Staib L, Duncan J S. Incremental learning meets transfer learning: application to multi- site prostate MRI segmentation. In: Proceedings of the 3rd MICCAI Workshop on Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health. 2022, 316 186. Chen Y, Qin X, Wang J, Yu C, Gao W. FedHealth: a federated transfer learning framework for wearable healthcare. IEEE Intelligent Systems, 2020, 35(4): 8393 187. Ditzler G, Polikar R. Incremental learning of concept drift from streaming imbalanced data. IEEE Transactions on Knowledge and Data Engineering, 2013, 25(10): 22832301 188. Elwell R, Polikar R. Incremental learning of concept drift in nonstationary environments. IEEE Transactions on Neural Networks, 2011, 22(10): 15171531 189. Tan D S, Lin Y X, Hua K L. Incremental learning of multi-domain image-to-image translations. IEEE Transactions on Circuits and Systems for Video Technology, 2021, 31(4): 15261539 190. Huang Y, Bert C, Gomaa A, Fietkau R, Maier A, Putz F. A survey of incremental transfer learning: combining peer-to-peer federated learning and domain incremental learning for multicenter collaboration. 2023, arXiv preprint arXiv: 2309.17192 191. Tang J, Lin K Y, Li L. Using domain adaptation for incremental SVM classification of drift data. Mathematics, 2022, 10(19): 3579 192. Alam S, Liu L, Yan M, Zhang M. FedRolex: model-heterogeneous federated learning with rolling sub-model extraction. In: Proceedings of the 36th International Conference on Neural Information Processing Systems. 2022, 2152 193. Yu F, Zhang W, Qin Z, Xu Z, Wang D, Liu C, Tian Z, Chen X. Heterogeneous federated learning. 2020, arXiv preprint arXiv: 2008.06767 194. Jin Y, Wei X, Liu Y, Yang Q. Towards utilizing unlabeled data in federated learning: a survey and prospective. 2020, arXiv preprint arXiv: 2002.11545 195. Diao E, Ding J, Tarokh V. SemiFL: semi-supervised federated learning for unlabeled clients with alternate training. In: Proceedings of the 36th International Conference on Neural Information Processing Systems. 2022, 1299 196. Shin J, Li Y, Liu Y, Lee S J. FedBalancer: data and pace control for efficient federated learning on heterogeneous clients. In: Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services. 2022, 436449 197. Pilla L L. Optimal task assignment for heterogeneous federated learning devices. In: Proceedings of 2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS). 2021, 661670 198. Zhang F, Kuang K, You Z, Shen T, Xiao J, Zhang Y, Wu C, Zhuang Y, Li X. Federated unsupervised representation learning. 2020, arXiv preprint arXiv: 2010.08982 199. Liao Y, Ma L, Zhou B, Zhao X, Xie F. DraftFed: a draft-based personalized federated learning approach for heterogeneous convolutional neural networks. IEEE Transactions on Mobile Computing, 2024, 23(5): 39383949 200. Liu Y, Guo S, Zhang J, Zhou Q, Wang Y, Zhao X. Feature correlation- guided knowledge transfer for federated self-supervised learning. 2022, arXiv preprint arXiv: 2211.07364 201. Li T, Sahu A K, Zaheer M, Sanjabi M, Talwalkar A, Smith V. Federated optimization in heterogeneous networks. In: Proceedings of 202. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 31 the Machine Learning and Systems. 2020, 429450 Duan J H, Li W, Zou D, Li R, Lu S. Federated learning with data- agnostic distribution fusion. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 80748083 203. Liu X, Xi W, Li W, Xu D, Bai G, Zhao J. Co-MDA: federated multisource domain adaptation on black-box models. IEEE Transactions on Circuits and Systems for Video Technology, 2023, 33(12): 76587670 204. Yoon J, Park G, Jeong W, Hwang S J. Bitwidth heterogeneous federated learning with progressive weight dequantization. In: Proceedings of the 39th International Conference on Machine Learning. 2022, 2555225565 205. Dai Z, Low B K H, Jaillet P. Differentially private federated Bayesian optimization with distributed exploration. In: Proceedings of the International Conference on Neural Information Processing Systems. 2021, 91259139 206. Zhu H, Wang X, Jin Y. Federated many-task Bayesian optimization. IEEE Transactions on Evolutionary Computation, 2023 207. Chawla N V, Bowyer K W, Hall L O, Kegelmeyer W P. SMOTE: synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 2002, 16: 321357 208. He H, Bai Y, Garcia E A, Li S. ADASYN: Adaptive synthetic sampling approach for imbalanced learning. In: Proceedings of 2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence). 2008, 13221328 209. Han H, Wang W Y, Mao B H. Borderline-smote: a new over-sampling method in imbalanced data sets learning. In: Proceedings of the International Conference on Intelligent Computing. 2005, 878887 210. Kubat M, Matwin S. Addressing the curse of imbalanced training sets: one-sided selection. In: Proceedings of the 14th International Conference on Machine Learning. 1997, 179186 211. Yen S J, Lee Y S. Cluster-based under-sampling approaches for imbalanced data distributions. Expert Systems with Applications, 2009, 36(3): 57185727 212. Tsai C F, Lin W C, Hu Y H, Yao G T. Under-sampling class imbalanced datasets by combining clustering analysis and instance selection. Information Sciences, 2019, 477: 4754 213. Zhang L, Li Y, Xiao X, Li X Y, Wang J, Zhou A, Li Q. CrowdBuy: privacy-friendly image dataset purchasing via crowdsourcing. In: Proceedings of the IEEE Conference on Computer Communications. 2018, 27352743 214. Li A, Zhang L, Qian J, Xiao X, Li X Y, Xie Y. TODQA: efficient task- oriented data quality assessment. In: Proceedings of the 15th International Conference on Mobile Ad-Hoc and Sensor Networks (MSN). 2019, 8188 215. Katharopoulos A, Fleuret F. Not all samples are created equal: deep learning with importance sampling. In: Proceedings of the 35th International Conference on Machine Learning. 2018, 25302539 216. Alain G, Lamb A, Sankar C, Courville A, Bengio Y. Variance reduction in SGD by distributed importance sampling. 2015, arXiv preprint arXiv: 1511.06481 217. Loshchilov I, Hutter F. Online batch selection for faster training of neural networks. 2015, arXiv preprint arXiv: 1511.06343 218. Schaul T, Quan J, Antonoglou I, Silver D. Prioritized experience replay. In: Proceedings of the 4th International Conference on Learning Representations. 2016 219. Wu C Y, Manmatha R, Smola A J, Krahenbuhl P. Sampling matters in deep embedding learning. In: Proceedings of the IEEE International Conference on Computer Vision. 2017, 28592867 220. Li K, Xiao C. CBFL: a communication-efficient federated learning framework from data redundancy perspective. IEEE Systems Journal, 221. 2022, 16(4): 55725583 Duan L, Tsang I W, Xu D, Chua T S. Domain adaptation from multiple sources via auxiliary classifiers. In: Proceedings of the 26th Annual International Conference on Machine Learning. 2009, 289296 222. Duan L, Xu D, Tsang I W H. Domain adaptation from multiple sources: a domain-dependent regularization approach. IEEE Transactions on Neural Networks and Learning Systems, 2012, 23(3): 504518 223. Zhuang F, Luo P, Xiong H, Xiong Y, He Q, Shi Z. Cross-domain learning from multiple sources: a consensus regularization perspective. IEEE Transactions on Knowledge and Data Engineering, 2010, 22(12): 16641678 224. Luo P, Zhuang F, Xiong H, Xiong Y, He Q. Transfer learning from multiple source domains via consensus regularization. In: Proceedings of the 17th ACM Conference on Information and Knowledge Management. 2008, 103112 225. Kirkpatrick J, Pascanu R, Rabinowitz N, Veness J, Desjardins G, Rusu A A, Milan K, Quan J, Ramalho T, Grabska-Barwinska A, Hassabis D, Clopath C, Kumaran D, Hadsell R. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences of the United States of America, 2017, 114(13): 35213526 226. Yu H, Zhang N, Deng S, Yuan Z, Jia Y, Chen H. The devil is the classifier: investigating long tail relation classification with decoupling analysis. 2020, arXiv preprint arXiv: 2009.07022 227. Kang B, Xie S, Rohrbach M, Yan Z, Gordo A, Feng J, Kalantidis Y. Decoupling representation and classifier for long-tailed recognition. In: Proceedings of the 8th International Conference on Learning Representations. 2020 228. Yosinski J, Clune J, Bengio Y, Lipson H. How transferable are features in deep neural networks? In: Proceedings of the 27th International Conference on Neural Information Processing Systems. 2014, 33203328 229. Devlin J, Chang M W, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language understanding. In: Proceedings of 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2019, 41714186 230. Li X, Huang K, Yang W, Wang S, Zhang Z. On the convergence of FedAvg on non-IID data. In: Proceedings of the 8th International Conference on Learning Representations. 2020 231. Yamada Y, Lindenbaum O, Negahban S, Kluger Y. Feature selection using stochastic gates. In: Proceedings of the 37th International Conference on Machine Learning. 2020, 987 232. Zhou K, Yang Y, Qiao Y, Xiang T. Domain generalization with mixstyle. In: Proceedings of the 9th International Conference on Learning Representations. 2021 233. Li Q, Huang J, Hu J, Gong S. Feature-distribution perturbation and calibration for generalized Reid. In: Proceedings of 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2024, 28802884 234. Huang X, Belongie S. Arbitrary style transfer in real-time with adaptive instance normalization. In: Proceedings of the IEEE International Conference on Computer Vision. 2017, 15101519 235. Han B, Yao Q, Yu X, Niu G, Xu M, Hu W, Tsang I W, Sugiyama M. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In: Proceedings of the 32nd International Conference on Neural Information Processing Systems. 2018, 85368546 236. Liu Y, Kang Y, Xing C, Chen T, Yang Q. A secure federated transfer learning framework. IEEE Intelligent Systems, 2020, 35(4): 7082 237. Yang H, He H, Zhang W, Cao X. FedSteg: a federated transfer learning framework for secure image steganalysis. IEEE Transactions 238. 32 Front. Comput. Sci., 2024, 18(6): 186356 on Network Science and Engineering, 2021, 8(2): 10841094 Li Y, Chen C Y, Wasserman W W. Deep feature selection: theory and application to identify enhancers and promoters. Journal of Computational Biology, 2016, 23(5): 322336 239. Louizos C, Welling M, Kingma D P. Learning sparse neural networks through L0 regularization. In: Proceedings of the 6th International Conference on Learning Representations. 2018 240. Chang H, Shejwalkar V, Shokri R, Houmansadr A. Cronus: robust and heterogeneous collaborative learning with black-box knowledge transfer. 2019, arXiv preprint arXiv: 1912.11279 241. Van Berlo B, Saeed A, Ozcelebi T. Towards federated unsupervised representation learning. In: Proceedings of the 3rd ACM International Workshop on Edge Systems, Analytics and Networking. 2020, 3136 242. Liu J, Zhao P, Zhuang F, Liu Y, Sheng V S, Xu J, Zhou X, Xiong H. Exploiting aesthetic preference in deep cross networks for cross- domain recommendation. In: Proceedings of the Web Conference 2020. 2020, 27682774 243. Li P, Tuzhilin A. DDTCDR: deep dual transfer cross domain recommendation. In: Proceedings of the 13th International Conference on Web Search and Data Mining. 2020, 331339 244. Ammad-Ud-Din M, Ivannikova E, Khan S A, Oyomno W, Fu Q, Tan K E, Flanagan A. Federated collaborative filtering for privacy- preserving personalized recommendation system. 2019, arXiv preprint arXiv: 1901.09888 245. Minto L, Haller M, Livshits B, Haddadi H. Stronger privacy for federated collaborative filtering with implicit feedback. In: Proceedings of the 15th ACM Conference on Recommender Systems. 2021, 342350 246. Chai D, Wang L, Chen K, Yang Q. Secure federated matrix factorization. IEEE Intelligent Systems, 2021, 36(5): 1120 247. Du Y, Zhou D, Xie Y, Shi J, Gong M. Federated matrix factorization for privacy-preserving recommender systems. Applied Soft Computing, 2021, 111: 107700 248. Li Z, Ding B, Zhang C, Li N, Zhou J. Federated matrix factorization with privacy guarantee. Proceedings of the VLDB Endowment, 2021, 15(4): 900913 249. Wu C, Wu F, Cao Y, Huang Y, Xie X. FedGNN: federated graph neural network for privacy-preserving recommendation. 2021, arXiv preprint arXiv: 2102.04925 250. Zhang C, Long G, Zhou T, Yan P, Zhang Z, Zhang C, Yang B. Dual personalization on federated recommendation. In: Proceedings of the 32nd International Joint Conference on Artificial Intelligence. 2023, 45584566 251. Wu J, Liu Q, Huang Z, Ning Y, Wang H, Chen E, Yi J, Zhou B. Hierarchical personalized federated learning for user modeling. In: Proceedings of the Web Conference 2021. 2021, 957968 252. Wu M, Li L, Chang T, Rigall E, Wang X, Xu C Z. FedCDR: federated cross-domain recommendation for privacy-preserving rating prediction. In: Proceedings of the 31st ACM International Conference on Information  Knowledge Management. 2022, 21792188 253. Luo S, Xiao Y, Song L. Personalized federated recommendation via joint representation learning, user clustering, and model adaptation. In: Proceedings of the 31st ACM International Conference on Information  Knowledge Management. 2022, 42894293 254. Kaissis G A, Makowski M R, Rückert D, Braren R F. Secure, privacy- preserving and federated machine learning in medical imaging. Nature Machine Intelligence, 2020, 2(6): 305311 255. Sui D, Chen Y, Zhao J, Jia Y, Xie Y, Sun W. FedED: federated learning via ensemble distillation for medical relation extraction. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020, 21182128 256. Silva S, Gutman B A, Romero E, Thompson P M, Altmann A, Lorenzi257. M. Federated learning in distributed medical databases: meta-analysis of large-scale subcortical brain data. In: Proceedings of the 16th IEEE international symposium on biomedical imaging (ISBI 2019). 2019, 270274 Jin H, Dai X, Xiao J, Li B, Li H, Zhang Y. Cross-cluster federated learning and blockchain for internet of medical things. IEEE Internet of Things Journal, 2021, 8(21): 1577615784 258. Xia Y, Yang D, Li W, Myronenko A, Xu D, Obinata H, Mori H, An P, Harmon S, Turkbey E, Turkbey B, Wood B, Patella F, Stellato E, Carrafiello G, Ierardi A, Yuille A, Roth H. Auto-FedAvg: learnable federated averaging for multi-institutional medical image segmentation. 2021, arXiv preprint arXiv: 2104.10195 259. Jiang M, Roth H R, Li W, Yang D, Zhao C, Nath V, Xu D, Dou Q, Xu Z. Fair federated medical image segmentation via client contribution estimation. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2023, 1630216311 260. Acar D A E, Zhao Y, Navarro R M, Mattina M, Whatmough P N, Saligrama V. Federated learning based on dynamic regularization. In: Proceedings of the 9th International Conference on Learning Representations. 2021 261. Chen Z, Yang C, Zhu M, Peng Z, Yuan Y. Personalized retrogress- resilient federated learning toward imbalanced medical data. IEEE Transactions on Medical Imaging, 2022, 41(12): 36633674 262. Xu A, Li W, Guo P, Yang D, Roth H, Hatamizadeh A, Zhao C, Xu D, Huang H, Xu Z. Closing the generalization gap of cross-silo federated medical image segmentation. In: Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition. 2022, 2083420843 263. Jiang M, Yang H, Cheng C, Dou Q. IOP-FL: Inside-outside personalization for federated medical image segmentation. IEEE Transactions on Medical Imaging, 2023, 42(7): 21062117 264. Wang J, Jin Y, Stoyanov D, Wang L. FedDP: dual personalization in federated medical image segmentation. IEEE Transactions on Medical Imaging, 2024, 43(1): 297308 265. Adnan M, Kalra S, Cresswell J C, Taylor G W, Tizhoosh H R. Federated learning and differential privacy for medical image analysis. Scientific Reports, 2022, 12(1): 1953 266. Wu Y, Zeng D, Wang Z, Shi Y, Hu J. Federated contrastive learning for volumetric medical image segmentation. In: Proceedings of the 24th International Conference on Medical Image Computing and Computer Assisted Intervention. 2021, 367377 267. Li Y, Wen G. Research and practice of financial credit risk management based on federated learning. Engineering Letters, 2023, 31(1): 268. Xu Z, Cheng J, Cheng L, Xu X, Bilal M. Mses credit risk assessment model based on federated learning and feature selection. Computers, Materials  Continua, 2023, 75(3): 55735595 269. Lee C M, Delgado Fernández J, Potenciano Menci S, Rieger A, Fridgen G. Federated learning for credit risk assessment. In: Proceedings of the 56th Hawaii International Conference on System Sciences. 2023, 386395 270. Yang W, Zhang Y, Ye K, Li L, Xu C Z. FFD: a federated learning based method for credit card fraud detection. In: Proceedings of the 8th International Congress on Big Data. 2019, 1832 271. Wang Z, Xiao J, Wang L, Yao J. A novel federated learning approach with knowledge transfer for credit scoring. Decision Support Systems, 2024, 177: 114084 272. Pourroostaei Ardakani S, Du N, Lin C, Yang J C, Bi Z, Chen L. A federated learning-enabled predictive analysis to forecast stock market trends. Journal of Ambient Intelligence and Humanized Computing, 2023, 14(4): 45294535 273. Yan Y, Yang G, Gao Y, Zang C, Chen J, Wang Q. Multi-participant274. Wei GUO et al. A comprehensive survey of federated transfer learning: challenges, methods and applications 33 vertical federated learning based time series prediction. In: Proceedings of the 8th International Conference on Computing and Artificial Intelligence. 2022, 165171 Shaheen M, Farooq M S, Umer T. Reduction in data imbalance for client-side training in federated learning for the prediction of stock market prices. Journal of Sensor and Actuator Networks, 2024, 13(1): 1 275. Myalil D, Rajan M A, Apte M, Lodha S. Robust collaborative fraudulent transaction detection using federated learning. In: Proceedings of the 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA) . 2021, 373378 276. Abadi A, Doyle B, Gini F, Guinamard K, Murakonda S K, Liddell J, Mellor P, Murdoch S J, Naseri M, Page H, Theodorakopoulos G, Weller S. Starlit: Privacy-preserving federated learning to enhance financial fraud detection . 2024, arXiv preprint arXiv: 2401.10765 277. Liu Y, Yu J J Q, Kang J, Niyato D, Zhang S. Privacy-preserving traffic flow prediction: a federated learning approach. IEEE Internet of Things Journal, 2020, 7(8): 77517763 278. Zhang C, Dang S, Shihada B, Alouini M S. Dual attention-based federated learning for wireless traffic prediction. In: Proceedings of the IEEE Conference on Computer Communications. 2021, 110 279. Zeng T, Guo J, Kim K J, Parsons K, Orlik P, Di Cairano S, Saad W . Multi-task federated learning for traffic prediction and its application to route planning. In: Proceedings of 2021 IEEE Intelligent Vehicles Symposium (IV) . 2021, 451457 280. Zhang C, Cui L, Yu S, Yu J J Q. A communication-efficient federated learning scheme for IoT-based traffic forecasting . IEEE Internet of Things Journal, 2022, 9(14): 1191811931 281. Qi T, Chen L, Li G, Li Y, Wang C. FedAGCN: a traffic flow prediction framework based on federated learning and asynchronous graph convolutional network. Applied Soft Computing , 2023, 138: 110175 282. Phyu H P, Stanica R, Naboulsi D . Multi-slice privacy-aware traffic forecasting at ran level: a scalable federated-learning approach. IEEE Transactions on Network and Service Management, 2023, 20(4): 50385052 283. Xia M, Jin D, Chen J . Short-term traffic flow prediction based on graph convolutional networks and federated learning. IEEE Transactions on Intelligent Transportation Systems, 2023, 24(1): 11911203 284. Zhang L, Zhang C, Shihada B . Efficient wireless traffic prediction at the edge: a federated meta-learning approach . IEEE Communications Letters, 2022, 26(7): 15731577 285. Hu S, Ye Y, Hu Q, Liu X, Cao S, Yang H H, Shen Y, Angeloudis P, Parada L, Wu C. A federated learning-based framework for ride- sourcing traffic demand prediction. IEEE Transactions on Vehicular Technology , 2023, 72(11): 1400214015 286. Huo J T, XU Y W, Huo Z S, Xiao L M, He Z X. Research on key technologies of edge cache in virtual data space across WAN. Frontiers of Computer Science , 2023, 17(1): 171102. 287. Li H Z, Jin H, Zheng L, Huang Y, Liao X F. ReCSA: a dedicated sort accelerator using ReRAM-based content addressable memory. Frontiers of Computer Science, 2023, 17(2): 172103. 288. Jia J, Liu Y, Zhang G Z, Gao Y L, Qian D P. Software approaches for resilience of high performance computing systems: a survey. Frontiers of Computer Science , 2023, 17(4): 174105. 289. Guo J Y, Zhang L, ROMERO HUNG J, Li C, Zhao J R, Guo M Y . FPGA sharing in the cloud: a comprehensive analysis. Frontiers of Computer Science , 2023, 17(5): 175106. 290. Wei Guo is a PhD student at the Institute of Artificial Intelligence, Beihang University, China. She received his MSc degree from the School of Electronics and Computer Science at Southampton University, UK. Her research interests primarily lie in federated learning and transfer learning. Fuzhen Zhuang received the BE degree from the College of Computer Science, Chongqing University, China in 2006, and the PhD degree in computer science from the Institute of Computing Technology, Chinese Academy of Sciences, China in 2011. He is currently a full professor with the Institute of Artificial Intelligence, Beihang University, China. He has published more than 140 papers in some prestigious refereed journals and conference proceedings. His research interests include transfer learning, machine learning, data mining, multitask learning, knowledge graph, and recommendation systems. He is a senior member of the CCF. He was a recipient of the Distinguished Dissertation Award of CAAI in 2013. Xiao Zhang is an associate professor at the School of Computer Science and Technology, Shandong University, China. His research interests include distributed learning, federated learning, edge intelligence, and data mining. He has published more than 20 papers in prestigious refereed journals and conference proceedings, such as IEEE TKDE, TMC, UBICOMP, SIGKDD, SIGIR, IJCAI, ACM CIKM, and IEEE ICDM. Yiqi Tong is a PhD student at the School of Computer Science and Engineering, Beihang University, China. He received his MSc degree from the School of Informatics at Xiamen University, China. His research interests primarily lie in natural language processing and recommendation systems. Jin Dong is the General Director of Beijing Academy of Blockchain and Edge Computing (BABEC), Director of National Blockchain Technology Innovation Center, Director of Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing. He has been dedicated to technical research in the fields of blockchain, privacy computing, chip design, etc. The team led by him developed the first of kind high performance hardware -software integrated blockchain system - ChainMaker around the globe, aiming to break through the performance and security bottlenecks of large- scale blockchain applications. This has been widely adopted by a variety of key economic and industrial applications in China. Jin Dong received his PhD degree from Tsinghua University, China and has filed more than forty US patents. 34 Front. Comput. Sci., 2024, 18(6): 186356",
    "page_start": null,
    "page_end": null,
    "word_count": 29122,
    "created_at": "2025-08-18T07:03:36",
    "updated_at": "2025-08-18T07:03:36"
  }
]